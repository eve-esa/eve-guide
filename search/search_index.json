{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"EVE \u2014 Earth Virtual Expert","text":"Earth Virtual Expert (EVE)   An open science initiative funded and supported by the European Space Agency's \u03a6-lab, developed by Pi School in collaboration with Imperative Space and Mistral AI, aimed at advancing the use of Large Language Models (LLMs) for the Earth Observation (EO) and Earth Science (ES) communities."},{"location":"#getting-started","title":"Getting started","text":"<p>In this documentation, you will find all the necessary information to understand, use, and contribute to the EVE project. The EVE project encompasses several components that have been open-sourced for the benefit of the EO community. In this guide we provide the documentation for the components we built and used as part of the EVE project:</p> <ul> <li>Evalkit: A comprehensive evaluation framework for assessing LLMs on EO-specific tasks and General Domain benchmarks.</li> <li>Data Scraping: Framework used to scrape relevant EO documents from trusted sources.</li> <li>Data Processing: Comprehensive pipeline for cleaning, filtering, chunking and embedding EO documents.</li> <li>Chat-Backend: Codebase that powers the backend of the EVE chat platform.</li> <li>Chat-Frontend: Codebase that powers the frontend of the EVE chat platform.</li> </ul> <p>All these components represent the building blocks of the EVE project and are designed to be reusable and extensible for future research and development in the field of LLMs for Earth Observation. You can navigate through the different sections of this documentation using the sidebar on the left. Each section provides detailed information, examples, and instructions to help you get started with EVE.</p> <p>Other than these components, the EVE project also includes several repositories of experiments containing hands-on notebooks that have been included in the components above. These experiments repositories are not part of the main documentation since they do not contain production-ready code, however they are a useful entry point for those who want a hands-on experience exploring the EVE project. Here below the list of experiments repositories:</p> <ul> <li>EVE-RAG: A collection of notebooks demonstrating how to build Retrieval-Augmented Generation (RAG) systems and how to benchmark it.</li> <li>EVE-Hallucination: repository containing experiments and analysis on the hallucination pipeline implemented in EVE</li> </ul>"},{"location":"#project-overview","title":"Project Overview","text":"<p>EVE is a domain-specialized AI system designed to support scientists, analysts, and decision-makers through natural-language interaction.</p> <p>It provides expert assistance across tasks such as:</p> <ul> <li>EO and ES knowledge search and summarisation</li> <li>Scientific Q&amp;A and policy brief generation</li> <li>Document understanding and retrieval</li> <li>Technical and educational content creation</li> <li>Integration of private EO document collections via RAG</li> </ul> <p>The project unites data curation, model development, evaluation, and deployment to deliver open, reusable tools for Europe\u2019s EO ecosystem.</p>"},{"location":"#chat-platform","title":"Chat Platform","text":"<p>EVE features an interactive chat platform that allows users to explore and query Earth Observation and Earth Science knowledge through natural dialogue. The platform is currently under closed access \u2014 you can request an early access to the beta version via this registration link here. It is targeted for public release in early 2026.</p>"},{"location":"#model-development","title":"Model Development","text":"<p><code>EVE-Instruct</code> is trained on domain-curated corpora by fine-tuning Mistral Small 3.2 24B, through continued pretraining and instruction fine-tuning.</p> <ul> <li>The model is optimized for factuality, reasoning, and integration with retrieval-augmented generation.</li> <li>Training was carried out using EuroHPC Marenostrum, cloud environments, and Mistral infrastructure.</li> <li>Evaluation targets both general domain and EO-specific benchmarks.</li> </ul>"},{"location":"#licence","title":"Licence","text":"<p>All EVE releases follow ESA and EU open-science principles, promoting transparency, reproducibility, and European AI sovereignty. The project\u2019s outputs\u2014models, datasets, pipelines, and documentation\u2014are shared under permissive open-source licences (Apache 2.0 / CC-BY).</p>"},{"location":"#main-links","title":"Main Links","text":"<ul> <li> <p>\ud83c\udf10 Project Website \u2014 eve.philab.esa.int   Central hub for project updates, communication materials, and registration for early access to the EVE chat platform.</p> </li> <li> <p>\ud83e\udd17 Models &amp; Data \u2014 huggingface.co/eve-esa   Repository for domain-specific model checkpoints, datasets, and evaluation benchmarks.</p> </li> <li> <p>\ud83d\udcbb Codebase \u2014 github.com/eve-esa   Open-source tools, pipelines, and scripts developed within the EVE initiative.</p> </li> </ul>"},{"location":"#contributors","title":"Contributors","text":"<p> European Space Agency (\u03a6-lab) Pi School \u00b7   Imperative Space \u00b7   Mistral AI \u00b7 </p>"},{"location":"eo_task/","title":"Earth Observation Evaluation Tasks","text":"<p>This page provides a comprehensive overview of all available Earth Observation (EO) evaluation tasks in eve-evalkit. Each task is designed to assess different capabilities of language models in the Earth Observation domain.</p>"},{"location":"eo_task/#quick-reference","title":"Quick Reference","text":"Task Name Type Dataset Size Primary Metrics MCQA Multiple Answer Multiple Choice eve-esa/eve-is-mcqa 432 IoU, Accuracy MCQA Single Answer Multiple Choice eve-esa/mcqa-single-answer 1308 Accuracy Open Ended Generation eve-esa/open-ended 1304 LLM as Judge Open Ended with Context Generation eve-esa/open-ended-w-context 606 LLM Judge Refusal Generation eve-esa/refusal ~2000 LLM Judge Hallucination Detection Classification eve-esa/hallucination-detection 2996 Accuracy, Precision, Recall, F1 Wiley MCQA Multiple Choice eve-esa/wiley-mcqa Exact Match"},{"location":"eo_task/#detailed-task-descriptions","title":"Detailed Task Descriptions","text":""},{"location":"eo_task/#mcqa-multiple-answer","title":"MCQA Multiple Answer","text":"<p>Task Name: <code>is_mcqa</code> or <code>mcqa_multiple_answer</code></p> <p>Description:</p> <p>EVE-mcqa-multiple-answers consists of multiple-choice questions from Imperative Space MOOC exams where questions may have one or more correct answers. Models must identify all correct options from an arbitrary number of choices, making this a challenging task that requires comprehensive understanding rather than simple fact recall.</p> <p>How to Call:</p> <pre><code>tasks:\n  - name: is_mcqa\n    num_fewshot: 2\n    max_tokens: 10000\n</code></pre> <p>Dataset:</p> <ul> <li>Source: eve-esa/eve-is-mcqa</li> <li>Split: train</li> <li>Size: 432 samples</li> <li>Structure: Each example contains a <code>Question</code>, <code>Answers</code> (list of correct labels), and <code>Choices</code> (list with labels and text)</li> </ul> <p>Evaluation Metrics:</p> <ul> <li>IoU (Intersection over Union): Measures partial correctness by calculating the overlap between predicted and correct answer sets. IoU = |Predicted \u2229 Correct| / |Predicted \u222a Correct| (higher is better)</li> <li>Accuracy (Exact Match): Binary score where 1.0 means the predicted answer set exactly matches the correct answer set (higher is better)</li> </ul> <p>Why It's Useful:</p> <p>This task tests a model's comprehensive understanding of EO concepts where multiple aspects or factors may be simultaneously correct. The IoU metric is particularly valuable as it rewards partially correct answers, providing a more nuanced evaluation than simple exact matching. This reflects real-world scenarios where partial knowledge is still valuable.</p> <p>Example:</p> <pre><code>{\n  \"Question\": \"Which bands of Sentinel-2 have 10m resolution?\",\n  \"Answers\": [\"A\", \"B\", \"C\"],\n  \"Choices\": {\n    \"label\": [\"A\", \"B\", \"C\", \"D\"],\n    \"text\": [\"B2 (Blue)\", \"B3 (Green)\", \"B4 (Red)\", \"B8 (NIR)\"]\n  }\n}\n</code></pre>"},{"location":"eo_task/#mcqa-single-answer","title":"MCQA Single Answer","text":"<p>Task Name: <code>mcqa_single_answer</code></p> <p>Description:</p> <p>EVE-mcqa-single-answer is a traditional multiple-choice dataset with exactly one correct answer per question. Models must identify the single best option from the provided choices, testing factual knowledge and reasoning abilities in the Earth Observation domain.</p> <p>How to Call:</p> <pre><code>tasks:\n  - name: mcqa_single_answer\n    num_fewshot: 2\n    max_tokens: 10000\n</code></pre> <p>Dataset:</p> <ul> <li>Source: eve-esa/mcqa-single-answer</li> <li>Split: train</li> <li>Size: ~1000 samples</li> <li>Structure: Each example contains a <code>question</code>, <code>choices</code> (list of answer texts), and <code>answer</code> (single letter indicating correct choice)</li> </ul> <p>Evaluation Metrics:</p> <ul> <li>Accuracy: Percentage of questions answered correctly (higher is better)</li> </ul> <p>Why It's Useful:</p> <p>This task evaluates factual knowledge and reasoning abilities in scenarios where there is a single definitively correct answer. It's particularly useful for assessing fundamental EO concepts, terminology, and principles. The single-answer format reduces ambiguity and provides clear, interpretable results.</p> <p>Example:</p> <pre><code>{\n  \"question\": \"What is the spatial resolution of Sentinel-2's visible bands?\",\n  \"choices\": [\"5 meters\", \"10 meters\", \"20 meters\", \"60 meters\"],\n  \"answer\": \"B\"  # \"10 meters\"\n}\n</code></pre>"},{"location":"eo_task/#open-ended","title":"Open Ended","text":"<p>Task Name: <code>open_ended</code></p> <p>Description:</p> <p>EVE-open-ended is a collection of ~969 open-ended question-answer pairs focused on Earth Observation. The dataset covers a wide range of EO topics including satellite imagery analysis, remote sensing techniques, environmental monitoring, and LiDAR. Models must generate free-form responses demonstrating deep understanding without the constraints of multiple-choice formats.</p> <p>How to Call:</p> <pre><code>tasks:\n  - name: open_ended\n    num_fewshot: 5\n    max_tokens: 40000\n    judge_api_key: !ref judge_api_key\n    judge_base_url: !ref judge_base_url\n    judge_name: !ref judge_name\n</code></pre> <p>Dataset:</p> <ul> <li>Source: eve-esa/open-ended</li> <li>Split: train</li> <li>Size: ~969 samples</li> <li>Structure: Each example contains a <code>Question</code> and <code>Answer</code> (reference)</li> </ul> <p>Evaluation Metrics:</p> <ul> <li>LLM as Judge: A judge model evaluates the quality, accuracy, and completeness of generated answers using strict fact-checking rules (0 = FAIL, 1 = PASS)</li> <li>Alternative metrics: BLEU, ROUGE, Cosine Similarity, BERTScore</li> </ul> <p>LLM Judge Evaluation Rules:</p> <ol> <li>Contradiction Check: Fails if the answer contains ANY fact contradicting the reference</li> <li>Relevance Check: Fails if the answer omits ESSENTIAL technical facts from the reference</li> <li>Additive Information: Additional correct information is acceptable if it doesn't contradict</li> <li>Focus on Substance: Ignores style, length, and tolerates minor phrasing differences</li> </ol> <p>Why It's Useful:</p> <p>This task assesses a model's ability to explain concepts, provide detailed answers, and demonstrate deep understanding. It's essential for evaluating models intended for educational or explanatory applications in EO, where nuanced explanations and technical accuracy are paramount.</p>"},{"location":"eo_task/#open-ended-with-context","title":"Open Ended with Context","text":"<p>Task Name: <code>open_ended_w_context</code></p> <p>Description:</p> <p>EVE-open-ended-w-context provides open-ended questions that must be answered using 1-3 accompanying context documents. This tests the model's ability to extract and synthesize information from reference materials, making it ideal for evaluating Retrieval-Augmented Generation (RAG) systems. Not all samples contain all three documents, requiring models to handle variable numbers of context documents gracefully.</p> <p>How to Call:</p> <pre><code>tasks:\n  - name: open_ended_w_context\n    num_fewshot: 5\n    max_tokens: 40000\n    judge_api_key: !ref judge_api_key\n    judge_base_url: !ref judge_base_url\n    judge_name: !ref judge_name\n</code></pre> <p>Dataset:</p> <ul> <li>Source: eve-esa/open-ended-w-context</li> <li>Split: train</li> <li>Structure: Each example contains a <code>Question</code>, <code>Answer</code>, and up to three context documents (<code>Doc 1</code>, <code>Doc 2</code>, <code>Doc 3</code>)</li> </ul> <p>Evaluation Metrics:</p> <ul> <li>LLM Judge: Evaluates whether answers are grounded in the provided context and correctly answer the question (higher is better)</li> <li>Uses the same strict fact-checking evaluation rules as open-ended tasks</li> </ul> <p>Why It's Useful:</p> <p>This task evaluates retrieval-augmented generation (RAG) capabilities, testing whether models can accurately extract information from provided documents rather than relying solely on parametric knowledge. This is crucial for applications where answers must be grounded in specific documentation or data sources. It also tests the model's ability to distinguish between context-provided information and pre-trained knowledge.</p> <p>Example:</p> <pre><code>{\n  \"Question\": \"What is the spatial resolution of Sentinel-2's visible bands?\",\n  \"Answer\": \"Sentinel-2's visible bands have a spatial resolution of 10 meters.\",\n  \"Doc 1\": \"The Sentinel-2 mission comprises a constellation...\",\n  \"Doc 2\": \"Sentinel-2 carries the Multi-Spectral Instrument (MSI)...\",\n  \"Doc 3\": \"\"  # May be empty\n}\n</code></pre>"},{"location":"eo_task/#refusal","title":"Refusal","text":"<p>Task Name: <code>refusal</code></p> <p>Description:</p> <p>EVE-Refusal tests whether language models can appropriately refuse to answer questions when the provided context does not contain sufficient information. The dataset presents questions alongside context documents that intentionally lack the necessary information to answer. A well-calibrated model should recognize this limitation and refuse to answer, rather than generating plausible but incorrect information.</p> <p>How to Call:</p> <pre><code>tasks:\n  - name: refusal\n    num_fewshot: 5\n    max_tokens: 40000\n    judge_api_key: !ref judge_api_key\n    judge_base_url: !ref judge_base_url\n    judge_name: !ref judge_name\n</code></pre> <p>Dataset:</p> <ul> <li>Source: eve-esa/refusal</li> <li>Split: train</li> <li>Structure: Each example contains a <code>question</code> and <code>context</code> (insufficient for answering)</li> <li>Expected Answer: \"I'm sorry, but the provided context does not contain enough information to answer that question.\"</li> </ul> <p>Evaluation Metrics:</p> <ul> <li>LLM Judge: Evaluates whether the model appropriately refuses to answer or acknowledges insufficient information (higher is better)</li> </ul> <p>Expected Behavior:</p> <ul> <li>Recognize when provided context lacks sufficient information</li> <li>Explicitly refuse to answer or state information is not available</li> <li>Avoid generating plausible-sounding but fabricated information</li> <li>Maintain accuracy and honesty over completeness</li> </ul> <p>Why It's Useful:</p> <p>This task tests a critical safety and reliability feature: the ability to recognize limitations and avoid generating potentially incorrect information when context is insufficient. This prevents hallucinations and ensures trustworthy behavior in production systems. It's particularly important for RAG systems and applications where factual accuracy is paramount.</p>"},{"location":"eo_task/#hallucination-detection","title":"Hallucination Detection","text":"<p>Task Name: <code>hallucination_detection</code></p> <p>Description:</p> <p>EVE-Hallucination is a specialized dataset for evaluating language models' tendency to hallucinate in the Earth Observation domain. Unlike typical QA datasets, this contains deliberately hallucinated answers with detailed annotations marking which portions of text are hallucinated. The task is to identify whether a given answer contains hallucinated (false or unsupported) information.</p> <p>How to Call:</p> <pre><code>tasks:\n  - name: hallucination_detection\n    num_fewshot: 0\n    max_tokens: 100\n    judge_api_key: !ref judge_api_key\n    judge_base_url: !ref judge_base_url\n    judge_name: !ref judge_name\n</code></pre> <p>Dataset:</p> <ul> <li>Source: eve-esa/hallucination-detection</li> <li>Split: train</li> <li>Structure: Each example contains <code>Question</code>, <code>Answer</code> (with hallucinations), <code>Soft labels</code> (probabilistic spans), and <code>Hard labels</code> (definite spans)</li> </ul> <p>Evaluation Metrics:</p> <ul> <li>Accuracy: Overall correctness of hallucination detection (higher is better)</li> <li>Precision: Ratio of correctly identified hallucinations to all predicted hallucinations (higher is better)</li> <li>Recall: Ratio of correctly identified hallucinations to all actual hallucinations (higher is better)</li> <li>F1 Score: Harmonic mean of precision and recall (higher is better)</li> </ul> <p>Task Levels:</p> <ol> <li>Binary Detection: Determine if answer contains any hallucinated information (yes/no)</li> <li>Hard Span Detection: Identify exact character spans that are hallucinated</li> <li>Soft Span Detection: Identify spans with confidence scores</li> </ol> <p>Why It's Useful:</p> <p>This task evaluates a model's ability to self-assess and identify unreliable or fabricated information in EO contexts. Models with strong hallucination detection capabilities are more trustworthy and can potentially be used to validate outputs from other systems. This is crucial for safety-critical applications like climate monitoring, disaster response, and environmental analysis.</p> <p>Example:</p> <pre><code>{\n  \"Question\": \"What is the spatial resolution of Sentinel-2's visible bands?\",\n  \"Answer\": \"Sentinel-2's visible bands have a spatial resolution of 5 meters, making it the highest resolution freely available satellite.\",\n  \"Hard labels\": [[52, 60], [73, 127]]  # Character spans that are hallucinated\n}\n</code></pre>"},{"location":"eo_task/#wiley-mcqa","title":"Wiley MCQA","text":"<p>Task Name: <code>wiley_mcqa</code></p> <p>Description:</p> <p>A group of multiple-choice question tasks sourced from Wiley educational textbooks. Includes subtasks for specific textbooks covering physics, biology, environmental science, and geography. These provide standardized benchmarks across multiple scientific domains relevant to Earth Observation.</p> <p>How to Call:</p> <pre><code>tasks:\n  - name: wiley_mcqa\n    num_fewshot: 0\n    max_tokens: 1000\n</code></pre> <p>Or call specific subtasks:</p> <pre><code>tasks:\n  - name: Halliday_Physics_12e\n    num_fewshot: 0\n    max_tokens: 1000\n</code></pre> <p>Dataset:</p> <ul> <li>Source: eve-esa/wiley-mcqa</li> <li>Split: train</li> <li>Structure: Each example contains question, choices, and answer</li> </ul> <p>Available Subtasks:</p> <ul> <li><code>Halliday_Physics_12e</code> - Physics (Halliday, 12th edition)</li> <li><code>Karp_Cell_and_Molecular_Biology_9e</code> - Cell and Molecular Biology (Karp, 9th edition)</li> <li><code>Berg_Visualizing_Environmental_Science_5e</code> - Environmental Science (Berg, 5th edition)</li> <li><code>Nijman_The_World_Today_8e</code> - Geography (Nijman, 8th edition)</li> </ul> <p>Evaluation Metrics:</p> <ul> <li>Exact Match: Percentage of questions with exact correct answer (higher is better)</li> </ul> <p>Why It's Useful:</p> <p>This task suite evaluates understanding of fundamental scientific concepts from established educational materials. It provides standardized benchmarks across multiple scientific domains relevant to Earth Observation, including physics principles underlying remote sensing, environmental processes, and geographical concepts. These textbook-based questions have well-established correct answers and test foundational knowledge.</p>"},{"location":"eo_task/#running-tasks","title":"Running Tasks","text":""},{"location":"eo_task/#using-configuration-file","title":"Using Configuration File","text":"<p>Add tasks to your <code>evals.yaml</code>:</p> <pre><code>constants:\n  judge_api_key: your-judge-api-key\n  judge_base_url: https://openrouter.ai/api/v1\n  judge_name: mistralai/mistral-large-2411\n  tasks:\n    - name: eo_summarization\n      num_fewshot: 0\n      max_tokens: 20000\n      judge_api_key: !ref judge_api_key\n      judge_base_url: !ref judge_base_url\n      judge_name: !ref judge_name\n    - name: is_mcqa\n      num_fewshot: 2\n      max_tokens: 10000\n    - name: hallucination_detection\n      num_fewshot: 0\n      max_tokens: 100\n\nmodels:\n  - name: your-model-name\n    base_url: https://api.provider.com/v1/chat/completions\n    api_key: your-api-key\n    temperature: 0.1\n    num_concurrent: 5\n    tasks: !ref tasks\n\noutput_dir: evals_outputs\n</code></pre> <p>Then run:</p> <pre><code>python scripts/evaluate.py evals.yaml\n</code></pre>"},{"location":"eo_task/#direct-command-line","title":"Direct Command Line","text":"<pre><code>lm_eval --model openai-chat-completions \\\n        --model_args base_url=https://api.provider.com,model=model-name,num_concurrent=5 \\\n        --tasks {task_name} \\\n        --include tasks \\\n        --num_fewshot 0 \\\n        --output_path ./outputs \\\n        --log_samples \\\n        --apply_chat_template\n</code></pre> <p>For tasks using LLM-as-judge metrics, set environment variables:</p> <pre><code>export JUDGE_API_KEY=your-judge-api-key\nexport JUDGE_BASE_URL=https://api.provider.com/v1\nexport JUDGE_NAME=judge-model-name\n</code></pre>"},{"location":"eo_task/#task-selection-guide","title":"Task Selection Guide","text":"<p>Choose tasks based on your evaluation goals:</p> <p>Factual Knowledge: - <code>mcqa_single_answer</code> - Single correct answer questions - <code>is_mcqa</code> - Multiple correct answers with partial credit - <code>wiley_mcqa</code> - Standardized textbook questions</p> <p>Generation Quality: - <code>eo_summarization</code> - Abstractive summarization of technical content - <code>open_ended</code> - Free-form explanatory answers</p> <p>Grounded Generation (RAG): - <code>open_ended_w_context</code> - Answer questions using provided documents - <code>refusal</code> - Recognize when context is insufficient</p> <p>Reliability &amp; Safety: - <code>hallucination_detection</code> - Identify fabricated information - <code>refusal</code> - Avoid answering without sufficient information</p> <p>Comprehensive Evaluation: - Run all tasks for a complete assessment across different capabilities</p>"},{"location":"eo_task/#evaluation-best-practices","title":"Evaluation Best Practices","text":"<ol> <li>Use Few-Shot Examples: Most tasks benefit from few-shot examples (typically 2-5) to demonstrate the expected format</li> <li>Set Appropriate Timeouts: Some tasks require longer generation (e.g., summarization), so adjust timeouts accordingly</li> <li>Configure Judge Model: For LLM-as-judge tasks, choose a capable judge model (e.g., GPT-4, Claude 3.5 Sonnet, Mistral Large)</li> <li>Log Samples: Always use <code>--log_samples</code> to inspect individual predictions and understand model behavior</li> <li>Monitor Costs: LLM-as-judge evaluation can be expensive; consider using smaller subsets for initial testing</li> </ol>"},{"location":"eo_task/#additional-resources","title":"Additional Resources","text":"<ul> <li>Dataset Repository: https://huggingface.co/eve-esa</li> <li>GitHub Repository: https://github.com/eve-esa/eve-evaluation</li> <li>LM Evaluation Harness: https://github.com/EleutherAI/lm-evaluation-harness</li> </ul>"},{"location":"eo_task/#citation","title":"Citation","text":"<p>If you use these tasks or datasets in your research, please cite:</p> <pre><code>@misc{eve2025,\n  title={EVE: Earth Virtual Expert},\n  author={ESA},\n  year={2025},\n  publisher={HuggingFace},\n  url={https://huggingface.co/eve-esa/eve_v0.1}\n}\n</code></pre> <p>For the underlying evaluation framework:</p> <pre><code>@software{eval-harness,\n  author       = {Gao, Leo and others},\n  title        = {A framework for few-shot language model evaluation},\n  month        = sep,\n  year         = 2021,\n  publisher    = {Zenodo},\n  version      = {v0.0.1},\n  doi          = {10.5281/zenodo.5371628},\n  url          = {https://doi.org/10.5281/zenodo.5371628}\n}\n</code></pre>"},{"location":"frontend/docs/","title":"EVE Frontend","text":"<p>Welcome to the EVE frontend documentation. This React + TypeScript app lets you chat with the EVE Large Language Model about Earth Observation (EO) data with rich controls for retrieval, generation, and collection management.</p>"},{"location":"frontend/docs/#key-features","title":"Key Features","text":""},{"location":"frontend/docs/#chat-and-conversations","title":"Chat and Conversations","text":"<p>Work with EVE using a chat-first interface:</p> <ul> <li>Markdown-rendered responses with citation links back to sources.</li> <li>Conversation management including create, rename, delete, and retry on errors.</li> <li>Contextual retries and follow-up questions within the same thread.</li> </ul>"},{"location":"frontend/docs/#retrieval-and-generation-controls","title":"Retrieval and Generation Controls","text":"<ul> <li>Adjust retrieval depth, temperature, and related generation parameters.</li> <li>Configure context windows to balance precision and recall.</li> <li>Apply presets or per-chat overrides through the Control Panel.</li> </ul>"},{"location":"frontend/docs/#collections-and-enrichment","title":"Collections and Enrichment","text":"<ul> <li>Browse shared EO collections provided by your organization.</li> <li>Upload and manage private collections for personalized RAG enrichment.</li> <li>Combine shared and private sources to tailor responses.</li> </ul>"},{"location":"frontend/docs/#account-and-security","title":"Account and Security","text":"<ul> <li>Authentication flows for login, logout, and password resets.</li> <li>Profile management for names, email, and other account details.</li> <li>Session-aware UI to keep conversations scoped to the right user.</li> </ul>"},{"location":"frontend/docs/getting_started/","title":"Getting Started","text":"<p>Follow these steps to run the EVE frontend locally.</p>"},{"location":"frontend/docs/getting_started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Node.js 18+ (use the version from the README).</li> <li>Yarn.</li> </ul>"},{"location":"frontend/docs/getting_started/#install-run","title":"Install &amp; Run","text":""},{"location":"frontend/docs/getting_started/#1-clone-the-repository","title":"1. Clone the repository.","text":""},{"location":"frontend/docs/getting_started/#2-install-dependencies","title":"2. Install dependencies:","text":"<pre><code>yarn install\n</code></pre>"},{"location":"frontend/docs/getting_started/#3-create-a-env-in-the-project-root-based-on-envexample","title":"3. Create a <code>.env</code> in the project root based on <code>.env.example</code>:","text":"<pre><code>VITE_API_URL=https://api.example.com\nVITE_CONTACT_URL=https://example.com\n</code></pre> <p>Never commit secrets; <code>.env*</code> is gitignored.</p>"},{"location":"frontend/docs/getting_started/#4-start-the-dev-server-httplocalhost5173","title":"4. Start the dev server (http://localhost:5173):","text":"<pre><code>yarn dev\n</code></pre>"},{"location":"frontend/docs/getting_started/#useful-scripts","title":"Useful scripts","text":"<ul> <li>Build: <code>yarn build</code></li> <li>Preview production build: <code>yarn preview</code></li> <li>Lint: <code>yarn lint</code></li> </ul>"},{"location":"frontend/docs/features/chat/","title":"Chat","text":"<p>EVE\u2019s chat is a multi-turn assistant focused on Earth Observation topics. Messages render Markdown/KaTeX, stream in real time, and include source citations when RAG is used.</p>"},{"location":"frontend/docs/features/chat/#layout","title":"Layout","text":"<ul> <li>Conversations sidebar (left): start a new chat, browse history, rename or delete conversations. On mobile, tap the burger icon to toggle it.</li> <li>Chat area (center): message list with streaming answers. Retry is offered on errors; \u201cShow more/less\u201d expands long prompts.</li> <li>Dynamic sidebar (right): opens contextual panels such as Sources, Control Panel, Shared collections, and My collections.</li> </ul>"},{"location":"frontend/docs/features/chat/#start-a-conversation","title":"Start a conversation","text":"<ol> <li>Click NEW CHAT in the conversations sidebar.</li> <li>Type your question and press Enter (or click the send icon).</li> <li>Continue the discussion; history stays linked to the conversation.</li> </ol>"},{"location":"frontend/docs/features/chat/#working-with-messages","title":"Working with messages","text":"<ul> <li>Sources: when a reply uses retrieved documents, click Sources (n) in the message footer to inspect the cited snippets.</li> <li>Feedback: thumbs up/down to rate answers; optionally leave a reason for negative feedback.</li> <li>Copy: copy any reply text directly from the footer.</li> </ul>"},{"location":"frontend/docs/features/chat/#knowledge-base-shortcuts","title":"Knowledge base shortcuts","text":"<ul> <li>Open Control Panel from the sliders button beside the input to tune RAG and generation settings.</li> <li>Open Shared collections or My collections from the Knowledge Base menu in the sidebar to control which documents inform answers.</li> </ul>"},{"location":"frontend/docs/features/control_panel/","title":"Control Panel","text":"<p>The Control Panel lets you adjust how the assistant retrieves and generates answers (RAG + LLM parameters). Open it from the sliders icon next to the chat input.</p>"},{"location":"frontend/docs/features/control_panel/#parameters","title":"Parameters","text":"<ul> <li>Temperature: creativity of the LLM (0 = deterministic, 1 = varied).</li> <li>Similarity threshold: minimum relevance score for retrieved chunks.</li> <li>Max documents (k): number of documents to pass into the answer.</li> <li>Year range: restrict document publication years.</li> <li>Journal filter: limit retrieval to a specific journal.</li> <li>Topic filters: thematic, scientific/technical, and market perspectives (hidden when <code>VITE_HIDE_CLASSIFICATION_FILTERS=true</code>).</li> <li>Minimum citations: require a minimum citation count.</li> </ul>"},{"location":"frontend/docs/features/control_panel/#persistence-defaults","title":"Persistence &amp; defaults","text":"<ul> <li>Settings are stored in the browser\u2019s <code>localStorage</code> per user/device.</li> <li>Click Save values to persist changes.</li> <li>Use Reset filters to default values to restore the defaults.</li> </ul>"},{"location":"frontend/docs/features/private_collection/","title":"Private Collections","text":"<p>Private collections are your own document sets stored in the vector database (Qdrant) and used for Retrieval-Augmented Generation in your chats.</p>"},{"location":"frontend/docs/features/private_collection/#create-and-manage","title":"Create and manage","text":"<ol> <li>Open the Knowledge Base menu in the left sidebar and choose My collections.</li> <li>Click NEW COLLECTION and provide a name.</li> <li>Select a collection to view its documents; delete the collection if you no longer need it.</li> </ol>"},{"location":"frontend/docs/features/private_collection/#upload-documents","title":"Upload documents","text":"<ul> <li>Supported formats: PDF and TXT.</li> <li>Use drag &amp; drop or click to browse files in the upload area for the selected collection.</li> <li>Uploaded files are embedded into Qdrant; relevant chunks will be pulled into chat answers when applicable.</li> <li>You can delete individual documents you own.</li> </ul>"},{"location":"frontend/docs/features/private_collection/#tips","title":"Tips","text":"<ul> <li>Keep uploads focused on the topics you\u2019ll query for better retrieval quality.</li> <li>After uploading, send a new message to allow the assistant to use the fresh context.</li> </ul>"},{"location":"frontend/docs/features/profile/","title":"Profile &amp; Authentication","text":"<p>The app provides email-based authentication plus basic profile management.</p>"},{"location":"frontend/docs/features/profile/#log-in","title":"Log in","text":"<ul> <li>Log in with email and password; a \u201cRemember me\u201d option stores your email locally.</li> </ul>"},{"location":"frontend/docs/features/profile/#password-recovery","title":"Password recovery","text":"<ul> <li>Forgot password: request a reset link via email.</li> <li>Reset password: open the link, set a new password, and confirm it.</li> </ul>"},{"location":"frontend/docs/features/profile/#profile-updates","title":"Profile updates","text":"<ul> <li>Open the profile menu from the sidebar and choose Profile.</li> <li>Update first and last name; email is displayed but not editable.</li> </ul>"},{"location":"frontend/docs/features/profile/#logout","title":"Logout","text":"<ul> <li>Use Logout in the profile menu to end the session safely.</li> </ul>"},{"location":"frontend/docs/features/shared_collection/","title":"Shared Collections","text":"<p>Shared collections are curated document sets provided to all users. When enabled, they act as additional context sources for RAG answers.</p>"},{"location":"frontend/docs/features/shared_collection/#enable-collections","title":"Enable collections","text":"<ol> <li>Open the Knowledge Base menu in the left sidebar.</li> <li>Select Shared collections to open the sidebar.</li> <li>Toggle the switch next to each collection you want to include; selections are saved locally in your browser.</li> </ol>"},{"location":"frontend/docs/features/shared_collection/#how-it-works","title":"How it works","text":"<ul> <li>Enabled collections are considered during retrieval when you send a prompt.</li> <li>Descriptions in the list explain the scope of each collection to help you choose the right ones.</li> </ul>"},{"location":"backend/docs/","title":"Home","text":""},{"location":"backend/docs/#eve-backend","title":"EVE Backend","text":"<p>A FastAPI-based backend service for chat with Retrieval-Augmented Generation (RAG). It provides authentication, collections and document ingestion, conversation/message management, streaming responses, and a hallucination detection pipeline.</p> <p>For setup, Docker usage, local development, and deployment details, refer to README.md.</p>"},{"location":"backend/docs/#architecture","title":"Architecture","text":"<ul> <li>FastAPI: HTTP API and dependency injection</li> <li>MongoDB/DocumentDB: Primary datastore for users, collections, documents, conversations, messages</li> <li>Qdrant: Vector store for embeddings and retrieval</li> <li>LLM providers: Pluggable via <code>src/core/llm_manager.py</code></li> <li>Docs: MkDocs Material + mkdocstrings (Google-style docstrings)</li> </ul>"},{"location":"backend/docs/#directory-structure","title":"Directory structure","text":"<pre><code>src/\n  routers/            # FastAPI route handlers (auth, collection, document, conversation, message, tool, user, health)\n  services/           # Business logic (auth, generate_answer, email, hallucination, etc.)\n  database/           # ODM-style models and pagination helpers\n  core/               # Vector store and LLM managers\n  middlewares/        # Authentication dependencies\n  schemas/            # Pydantic request/response models\n  templates/          # Prompt templates and pipeline configs\n  utils/              # Helpers, parsers, embeddings, rerankers\ntests/                # API and domain tests\ndocs/                 # Site content (this page, api references)\n</code></pre>"},{"location":"backend/docs/#key-workflows","title":"Key workflows","text":"<ul> <li>Authentication<ul> <li>Signup, email activation, login, refresh</li> <li>Endpoints in <code>routers.auth</code> and <code>routers.forgot_password</code></li> </ul> </li> <li>Collections &amp; Documents<ul> <li>Create Qdrant collections, upload documents, delete documents</li> <li>Ingestion triggers parsing, chunking, embedding, and vector upsert</li> <li>Endpoints in <code>routers.collection</code> and <code>routers.document</code></li> </ul> </li> <li>Conversations &amp; Messages<ul> <li>Create conversations, post messages, stream responses (SSE)</li> <li>Retry message generation, update feedback/annotations</li> <li>Endpoints in <code>routers.conversation</code> and <code>routers.message</code></li> </ul> </li> <li>Hallucination Detection<ul> <li>Synchronous detection and streaming modes</li> <li>Annotates message metadata with label, reason, timings</li> </ul> </li> </ul>"},{"location":"backend/docs/#api-surface-reference","title":"API surface (reference)","text":"<ul> <li>Auth: <code>[routers-auth]</code> \u2014 <code>routers.auth</code></li> <li>Collections: <code>[routers-collection]</code> \u2014 <code>routers.collection</code></li> <li>Documents: <code>[routers-document]</code> \u2014 <code>routers.document</code></li> <li>Conversations: <code>[routers-conversation]</code> \u2014 <code>routers.conversation</code></li> <li>Messages: <code>[routers-message]</code> \u2014 <code>routers.message</code></li> <li>Users: <code>[routers-user]</code> \u2014 <code>routers.user</code></li> <li>Health: <code>[routers-health_check]</code> \u2014 <code>routers.health_check</code></li> </ul>"},{"location":"backend/docs/#message-generation-flow-high-level","title":"Message generation flow (high-level)","text":"<ol> <li>Validate conversation ownership and requested collections</li> <li>Expand requested collections with allowed public and user-owned collections</li> <li>Optionally extract year range from filters for MCP usage</li> <li>If starting a new chat, create <code>Conversation</code> first; then create placeholder <code>Message</code> record</li> <li>Run the answer generation pipeline:<ul> <li>Build context (RAG decision, retrieval, reranking)</li> <li>Generate answer from LLM and record timings and prompt metadata</li> </ul> </li> <li>Update <code>Message</code> output (answer), documents, flags, and latencies</li> <li>Optionally schedule rollup/trim in background</li> <li>For streaming endpoints, publish tokens and lifecycle events via bus</li> </ol>"},{"location":"backend/docs/#documentation-notes","title":"Documentation notes","text":"<ul> <li>Docstrings are Google-style with <code>Args:</code>, <code>Returns:</code>, <code>Raises:</code>.</li> <li>Module reference pages in <code>docs/</code> use:</li> </ul> <pre><code>::: routers.&lt;module&gt;\n</code></pre>"},{"location":"backend/docs/getting_started/","title":"Getting Started with Backend","text":"<p>This guide will help you set up and run the backend on local machine or production.</p>"},{"location":"backend/docs/getting_started/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following installed:</p> <ul> <li>Python 3.12+</li> <li>MongoDB</li> <li>Docker and Docker Compose</li> </ul>"},{"location":"backend/docs/getting_started/#installing-prerequisites","title":"Installing Prerequisites","text":""},{"location":"backend/docs/getting_started/#python-312","title":"Python 3.12+","text":"<p>Ubuntu:</p> <pre><code># Update package list\nsudo apt update\n\n# Install Python 3.12 and pip\nsudo apt install python3.12 python3.12-venv python3-pip\n\n# Verify installation\npython3.12 --version\n</code></pre> <p>Windows:</p> <ol> <li>Download Python 3.12+ from the official Python website</li> <li>Run the installer and check \"Add Python to PATH\"</li> <li>Verify installation:</li> </ol> <pre><code>python --version\n</code></pre> <p>Reference: Python Installation Guide</p>"},{"location":"backend/docs/getting_started/#mongodb","title":"MongoDB","text":"<p>Ubuntu:</p> <pre><code># Import MongoDB public GPG key\ncurl -fsSL https://www.mongodb.org/static/pgp/server-7.0.asc | sudo gpg -o /usr/share/keyrings/mongodb-server-7.0.gpg --dearmor\n\n# Add MongoDB repository\necho \"deb [ arch=amd64,arm64 signed-by=/usr/share/keyrings/mongodb-server-7.0.gpg ] https://repo.mongodb.org/apt/ubuntu jammy/mongodb-org/7.0 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-7.0.list\n\n# Update package list and install MongoDB\nsudo apt update\nsudo apt install -y mongodb-org\n\n# Start MongoDB service\nsudo systemctl start mongod\nsudo systemctl enable mongod\n</code></pre> <p>Windows:</p> <ol> <li>Download MongoDB Community Server from the official MongoDB website</li> <li>Run the installer and follow the setup wizard</li> <li>MongoDB will be installed as a Windows service and start automatically</li> </ol> <p>Reference:  - MongoDB Installation Guide - Ubuntu - MongoDB Installation Guide - Windows</p>"},{"location":"backend/docs/getting_started/#docker-and-docker-compose","title":"Docker and Docker Compose","text":"<p>Ubuntu:</p> <pre><code># Remove old versions\nsudo apt remove docker docker-engine docker.io containerd runc\n\n# Install prerequisites\nsudo apt update\nsudo apt install ca-certificates curl gnupg lsb-release\n\n# Add Docker's official GPG key\nsudo install -m 0755 -d /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\nsudo chmod a+r /etc/apt/keyrings/docker.gpg\n\n# Add Docker repository\necho \\\n  \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\\n  $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\n\n# Install Docker Engine and Docker Compose\nsudo apt update\nsudo apt install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\n\n# Add your user to docker group (to run without sudo)\nsudo usermod -aG docker $USER\n\n# Verify installation\ndocker --version\ndocker compose version\n</code></pre> <p>Windows:</p> <ol> <li>Download Docker Desktop from the official Docker website</li> <li>Run the installer and follow the setup wizard</li> <li>Restart your computer if prompted</li> <li>Docker Desktop includes both Docker and Docker Compose</li> <li>Verify installation:</li> </ol> <pre><code>docker --version\ndocker compose version\n</code></pre> <p>Reference: - Docker Installation Guide - Ubuntu - Docker Desktop for Windows</p>"},{"location":"backend/docs/getting_started/#local-development-setup","title":"Local Development Setup","text":""},{"location":"backend/docs/getting_started/#1-create-virtual-environment","title":"1. Create Virtual Environment","text":"<p>Create a virtual environment in the <code>venv</code> folder:</p> <pre><code>python3 -m venv venv\n</code></pre> <p>Activate the virtual environment:</p> <pre><code>source venv/bin/activate\n</code></pre>"},{"location":"backend/docs/getting_started/#2-environment-configuration","title":"2. Environment Configuration","text":"<p>Create a <code>.env</code> file in the root of the project with the following content:</p> <pre><code># QDRANT Configuration\nQDRANT_URL=\nQDRANT_API_KEY=\n\n# LLM Model URLs (OpenAI-compatible format)\nMAIN_MODEL_URL=https://api.runpod.ai/v2/2f9o93xc90871m/openai/v1\nFALLBACK_MODEL_URL=https://api.mistral.ai/v1\n# Optional: Override model names (defaults from config.yaml)\nMAIN_MODEL_NAME=\nFALLBACK_MODEL_NAME=\n\nMAIN_MODEL_API_KEY=\nFALLBACK_MODEL_API_KEY=\n\nMODEL_TIMEOUT=13\n\nEMBEDDING_URL=https://api.deepinfra.com/v1/openai\nEMBEDDING_API_KEY=\n\nEMBEDDING_FALLBACK_URL=https://api.siliconflow.com/v1\nEMBEDDING_FALLBACK_API_KEY=\n\n# MongoDB Configuration\nMONGO_HOST=localhost\nMONGO_PORT=27017\nMONGO_USERNAME=root\nMONGO_PASSWORD=\nMONGO_DATABASE=eve-backend\nMONGO_PARAMS=\n\n# JWT Configuration\nJWT_SECRET_KEY=\nJWT_ALGORITHM=HS256\nJWT_ACCESS_TOKEN_EXPIRE_MINUTES=15\nJWT_REFRESH_TOKEN_EXPIRE_DAYS=7\n\n# SMTP\nSMTP_HOST=smtp.gmail.com\nSMTP_PORT=587\nSMTP_USERNAME=\nSMTP_PASSWORD=\nEMAIL_FROM_ADDRESS=noreply@eve-esa.com\nEMAIL_FROM_NAME=EVE\n\n# CORS (comma separated list)\nCORS_ALLOWED_ORIGINS=http://localhost:5173\n\nDEEPINFRA_API_TOKEN=\nINFERENCE_API_KEY=\nSILICONFLOW_API_TOKEN=\n\nSCRAPING_DOG_API_KEY=\n\nSATCOM_SMALL_MODEL_NAME=esa-sceva/satcom-chat-8b\nSATCOM_LARGE_MODEL_NAME=esa-sceva/satcom-chat-70b\nSATCOM_LARGE_BASE_URL=https://api.runpod.ai/v2/zyy9iu4i7vmcxc/openai/v1\nSATCOM_SMALL_BASE_URL=https://api.runpod.ai/v2/ucttr8up9sxh0k/openai/v1\nSATCOM_RUNPOD_API_KEY=\n\nSATCOM_QDRANT_URL=\nSATCOM_QDRANT_API_KEY=\n\nREDIS_URL=\n\nIS_PROD=false\n</code></pre> Variable Required Description / Default <code>QDRANT_URL</code> Yes Base URL for the primary Qdrant instance. <code>QDRANT_API_KEY</code> Yes API key for the primary Qdrant instance. <code>SATCOM_QDRANT_URL</code> No Base URL for the Satcom-specific Qdrant instance that is used when SatcomLLM is selected on staging. <code>SATCOM_QDRANT_API_KEY</code> No API key for the Satcom-specific Qdrant instance that is used when SatcomLLM is selected on staging. <code>MAIN_MODEL_URL</code> Yes OpenAI-compatible URL for the main LLM model (e.g., <code>https://api.runpod.ai/v2/{endpoint_id}/openai/v1</code> or <code>http://localhost:8000/v1</code>). <code>FALLBACK_MODEL_URL</code> Yes OpenAI-compatible URL for the fallback LLM model (e.g., <code>https://api.mistral.ai/v1</code> or any OpenAI-compatible endpoint). <code>MAIN_MODEL_NAME</code> No Model name for the main model (defaults to value in config.yaml). <code>FALLBACK_MODEL_NAME</code> No Model name for the fallback model (defaults to value in config.yaml). <code>MAIN_MODEL_API_KEY</code> No API key for the main model (falls back to <code>RUNPOD_API_KEY</code> if not set). <code>FALLBACK_MODEL_API_KEY</code> No API key for the fallback model (falls back to <code>MISTRAL_API_KEY</code> if not set). <code>MODEL_TIMEOUT</code> Yes Timeout for OpenAI setting <code>EMBEDDING_URL</code> Yes Main Embedding Model(Qwen/Qwen3-Embedding-4B) provider url, OpenAI capatible (e.g., <code>https://api.deepinfra.com/v1/openai</code>) <code>EMBEDDING_API_KEY</code> Yes Main Embedding Model provider API token <code>EMBEDDING_FALLBACK_URL</code> Yes Fallback Embedding Model provider url, OpenAI capatible (e.g., https://api.siliconflow.com/v1) <code>EMBEDDING_FALLBACK_API_KEY</code> Yes Fallback Embedding Model provider API token <code>DEEPINFRA_API_TOKEN</code> Yes DeepInfra API token for reranking retrieved documents. <code>INFERENCE_API_KEY</code> Yes Inference API key for embedding queries, used as a fallback. <code>SILICONFLOW_API_TOKEN</code> Yes SiliconFlow API token for reranking, used as a fallback. <code>SATCOM_RUNPOD_API_KEY</code> No Runpod key dedicated to Satcom workloads. <code>MONGO_HOST</code> Yes MongoDB host (default <code>localhost</code> or <code>mongo</code> in docker). <code>MONGO_PORT</code> Yes MongoDB port (default <code>27017</code>). <code>MONGO_USERNAME</code> No MongoDB username (empty allowed for local). <code>MONGO_PASSWORD</code> No MongoDB password. <code>MONGO_DATABASE</code> Yes MongoDB database name (default <code>eve-backend</code>). <code>MONGO_PARAMS</code> No Extra Mongo connection params (default <code>?authSource=admin</code>). <code>JWT_SECRET_KEY</code> Yes Secret for signing JWTs. <code>JWT_ALGORITHM</code> No JWT algorithm (default <code>HS256</code>). <code>JWT_ACCESS_TOKEN_EXPIRE_MINUTES</code> No Access token lifetime in minutes (default <code>15</code>). <code>JWT_REFRESH_TOKEN_EXPIRE_DAYS</code> No Refresh token lifetime in days (default <code>7</code>). <code>SMTP_HOST</code> No SMTP host (default <code>smtp.gmail.com</code>). <code>SMTP_PORT</code> No SMTP port (default <code>587</code>). <code>SMTP_USERNAME</code> No SMTP username. <code>SMTP_PASSWORD</code> No SMTP password. <code>EMAIL_FROM_ADDRESS</code> No Sender email address (default <code>noreply@eve-ai.com</code>). <code>EMAIL_FROM_NAME</code> No Sender display name (default <code>EVE AI</code>). <code>CORS_ALLOWED_ORIGINS</code> No Comma-separated list of allowed origins (default <code>http://localhost:5173</code>). <code>SCRAPING_DOG_API_KEY</code> No API key for ScrapingDog service, used as fallback of retrieval. <code>SATCOM_SMALL_MODEL_NAME</code> No Model name for Satcom small LLM. <code>SATCOM_LARGE_MODEL_NAME</code> No Model name for Satcom large LLM. <code>SATCOM_RUNPOD_API_KEY</code> No API key for Satcom Runpod workloads. <code>REDIS_URL</code> Yes Redis connection string for pub/sub and cancellations. <code>IS_PROD</code> No Set to <code>true</code> to enable production mode toggles."},{"location":"backend/docs/getting_started/#21-obtaining-api-keys-and-tokens","title":"2.1. Obtaining API Keys and Tokens","text":""},{"location":"backend/docs/getting_started/#qdrant-url-and-api-key","title":"Qdrant URL and API Key","text":"<p>Qdrant Cloud (Recommended):</p> <ol> <li>Sign up for a free account at Qdrant Cloud</li> <li>Create a new cluster</li> <li>Copy the cluster URL (e.g., <code>https://xxxxx-xxxxx-xxxxx.qdrant.io</code>)</li> <li>Navigate to API Keys section and create a new API key</li> <li>Use the cluster URL as <code>QDRANT_URL</code> and the API key as <code>QDRANT_API_KEY</code></li> </ol> <p>Self-hosted Qdrant:</p> <ul> <li>If running Qdrant locally, use <code>http://localhost:6333</code> as <code>QDRANT_URL</code></li> <li>For self-hosted instances, API key may not be required (leave empty or check your Qdrant configuration)</li> </ul> <p>Reference: Qdrant Cloud Documentation</p>"},{"location":"backend/docs/getting_started/#jwt-secret-key","title":"JWT Secret Key","text":"<p>Generate a secure random string for JWT token signing. You can use one of these methods:</p> <p>Using Python:</p> <pre><code>import secrets\nprint(secrets.token_urlsafe(32))\n</code></pre> <p>Using OpenSSL (Linux/Mac):</p> <pre><code>openssl rand -base64 32\n</code></pre> <p>Using PowerShell (Windows):</p> <pre><code>[Convert]::ToBase64String((1..32 | ForEach-Object { Get-Random -Maximum 256 }))\n</code></pre> <p>Online Generator:</p> <ul> <li>Use a secure random string generator like randomkeygen.com</li> <li>Copy a 256-bit key and use it as <code>JWT_SECRET_KEY</code></li> </ul> <p>Important: Keep this key secret and never commit it to version control.</p>"},{"location":"backend/docs/getting_started/#runpod-api-key","title":"Runpod API Key","text":"<ol> <li>Sign up for an account at Runpod</li> <li>Navigate to your account settings</li> <li>Go to the API Keys section</li> <li>Create a new API key</li> <li>Copy the key and use it as <code>RUNPOD_API_KEY</code></li> </ol> <p>Reference: Runpod API Documentation</p>"},{"location":"backend/docs/getting_started/#mistral-api-key","title":"Mistral API Key","text":"<ol> <li>Sign up for an account at Mistral AI</li> <li>Navigate to your account dashboard</li> <li>Go to API Keys section</li> <li>Create a new API key</li> <li>Copy the key and use it as <code>MISTRAL_API_KEY</code></li> </ol> <p>Reference: Mistral AI API Documentation</p>"},{"location":"backend/docs/getting_started/#main-and-fallback-model-urls","title":"Main and Fallback Model URLs","text":"<p>The system uses two LLM models: MAIN (primary) and FALLBACK (backup). Both must be configured with OpenAI-compatible API endpoints.</p> <p>For RunPod endpoints:</p> <pre><code>MAIN_MODEL_URL=https://api.runpod.ai/v2/{endpoint_id}/openai/v1\n</code></pre> <p>Replace <code>{endpoint_id}</code> with your RunPod endpoint ID (e.g., <code>2f9o93xc90871m</code>).</p> <p>For localhost/self-hosted models:</p> <pre><code>MAIN_MODEL_URL=http://localhost:8000/v1\n</code></pre> <p>Use the base URL of your OpenAI-compatible API endpoint.</p> <p>For Mistral (fallback):</p> <pre><code>FALLBACK_MODEL_URL=https://api.mistral.ai/v1\n</code></pre> <p>For other OpenAI-compatible services: Simply use the base URL of the service's OpenAI-compatible endpoint.</p> <p>Note: Both URLs must be in OpenAI-compatible format. The model names can be optionally overridden using <code>MAIN_MODEL_NAME</code> and <code>FALLBACK_MODEL_NAME</code> environment variables, otherwise they default to values in <code>config.yaml</code>.</p>"},{"location":"backend/docs/getting_started/#deepinfra-api-token","title":"DeepInfra API Token","text":"<ol> <li>Sign up for an account at DeepInfra</li> <li>Navigate to your dashboard</li> <li>Go to API Keys section</li> <li>Create a new API token</li> <li>Copy the token and use it as <code>DEEPINFRA_API_TOKEN</code></li> </ol> <p>Reference: DeepInfra API Documentation</p>"},{"location":"backend/docs/getting_started/#inference-api-key","title":"Inference API Key","text":"<p>The <code>INFERENCE_API_KEY</code> is used for embedding queries with the Qwen 3.4B embedding model via Inference.net.</p> <ol> <li>Sign up for an account at Inference.net (you can use GitHub, Google, or email)</li> <li>After registration, you'll be redirected to your dashboard</li> <li>Navigate to the API Keys section</li> <li>Click on \"Create API Key\" to generate a new key</li> <li>Copy the generated API key and use it as <code>INFERENCE_API_KEY</code></li> </ol> <p>Reference: Inference.net Documentation</p>"},{"location":"backend/docs/getting_started/#siliconflow-api-token","title":"SiliconFlow API Token","text":"<ol> <li>Sign up for an account at SiliconFlow</li> <li>Navigate to your account settings</li> <li>Go to API Keys section</li> <li>Create a new API token</li> <li>Copy the token and use it as <code>SILICONFLOW_API_TOKEN</code></li> </ol> <p>Reference: SiliconFlow Documentation</p>"},{"location":"backend/docs/getting_started/#redis-url","title":"Redis URL","text":"<p>Redis Cloud (Recommended):</p> <ol> <li>Sign up for a free account at Redis Cloud</li> <li>Create a new database</li> <li>Copy the connection URL (format: <code>redis://:password@host:port</code>)</li> <li>Use it as <code>REDIS_URL</code></li> </ol> <p>Local Redis: - If running Redis locally: <code>redis://localhost:6379</code> - If Redis has a password: <code>redis://:password@localhost:6379</code></p> <p>Reference: Redis Cloud Documentation</p>"},{"location":"backend/docs/getting_started/#scrapingdog-api-key-optional","title":"ScrapingDog API Key (Optional)","text":"<ol> <li>Sign up for an account at ScrapingDog</li> <li>Navigate to your dashboard</li> <li>Copy your API key</li> <li>Use it as <code>SCRAPING_DOG_API_KEY</code></li> </ol> <p>Reference: ScrapingDog Documentation</p>"},{"location":"backend/docs/getting_started/#3-installation","title":"3. Installation","text":"<pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"backend/docs/getting_started/#4-start-the-server","title":"4. Start the Server","text":"<pre><code>chmod +x start.sh\n./start.sh\n</code></pre> <p>The server will be available at http://localhost:8000/docs.</p>"},{"location":"backend/docs/getting_started/#5-build-and-run-the-containers","title":"5. Build and run the Containers","text":"<p>To run this backend using Docker, fix <code>MONGO_HOST=localhost</code> to <code>MONGO_HOST=mongo</code> in <code>.env</code>.</p> <pre><code>docker compose build\ndocker compose up -d\n</code></pre>"},{"location":"backend/docs/api/routers-auth/","title":"Auth","text":""},{"location":"backend/docs/api/routers-auth/#routers.auth.login","title":"<code>login(request)</code>  <code>async</code>","text":"<p>Authenticate a user and issue JWT tokens.</p> <p>Validates the provided credentials, ensures the account is active, and returns an access and refresh token.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>LoginRequest</code> <p>Login credentials payload.</p> required <p>Returns:</p> Type Description <code>LoginResponse</code> <p>Access and refresh tokens.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>401 if invalid credentials or user not found; 403 if account not activated.</p> Source code in <code>backend/src/routers/auth.py</code> <pre><code>@router.post(\"/login\", response_model=LoginResponse)\nasync def login(request: LoginRequest) -&gt; LoginResponse:\n    \"\"\"\n    Authenticate a user and issue JWT tokens.\n\n    Validates the provided credentials, ensures the account is active, and returns an access and refresh token.\n\n    Args:\n        request (LoginRequest): Login credentials payload.\n\n    Returns:\n        Access and refresh tokens.\n\n    Raises:\n        HTTPException: 401 if invalid credentials or user not found; 403 if account not activated.\n    \"\"\"\n    if not await verify_user(request.email, request.password):\n        raise HTTPException(status_code=401, detail=\"Invalid credentials\")\n\n    user = await User.find_one({\"email\": request.email})\n    if not user:\n        raise HTTPException(status_code=401, detail=\"User not found\")\n    if not user.is_active:\n        raise HTTPException(\n            status_code=403, detail=\"Account not activated. Please check your email.\"\n        )\n\n    return LoginResponse(\n        access_token=create_access_token(sub=user.id),\n        refresh_token=create_refresh_token(sub=user.id),\n    )\n</code></pre>"},{"location":"backend/docs/api/routers-auth/#routers.auth.refresh","title":"<code>refresh(request)</code>  <code>async</code>","text":"<p>Exchange a refresh token for a new access token.</p> <p>Decodes and validates the provided refresh token and returns a new access token if the token and user are valid.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>RefreshRequest</code> <p>Refresh token payload.</p> required <p>Returns:</p> Type Description <code>RefreshResponse</code> <p>Fresh access token.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>401 if invalid refresh token or user not found.</p> Source code in <code>backend/src/routers/auth.py</code> <pre><code>@router.post(\"/refresh\", response_model=RefreshResponse)\nasync def refresh(request: RefreshRequest) -&gt; RefreshResponse:\n    \"\"\"\n    Exchange a refresh token for a new access token.\n\n    Decodes and validates the provided refresh token and returns a new access token if the token and user are valid.\n\n    Args:\n        request (RefreshRequest): Refresh token payload.\n\n    Returns:\n        Fresh access token.\n\n    Raises:\n        HTTPException: 401 if invalid refresh token or user not found.\n    \"\"\"\n    try:\n        payload = jwt.decode(\n            request.refresh_token,\n            JWT_SECRET_KEY,\n            algorithms=[JWT_ALGORITHM],\n            audience=JWT_AUDIENCE_REFRESH,\n        )\n        user_id = payload.get(\"sub\")\n        if not user_id:\n            raise HTTPException(status_code=401, detail=\"Invalid refresh token\")\n\n        # Use find_by_id instead of find_one with _id\n        user = await User.find_by_id(user_id)\n        if not user:\n            raise HTTPException(status_code=401, detail=\"User not found\")\n    except JWTError:\n        raise HTTPException(status_code=401, detail=\"Invalid refresh token\")\n\n    return RefreshResponse(\n        access_token=create_access_token(sub=user.id),\n    )\n</code></pre>"},{"location":"backend/docs/api/routers-auth/#routers.auth.resend_activation","title":"<code>resend_activation(request)</code>  <code>async</code>","text":"<p>Resend the account activation email.</p> <p>Generates a new activation code (if the account is not yet active) and sends the activation email again.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>ResendActivationRequest</code> <p>Email address for the account.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Confirmation message.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>404 if user is not found.</p> Source code in <code>backend/src/routers/auth.py</code> <pre><code>@router.post(\"/resend-activation\")\nasync def resend_activation(request: ResendActivationRequest) -&gt; dict:\n    \"\"\"\n    Resend the account activation email.\n\n    Generates a new activation code (if the account is not yet active) and sends the activation email again.\n\n    Args:\n        request (ResendActivationRequest): Email address for the account.\n\n    Returns:\n        Confirmation message.\n\n    Raises:\n        HTTPException: 404 if user is not found.\n    \"\"\"\n    user = await User.find_one({\"email\": request.email})\n    if not user:\n        raise HTTPException(status_code=404, detail=\"User not found\")\n    if user.is_active:\n        return {\"message\": \"Account already activated.\"}\n    user.activation_code = generate_activation_code()\n    await user.save()\n    verification_url = (\n        f\"{FRONTEND_URL}/verify?email={user.email}&amp;code={user.activation_code}\"\n    )\n    email_service.send_email(\n        to_email=user.email,\n        subject=\"Activate your EVE account\",\n        template_name=\"activation.html\",\n        context={\n            \"verification_url\": verification_url,\n        },\n    )\n    return {\"message\": \"Activation code resent.\"}\n</code></pre>"},{"location":"backend/docs/api/routers-auth/#routers.auth.signup","title":"<code>signup(request)</code>  <code>async</code>","text":"<p>Register a new user and send an activation email.</p> <p>Creates a user account and emails an activation link containing a one-time activation code.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>SignupRequest</code> <p>Signup payload with user details and password.</p> required <p>Returns:</p> Type Description <code>SignupResponse</code> <p>Created user summary.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>400 if invalid or duplicate signup data.</p> Source code in <code>backend/src/routers/auth.py</code> <pre><code>@router.post(\"/signup\", response_model=SignupResponse)\nasync def signup(request: SignupRequest) -&gt; SignupResponse:\n    \"\"\"\n    Register a new user and send an activation email.\n\n    Creates a user account and emails an activation link containing a one-time activation code.\n\n    Args:\n        request (SignupRequest): Signup payload with user details and password.\n\n    Returns:\n        Created user summary.\n\n    Raises:\n        HTTPException: 400 if invalid or duplicate signup data.\n    \"\"\"\n    try:\n        user = await create_user(\n            email=request.email,\n            password=request.password,\n            first_name=request.first_name,\n            last_name=request.last_name,\n        )\n    except ValueError as e:\n        raise HTTPException(status_code=400, detail=str(e))\n    verification_url = (\n        f\"{FRONTEND_URL}/verify?email={user.email}&amp;code={user.activation_code}\"\n    )\n    email_service.send_email(\n        to_email=user.email,\n        subject=\"Activate your EVE account\",\n        template_name=\"activation.html\",\n        context={\n            \"verification_url\": verification_url,\n        },\n    )\n    return SignupResponse(\n        id=user.id,\n        email=user.email,\n        first_name=user.first_name,\n        last_name=user.last_name,\n    )\n</code></pre>"},{"location":"backend/docs/api/routers-auth/#routers.auth.verify","title":"<code>verify(request)</code>  <code>async</code>","text":"<p>Verify account activation using the activation code.</p> <p>Marks the user as active if the provided code matches and clears the code.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>VerifyRequest</code> <p>Verification payload with email and activation code.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Confirmation message.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>404 if user not found; 400 if activation code is invalid.</p> Source code in <code>backend/src/routers/auth.py</code> <pre><code>@router.post(\"/verify\")\nasync def verify(request: VerifyRequest) -&gt; dict:\n    \"\"\"\n    Verify account activation using the activation code.\n\n    Marks the user as active if the provided code matches and clears the code.\n\n    Args:\n        request (VerifyRequest): Verification payload with email and activation code.\n\n    Returns:\n        Confirmation message.\n\n    Raises:\n        HTTPException: 404 if user not found; 400 if activation code is invalid.\n    \"\"\"\n    user = await User.find_one({\"email\": request.email})\n    if not user:\n        raise HTTPException(status_code=404, detail=\"User not found\")\n    if user.is_active:\n        return {\"message\": \"Account already activated.\"}\n    if user.activation_code != request.activation_code:\n        raise HTTPException(status_code=400, detail=\"Invalid activation code\")\n    user.is_active = True\n    user.activation_code = None\n    await user.save()\n    return {\"message\": \"Account activated successfully.\"}\n</code></pre>"},{"location":"backend/docs/api/routers-collection/","title":"Collection","text":""},{"location":"backend/docs/api/routers-collection/#routers.collection.create_collection","title":"<code>create_collection(request, requesting_user=Depends(get_current_user))</code>  <code>async</code>","text":"<p>Create a private collection and provision its backing vector index (Qdrant).</p> <p>The new collection is private to its creator. The MongoDB collection ID is used as the Qdrant collection name.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>CollectionRequest</code> <p>New collection parameters.</p> required <code>requesting_user</code> <code>User</code> <p>Authenticated user injected by dependency.</p> <code>Depends(get_current_user)</code> <p>Returns:</p> Type Description <code>Collection</code> <p>Created collection.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>500 if vector collection creation fails or on server errors.</p> Source code in <code>backend/src/routers/collection.py</code> <pre><code>@router.post(\"/collections\", response_model=Collection)\nasync def create_collection(\n    request: CollectionRequest,\n    requesting_user: User = Depends(get_current_user),\n) -&gt; Collection:\n    \"\"\"\n    Create a private collection and provision its backing vector index (Qdrant).\n\n    The new collection is private to its creator. The MongoDB collection ID is used as the Qdrant collection name.\n\n    Args:\n        request (CollectionRequest): New collection parameters.\n        requesting_user (User): Authenticated user injected by dependency.\n\n    Returns:\n        Created collection.\n\n    Raises:\n        HTTPException: 500 if vector collection creation fails or on server errors.\n    \"\"\"\n    collection = Collection(\n        name=request.name,\n        user_id=requesting_user.id,\n        description=request.description,\n        embeddings_model=request.embeddings_model,\n    )\n    await collection.save()\n\n    try:\n        VectorStoreManager(embeddings_model=request.embeddings_model).create_collection(\n            collection.id\n        )\n    except HTTPException as e:\n        raise e\n    except Exception as e:\n        logger.warning(\n            f\"Warning: failed to create Qdrant collection {collection.id}: {e}\"\n        )\n        await collection.delete()\n        raise HTTPException(\n            status_code=500,\n            detail=f\"Failed to create vector collection: {str(e)}\",\n        )\n\n    return collection\n</code></pre>"},{"location":"backend/docs/api/routers-collection/#routers.collection.delete_collection","title":"<code>delete_collection(collection_id, requesting_user=Depends(get_current_user))</code>  <code>async</code>","text":"<p>Delete a collection and its related resources.</p> <p>Deletes documents in the collection, attempts to remove the vector index, and finally deletes the collection record.</p> <p>Parameters:</p> Name Type Description Default <code>collection_id</code> <code>str</code> <p>Collection identifier.</p> required <code>requesting_user</code> <code>User</code> <p>Authenticated user injected by dependency.</p> <code>Depends(get_current_user)</code> <p>Returns:</p> Type Description <code>dict</code> <p>Confirmation message.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>404 if not found; 403 if deletion is forbidden; 500 for server errors.</p> Source code in <code>backend/src/routers/collection.py</code> <pre><code>@router.delete(\"/collections/{collection_id}\")\nasync def delete_collection(\n    collection_id: str,\n    requesting_user: User = Depends(get_current_user),\n) -&gt; dict:\n    \"\"\"\n    Delete a collection and its related resources.\n\n    Deletes documents in the collection, attempts to remove the vector index, and finally deletes the collection record.\n\n    Args:\n        collection_id (str): Collection identifier.\n        requesting_user (User): Authenticated user injected by dependency.\n\n    Returns:\n        Confirmation message.\n\n    Raises:\n        HTTPException: 404 if not found; 403 if deletion is forbidden; 500 for server errors.\n    \"\"\"\n    try:\n        collection = await Collection.find_by_id(collection_id)\n        if not collection:\n            raise HTTPException(status_code=404, detail=\"Collection not found\")\n        if collection.user_id != requesting_user.id:\n            raise HTTPException(\n                status_code=403,\n                detail=\"You are not allowed to delete this collection\",\n            )\n\n        await Document.delete_many({\"collection_id\": collection_id})\n\n        try:\n            await anyio.to_thread.run_sync(\n                vector_store.delete_collection, collection_id\n            )\n        except Exception as e:\n            logger.warning(\n                f\"Warning: failed to delete Qdrant collection {collection_id}: {e}\"\n            )\n\n        await collection.delete()\n        return {\"message\": \"Collection deleted successfully\"}\n\n    except HTTPException as e:\n        raise e\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Server error: {str(e)}\")\n</code></pre>"},{"location":"backend/docs/api/routers-collection/#routers.collection.get_collection","title":"<code>get_collection(collection_id)</code>  <code>async</code>","text":"<p>Get a collection by id with document and vector counts.</p> <p>Parameters:</p> Name Type Description Default <code>collection_id</code> <code>str</code> <p>Target collection identifier.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Collection data including documents_count and points_count.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>404 if collection is not found.</p> Source code in <code>backend/src/routers/collection.py</code> <pre><code>@router.get(\"/collections/{collection_id}\")\nasync def get_collection(collection_id: str) -&gt; dict:\n    \"\"\"\n    Get a collection by id with document and vector counts.\n\n    Args:\n        collection_id (str): Target collection identifier.\n\n    Returns:\n        Collection data including documents_count and points_count.\n\n    Raises:\n        HTTPException: 404 if collection is not found.\n    \"\"\"\n    collection = await Collection.find_by_id(collection_id)\n    if not collection:\n        raise HTTPException(status_code=404, detail=\"Collection not found\")\n\n    documents_count, points_count = await _get_counts_for_id(collection_id)\n\n    return {\n        **collection.dict(),\n        \"documents_count\": documents_count,\n        \"points_count\": points_count,\n    }\n</code></pre>"},{"location":"backend/docs/api/routers-collection/#routers.collection.list_collections","title":"<code>list_collections(request=Depends(), request_user=Depends(get_current_user))</code>  <code>async</code>","text":"<p>List collections owned by the current user.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Pagination</code> <p>Pagination parameters.</p> <code>Depends()</code> <code>request_user</code> <code>User</code> <p>Authenticated user injected by dependency.</p> <code>Depends(get_current_user)</code> <p>Returns:</p> Type Description <code>PaginatedResponse[Collection]</code> <p>Paginated list of user collections.</p> Source code in <code>backend/src/routers/collection.py</code> <pre><code>@router.get(\"/collections\", response_model=PaginatedResponse[Collection])\nasync def list_collections(\n    request: Pagination = Depends(), request_user: User = Depends(get_current_user)\n) -&gt; PaginatedResponse[Collection]:\n    \"\"\"\n    List collections owned by the current user.\n\n    Args:\n        request (Pagination): Pagination parameters.\n        request_user (User): Authenticated user injected by dependency.\n\n    Returns:\n        Paginated list of user collections.\n    \"\"\"\n    return await Collection.find_all_with_pagination(\n        limit=request.limit,\n        page=request.page,\n        filter_dict={\"user_id\": request_user.id},\n        sort=[(\"timestamp\", -1)],\n    )\n</code></pre>"},{"location":"backend/docs/api/routers-collection/#routers.collection.list_public_collections","title":"<code>list_public_collections(pagination=Depends())</code>  <code>async</code>","text":"<p>List public collections with pagination.</p> <p>Combines platform-curated public collections and environment-specific public collections, then paginates the combined list.</p> <p>Parameters:</p> Name Type Description Default <code>pagination</code> <code>Pagination</code> <p>Pagination parameters.</p> <code>Depends()</code> <p>Returns:</p> Type Description <code>PaginatedResponse[Collection]</code> <p>Paginated list of public collections.</p> Source code in <code>backend/src/routers/collection.py</code> <pre><code>@router.get(\"/collections/public\", response_model=PaginatedResponse[Collection])\nasync def list_public_collections(pagination: Pagination = Depends()) -&gt; PaginatedResponse[Collection]:\n    \"\"\"\n    List public collections with pagination.\n\n    Combines platform-curated public collections and environment-specific public collections, then paginates the combined list.\n\n    Args:\n        pagination (Pagination): Pagination parameters.\n\n    Returns:\n        Paginated list of public collections.\n    \"\"\"\n    public_collections, total_count = await vector_store.list_public_collections(\n        page=pagination.page, limit=pagination.limit\n    )\n\n    if IS_PROD:\n        public_collections = WILEY_PUBLIC_COLLECTIONS + public_collections\n    else:\n        public_collections = WILEY_PUBLIC_COLLECTIONS + STAGING_PUBLIC_COLLECTIONS\n    total_count = len(public_collections)\n    # Pagination must be done manually since Qdrant doesn't support collection pagination\n    return PaginatedResponse(\n        data=[\n            Collection(\n                id=collection[\"name\"],\n                name=collection.get(\"alias\") or collection[\"name\"],\n                description=collection[\"description\"],\n                user_id=None,\n                embeddings_model=DEFAULT_EMBEDDING_MODEL,\n            )\n            for collection in public_collections\n        ],\n        meta=get_pagination_metadata(total_count, pagination.page, pagination.limit),\n    )\n</code></pre>"},{"location":"backend/docs/api/routers-collection/#routers.collection.update_collection","title":"<code>update_collection(request, collection_id, requesting_user=Depends(get_current_user))</code>  <code>async</code>","text":"<p>Update a collection's mutable fields.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>CollectionUpdate</code> <p>Update payload (e.g., name).</p> required <code>collection_id</code> <code>str</code> <p>Collection identifier.</p> required <code>requesting_user</code> <code>User</code> <p>Authenticated user injected by dependency.</p> <code>Depends(get_current_user)</code> <p>Returns:</p> Type Description <code>Collection</code> <p>Updated collection.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>404 if not found; 403 if update is forbidden; 500 for server errors.</p> Source code in <code>backend/src/routers/collection.py</code> <pre><code>@router.patch(\"/collections/{collection_id}\")\nasync def update_collection(\n    request: CollectionUpdate,\n    collection_id: str,\n    requesting_user: User = Depends(get_current_user),\n) -&gt; Collection:\n    \"\"\"\n    Update a collection's mutable fields.\n\n    Args:\n        request (CollectionUpdate): Update payload (e.g., name).\n        collection_id (str): Collection identifier.\n        requesting_user (User): Authenticated user injected by dependency.\n\n    Returns:\n        Updated collection.\n\n    Raises:\n        HTTPException: 404 if not found; 403 if update is forbidden; 500 for server errors.\n    \"\"\"\n    try:\n        collection = await Collection.find_by_id(collection_id)\n        if not collection:\n            raise HTTPException(status_code=404, detail=\"Collection not found\")\n\n        if collection.user_id != requesting_user.id:\n            raise HTTPException(\n                status_code=403,\n                detail=\"You are not allowed to update this collection\",\n            )\n\n        collection.name = request.name\n        updated_collection = await collection.save()\n        return updated_collection\n\n    except HTTPException as e:\n        raise e\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Server error: {str(e)}\")\n</code></pre>"},{"location":"backend/docs/api/routers-conversation/","title":"Conversation","text":""},{"location":"backend/docs/api/routers-conversation/#routers.conversation.create_conversation","title":"<code>create_conversation(request, requesting_user=Depends(get_current_user))</code>  <code>async</code>","text":"<p>Create a new conversation for the current user.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>ConversationCreate</code> <p>New conversation payload.</p> required <code>requesting_user</code> <code>User</code> <p>Authenticated user injected by dependency.</p> <code>Depends(get_current_user)</code> <p>Returns:</p> Type Description <code>Conversation</code> <p>Created conversation.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>500 for server errors.</p> Source code in <code>backend/src/routers/conversation.py</code> <pre><code>@router.post(\"/conversations\", response_model=Conversation)\nasync def create_conversation(\n    request: ConversationCreate,\n    requesting_user: User = Depends(get_current_user),\n) -&gt; Conversation:\n    \"\"\"\n    Create a new conversation for the current user.\n\n    Args:\n        request (ConversationCreate): New conversation payload.\n        requesting_user (User): Authenticated user injected by dependency.\n\n    Returns:\n        Created conversation.\n\n    Raises:\n        HTTPException: 500 for server errors.\n    \"\"\"\n    try:\n        return await Conversation.create(\n            user_id=requesting_user.id,\n            name=request.name,\n        )\n\n    except HTTPException as e:\n        raise e\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Server error: {str(e)}\")\n</code></pre>"},{"location":"backend/docs/api/routers-conversation/#routers.conversation.delete_conversation","title":"<code>delete_conversation(conversation_id, requesting_user=Depends(get_current_user))</code>  <code>async</code>","text":"<p>Delete a conversation and its messages.</p> <p>Parameters:</p> Name Type Description Default <code>conversation_id</code> <code>str</code> <p>Conversation identifier.</p> required <code>requesting_user</code> <code>User</code> <p>Authenticated user injected by dependency.</p> <code>Depends(get_current_user)</code> <p>Returns:</p> Type Description <code>dict</code> <p>Confirmation message.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>404 if not found; 403 if deletion is forbidden; 500 for server errors.</p> Source code in <code>backend/src/routers/conversation.py</code> <pre><code>@router.delete(\"/conversations/{conversation_id}\")\nasync def delete_conversation(\n    conversation_id: str,\n    requesting_user: User = Depends(get_current_user),\n) -&gt; dict:\n    \"\"\"\n    Delete a conversation and its messages.\n\n    Args:\n        conversation_id (str): Conversation identifier.\n        requesting_user (User): Authenticated user injected by dependency.\n\n    Returns:\n        Confirmation message.\n\n    Raises:\n        HTTPException: 404 if not found; 403 if deletion is forbidden; 500 for server errors.\n    \"\"\"\n    try:\n        conversation = await Conversation.find_by_id(conversation_id)\n        if not conversation:\n            raise HTTPException(status_code=404, detail=\"Conversation not found\")\n        if conversation.user_id != requesting_user.id:\n            raise HTTPException(\n                status_code=403,\n                detail=\"You are not allowed to delete this conversation\",\n            )\n\n        await Message.delete_many({\"conversation_id\": conversation_id})\n        await conversation.delete()\n        return {\"message\": \"Conversation deleted successfully\"}\n\n    except HTTPException as e:\n        raise e\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Server error: {str(e)}\")\n</code></pre>"},{"location":"backend/docs/api/routers-conversation/#routers.conversation.get_conversation","title":"<code>get_conversation(conversation_id, requesting_user=Depends(get_current_user))</code>  <code>async</code>","text":"<p>Get a conversation and its messages.</p> <p>Parameters:</p> Name Type Description Default <code>conversation_id</code> <code>str</code> <p>Conversation identifier.</p> required <code>requesting_user</code> <code>User</code> <p>Authenticated user injected by dependency.</p> <code>Depends(get_current_user)</code> <p>Returns:</p> Type Description <code>ConversationDetail</code> <p>Conversation with messages and metadata.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>404 if not found; 403 if access is forbidden; 500 for server errors.</p> Source code in <code>backend/src/routers/conversation.py</code> <pre><code>@router.get(\"/conversations/{conversation_id}\", response_model=ConversationDetail)\nasync def get_conversation(\n    conversation_id: str,\n    requesting_user: User = Depends(get_current_user),\n) -&gt; ConversationDetail:\n    \"\"\"\n    Get a conversation and its messages.\n\n    Args:\n        conversation_id (str): Conversation identifier.\n        requesting_user (User): Authenticated user injected by dependency.\n\n    Returns:\n        Conversation with messages and metadata.\n\n    Raises:\n        HTTPException: 404 if not found; 403 if access is forbidden; 500 for server errors.\n    \"\"\"\n    try:\n        conversation = await Conversation.find_by_id(conversation_id)\n        if not conversation:\n            raise HTTPException(status_code=404, detail=\"Conversation not found\")\n\n        if conversation.user_id != requesting_user.id:\n            raise HTTPException(\n                status_code=403,\n                detail=\"You are not allowed to get this conversation\",\n            )\n\n        # Fetch messages for this conversation\n        messages = await Message.find_all(\n            filter_dict={\"conversation_id\": conversation_id}, sort=[(\"timestamp\", 1)]\n        )\n\n        return ConversationDetail(\n            id=conversation.id,\n            user_id=conversation.user_id,\n            name=conversation.name,\n            timestamp=conversation.timestamp,\n            messages=messages,\n        )\n\n    except HTTPException as e:\n        raise e\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Server error: {str(e)}\")\n</code></pre>"},{"location":"backend/docs/api/routers-conversation/#routers.conversation.list_conversations","title":"<code>list_conversations(request_user=Depends(get_current_user), pagination=Depends())</code>  <code>async</code>","text":"<p>List conversations owned by the current user.</p> <p>Parameters:</p> Name Type Description Default <code>request_user</code> <code>User</code> <p>Authenticated user injected by dependency.</p> <code>Depends(get_current_user)</code> <code>pagination</code> <code>Pagination</code> <p>Pagination parameters.</p> <code>Depends()</code> <p>Returns:</p> Type Description <code>PaginatedResponse[Conversation]</code> <p>Paginated conversations for the user.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>500 for server errors.</p> Source code in <code>backend/src/routers/conversation.py</code> <pre><code>@router.get(\"/conversations\", response_model=PaginatedResponse[Conversation])\nasync def list_conversations(\n    request_user: User = Depends(get_current_user), pagination: Pagination = Depends()\n) -&gt; PaginatedResponse[Conversation]:\n    \"\"\"\n    List conversations owned by the current user.\n\n    Args:\n        request_user (User): Authenticated user injected by dependency.\n        pagination (Pagination): Pagination parameters.\n\n    Returns:\n        Paginated conversations for the user.\n\n    Raises:\n        HTTPException: 500 for server errors.\n    \"\"\"\n    try:\n        result = await Conversation.find_all_with_pagination(\n            filter_dict={\"user_id\": request_user.id},\n            page=pagination.page,\n            limit=pagination.limit,\n            sort=[(\"timestamp\", -1)],\n        )\n\n        return result\n\n    except HTTPException as e:\n        raise e\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Server error: {str(e)}\")\n</code></pre>"},{"location":"backend/docs/api/routers-conversation/#routers.conversation.update_conversation_name","title":"<code>update_conversation_name(conversation_id, request, requesting_user=Depends(get_current_user))</code>  <code>async</code>","text":"<p>Update a conversation's name.</p> <p>Parameters:</p> Name Type Description Default <code>conversation_id</code> <code>str</code> <p>Conversation identifier.</p> required <code>request</code> <code>ConversationNameUpdate</code> <p>New name payload.</p> required <code>requesting_user</code> <code>User</code> <p>Authenticated user injected by dependency.</p> <code>Depends(get_current_user)</code> <p>Returns:</p> Type Description <code>Conversation</code> <p>Updated conversation.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>404 if not found; 403 if update is forbidden; 500 for server errors.</p> Source code in <code>backend/src/routers/conversation.py</code> <pre><code>@router.patch(\"/conversations/{conversation_id}\")\nasync def update_conversation_name(\n    conversation_id: str,\n    request: ConversationNameUpdate,\n    requesting_user: User = Depends(get_current_user),\n) -&gt; Conversation:\n    \"\"\"\n    Update a conversation's name.\n\n    Args:\n        conversation_id (str): Conversation identifier.\n        request (ConversationNameUpdate): New name payload.\n        requesting_user (User): Authenticated user injected by dependency.\n\n    Returns:\n        Updated conversation.\n\n    Raises:\n        HTTPException: 404 if not found; 403 if update is forbidden; 500 for server errors.\n    \"\"\"\n    try:\n        conversation = await Conversation.find_by_id(conversation_id)\n        if not conversation:\n            raise HTTPException(status_code=404, detail=\"Conversation not found\")\n\n        if conversation.user_id != requesting_user.id:\n            raise HTTPException(\n                status_code=403,\n                detail=\"You are not allowed to update this conversation\",\n            )\n\n        conversation.name = request.name\n        await conversation.save()\n\n        return conversation\n\n    except HTTPException as e:\n        raise e\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Server error: {str(e)}\")\n</code></pre>"},{"location":"backend/docs/api/routers-document/","title":"Document","text":"<p>RESTful document endpoints for collections.</p>"},{"location":"backend/docs/api/routers-document/#routers.document.delete_document","title":"<code>delete_document(collection_id=Path(..., description='Collection ID'), document_id=Path(..., description='Document ID'), requesting_user=Depends(get_current_user))</code>  <code>async</code>","text":"<p>Delete a document from a collection.</p> <p>Removes the document record and attempts to delete associated vectors.</p> <p>Parameters:</p> Name Type Description Default <code>collection_id</code> <code>str</code> <p>Collection identifier.</p> <code>Path(..., description='Collection ID')</code> <code>document_id</code> <code>str</code> <p>Document identifier.</p> <code>Path(..., description='Document ID')</code> <code>requesting_user</code> <code>User</code> <p>Authenticated user injected by dependency.</p> <code>Depends(get_current_user)</code> <p>Returns:</p> Type Description <code>dict</code> <p>Confirmation message.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>404 if not found; 400 if document not in collection; 403 if deletion is forbidden.</p> Source code in <code>backend/src/routers/document.py</code> <pre><code>@router.delete(\"/collections/{collection_id}/documents/{document_id}\")\nasync def delete_document(\n    collection_id: str = Path(..., description=\"Collection ID\"),\n    document_id: str = Path(..., description=\"Document ID\"),\n    requesting_user: User = Depends(get_current_user),\n) -&gt; dict:\n    \"\"\"\n    Delete a document from a collection.\n\n    Removes the document record and attempts to delete associated vectors.\n\n    Args:\n        collection_id (str): Collection identifier.\n        document_id (str): Document identifier.\n        requesting_user (User): Authenticated user injected by dependency.\n\n    Returns:\n        Confirmation message.\n\n    Raises:\n        HTTPException: 404 if not found; 400 if document not in collection; 403 if deletion is forbidden.\n    \"\"\"\n    await get_collection_and_validate_ownership(collection_id, requesting_user)\n\n    document = await DocumentModel.find_by_id(document_id)\n    if not document:\n        raise HTTPException(status_code=404, detail=\"Document not found\")\n\n    if document.collection_id != collection_id:\n        raise HTTPException(\n            status_code=400, detail=\"Document does not belong to this collection\"\n        )\n\n    if document.user_id != requesting_user.id:\n        raise HTTPException(\n            status_code=403, detail=\"You are not allowed to delete this document\"\n        )\n\n    vector_store = VectorStoreManager()\n    try:\n        vector_store.delete_docs_by_metadata_filter(\n            collection_name=collection_id,\n            metadata={\"document_id\": document_id},\n        )\n    except HTTPException as e:\n        raise e\n    except Exception as e:\n        logger.error(f\"Failed to delete vectors for document {document_id}: {e}\")\n\n    await document.delete()\n    return {\"message\": \"Document and embeddings deleted successfully\"}\n</code></pre>"},{"location":"backend/docs/api/routers-document/#routers.document.get_collection_and_validate_ownership","title":"<code>get_collection_and_validate_ownership(collection_id, requesting_user)</code>  <code>async</code>","text":"<p>Get collection and validate user ownership.</p> Source code in <code>backend/src/routers/document.py</code> <pre><code>async def get_collection_and_validate_ownership(\n    collection_id: str, requesting_user: User\n) -&gt; Collection:\n    \"\"\"Get collection and validate user ownership.\"\"\"\n    collection = await Collection.find_by_id(collection_id)\n    if not collection:\n        raise HTTPException(status_code=404, detail=\"Collection not found\")\n\n    if collection.user_id != requesting_user.id:\n        raise HTTPException(\n            status_code=403, detail=\"You are not allowed to access this collection\"\n        )\n\n    return collection\n</code></pre>"},{"location":"backend/docs/api/routers-document/#routers.document.get_document","title":"<code>get_document(collection_id=Path(..., description='Collection ID'), document_id=Path(..., description='Document ID'), requesting_user=Depends(get_current_user))</code>  <code>async</code>","text":"<p>Get a specific document from a collection.</p> <p>Parameters:</p> Name Type Description Default <code>collection_id</code> <code>str</code> <p>Collection identifier.</p> <code>Path(..., description='Collection ID')</code> <code>document_id</code> <code>str</code> <p>Document identifier.</p> <code>Path(..., description='Document ID')</code> <code>requesting_user</code> <code>User</code> <p>Authenticated user injected by dependency.</p> <code>Depends(get_current_user)</code> <p>Returns:</p> Type Description <code>Document</code> <p>Document details.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>404 if not found; 400 if document not in collection; 403 if access is forbidden.</p> Source code in <code>backend/src/routers/document.py</code> <pre><code>@router.get(\n    \"/collections/{collection_id}/documents/{document_id}\", response_model=DocumentModel\n)\nasync def get_document(\n    collection_id: str = Path(..., description=\"Collection ID\"),\n    document_id: str = Path(..., description=\"Document ID\"),\n    requesting_user: User = Depends(get_current_user),\n) -&gt; DocumentModel:\n    \"\"\"\n    Get a specific document from a collection.\n\n    Args:\n        collection_id (str): Collection identifier.\n        document_id (str): Document identifier.\n        requesting_user (User): Authenticated user injected by dependency.\n\n    Returns:\n        Document details.\n\n    Raises:\n        HTTPException: 404 if not found; 400 if document not in collection; 403 if access is forbidden.\n    \"\"\"\n    await get_collection_and_validate_ownership(collection_id, requesting_user)\n\n    document = await DocumentModel.find_by_id(document_id)\n    if not document:\n        raise HTTPException(status_code=404, detail=\"Document not found\")\n\n    if document.collection_id != collection_id:\n        raise HTTPException(\n            status_code=400, detail=\"Document does not belong to this collection\"\n        )\n\n    if document.user_id != requesting_user.id:\n        raise HTTPException(\n            status_code=403, detail=\"You are not allowed to access this document\"\n        )\n\n    return document\n</code></pre>"},{"location":"backend/docs/api/routers-document/#routers.document.list_documents","title":"<code>list_documents(collection_id=Path(..., description='Collection ID'), pagination=Depends(), requesting_user=Depends(get_current_user))</code>  <code>async</code>","text":"<p>List documents in a collection.</p> <p>Parameters:</p> Name Type Description Default <code>collection_id</code> <code>str</code> <p>Collection identifier.</p> <code>Path(..., description='Collection ID')</code> <code>pagination</code> <code>Pagination</code> <p>Pagination parameters.</p> <code>Depends()</code> <code>requesting_user</code> <code>User</code> <p>Authenticated user injected by dependency.</p> <code>Depends(get_current_user)</code> <p>Returns:</p> Type Description <code>PaginatedResponse[Document]</code> <p>Paginated documents for the collection.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>404 if collection is not found; 403 if access is forbidden.</p> Source code in <code>backend/src/routers/document.py</code> <pre><code>@router.get(\n    \"/collections/{collection_id}/documents\",\n    response_model=PaginatedResponse[DocumentModel],\n)\nasync def list_documents(\n    collection_id: str = Path(..., description=\"Collection ID\"),\n    pagination: Pagination = Depends(),\n    requesting_user: User = Depends(get_current_user),\n) -&gt; PaginatedResponse[DocumentModel]:\n    \"\"\"\n    List documents in a collection.\n\n    Args:\n        collection_id (str): Collection identifier.\n        pagination (Pagination): Pagination parameters.\n        requesting_user (User): Authenticated user injected by dependency.\n\n    Returns:\n        Paginated documents for the collection.\n\n    Raises:\n        HTTPException: 404 if collection is not found; 403 if access is forbidden.\n    \"\"\"\n    await get_collection_and_validate_ownership(collection_id, requesting_user)\n\n    return await DocumentModel.find_all_with_pagination(\n        filter_dict={\"collection_id\": collection_id},\n        limit=pagination.limit,\n        page=pagination.page,\n        sort=[(\"timestamp\", -1)],\n    )\n</code></pre>"},{"location":"backend/docs/api/routers-document/#routers.document.upload_documents","title":"<code>upload_documents(collection_id=Path(..., description='Collection ID'), files=File(...), metadata_urls=Form(default=None), metadata_names=Form(default=None), embeddings_model=Form(default=DEFAULT_EMBEDDING_MODEL), chunk_size=Form(default=DEFAULT_CHUNK_SIZE), chunk_overlap=Form(default=DEFAULT_CHUNK_OVERLAP), requesting_user=Depends(get_current_user))</code>  <code>async</code>","text":"<p>Upload documents to a collection.</p> <p>Stores document records and triggers asynchronous parsing, chunking, and vectorization for retrieval.</p> <p>Parameters:</p> Name Type Description Default <code>collection_id</code> <code>str</code> <p>Collection identifier.</p> <code>Path(..., description='Collection ID')</code> <code>files</code> <code>list[UploadFile]</code> <p>One or more files to ingest.</p> <code>File(...)</code> <code>metadata_urls</code> <code>list[str] | str | None</code> <p>Optional list or single URL per file.</p> <code>Form(default=None)</code> <code>metadata_names</code> <code>list[str] | str | None</code> <p>Optional list or single display name per file.</p> <code>Form(default=None)</code> <code>embeddings_model</code> <code>str</code> <p>Embeddings model to use for vectorization.</p> <code>Form(default=DEFAULT_EMBEDDING_MODEL)</code> <code>chunk_size</code> <code>int</code> <p>Chunk size for splitting documents.</p> <code>Form(default=DEFAULT_CHUNK_SIZE)</code> <code>chunk_overlap</code> <code>int</code> <p>Overlap between chunks.</p> <code>Form(default=DEFAULT_CHUNK_OVERLAP)</code> <code>requesting_user</code> <code>User</code> <p>Authenticated user injected by dependency.</p> <code>Depends(get_current_user)</code> <p>Returns:</p> Type Description <code>dict</code> <p>Service response with ingestion details.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>404 if collection is not found; 403 if access is forbidden; 500 for processing errors.</p> Source code in <code>backend/src/routers/document.py</code> <pre><code>@router.post(\"/collections/{collection_id}/documents\")\nasync def upload_documents(\n    collection_id: str = Path(..., description=\"Collection ID\"),\n    files: List[UploadFile] = File(...),\n    metadata_urls: Optional[List[str] | str] = Form(default=None),\n    metadata_names: Optional[List[str] | str] = Form(default=None),\n    embeddings_model: str = Form(default=DEFAULT_EMBEDDING_MODEL),\n    chunk_size: int = Form(default=DEFAULT_CHUNK_SIZE),\n    chunk_overlap: int = Form(default=DEFAULT_CHUNK_OVERLAP),\n    requesting_user: User = Depends(get_current_user),\n) -&gt; dict:\n    \"\"\"\n    Upload documents to a collection.\n\n    Stores document records and triggers asynchronous parsing, chunking, and vectorization for retrieval.\n\n    Args:\n        collection_id (str): Collection identifier.\n        files (list[UploadFile]): One or more files to ingest.\n        metadata_urls (list[str] | str | None): Optional list or single URL per file.\n        metadata_names (list[str] | str | None): Optional list or single display name per file.\n        embeddings_model (str): Embeddings model to use for vectorization.\n        chunk_size (int): Chunk size for splitting documents.\n        chunk_overlap (int): Overlap between chunks.\n        requesting_user (User): Authenticated user injected by dependency.\n\n    Returns:\n        Service response with ingestion details.\n\n    Raises:\n        HTTPException: 404 if collection is not found; 403 if access is forbidden; 500 for processing errors.\n    \"\"\"\n    collection = await get_collection_and_validate_ownership(\n        collection_id, requesting_user\n    )\n\n    logger.info(\n        f\"Received {len(files)} files for processing in collection {collection_id}\"\n    )\n\n    docs_data = [\n        DocumentModel(\n            user_id=requesting_user.id,\n            collection_id=collection_id,\n            name=file.filename,\n            filename=file.filename,\n            file_type=os.path.splitext(file.filename)[1].lstrip(\".\"),\n            source_url=metadata_urls[i] if metadata_urls else None,\n        )\n        for i, file in enumerate(files)\n    ]\n\n    try:\n        effective_model = collection.embeddings_model or embeddings_model\n        result = await document_service.add_documents(\n            collection_name=collection_id,\n            files=files,\n            request=AddDocumentRequest(\n                embeddings_model=effective_model,\n                chunk_size=chunk_size,\n                chunk_overlap=chunk_overlap,\n                metadata_urls=metadata_urls,\n                metadata_names=metadata_names,\n            ),\n            metadata_urls=metadata_urls,\n            metadata_names=metadata_names,\n        )\n\n        if not result.success:\n            raise HTTPException(status_code=500, detail=result.error)\n\n        await DocumentModel.bulk_create(docs_data)\n        return result.data\n    except HTTPException as e:\n        raise e\n    except Exception as e:\n        logger.error(f\"Error processing documents: {str(e)}\", exc_info=True)\n        raise HTTPException(\n            status_code=500, detail=f\"Error processing documents: {str(e)}\"\n        )\n</code></pre>"},{"location":"backend/docs/api/routers-forgot_password/","title":"Forgot Password","text":""},{"location":"backend/docs/api/routers-forgot_password/#routers.forgot_password.confirm_new_password","title":"<code>confirm_new_password(request)</code>  <code>async</code>","text":"<p>Confirm a password reset with the provided code and new password.</p> <p>Validates the reset code and its expiry, ensures password confirmation matches, updates the user's password, and invalidates the code.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>ForgotPasswordConfirmation</code> <p>Confirmation payload with code and new password.</p> required <p>Returns:</p> Type Description <p>Dict[str, str]: Confirmation message.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>404 if code is invalid or expired, or user not found.</p> <code>HTTPException</code> <p>400 if passwords do not match.</p> Source code in <code>backend/src/routers/forgot_password.py</code> <pre><code>@router.post(\"/confirm\", response_model=Dict[str, str])\nasync def confirm_new_password(request: ForgotPasswordConfirmation):\n    \"\"\"Confirm a password reset with the provided code and new password.\n\n    Validates the reset code and its expiry, ensures password confirmation\n    matches, updates the user's password, and invalidates the code.\n\n    Args:\n        request (ForgotPasswordConfirmation): Confirmation payload with code and new password.\n\n    Returns:\n        Dict[str, str]: Confirmation message.\n\n    Raises:\n        HTTPException: 404 if code is invalid or expired, or user not found.\n        HTTPException: 400 if passwords do not match.\n    \"\"\"\n    forgot_password = await ForgotPassword.find_one({\"code\": request.code})\n    if not forgot_password:\n        raise HTTPException(status_code=404, detail=\"Invalid code\")\n\n    current_time = datetime.now(timezone.utc)\n    expires_at = forgot_password.expires_at\n\n    if expires_at.tzinfo is None:\n        expires_at = expires_at.replace(tzinfo=timezone.utc)\n\n    if not forgot_password or expires_at &lt; current_time:\n        raise HTTPException(status_code=404, detail=\"Invalid code\")\n\n    if request.new_password != request.confirm_password:\n        raise HTTPException(status_code=400, detail=\"Passwords do not match\")\n\n    user = await User.find_one({\"email\": forgot_password.email})\n    if not user:\n        raise HTTPException(status_code=404, detail=\"User not found\")\n\n    user.password_hash = hash_password(request.new_password)\n    await user.save()\n    await forgot_password.delete()\n    return {\"message\": \"Password changed\"}\n</code></pre>"},{"location":"backend/docs/api/routers-forgot_password/#routers.forgot_password.send_forgot_password_code","title":"<code>send_forgot_password_code(request)</code>  <code>async</code>","text":"<p>Send a password reset code to the user's email.</p> <p>Generates a one-time code, stores it with an expiry, and emails a reset link.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>ForgotPasswordRequest</code> <p>Request containing the user's email.</p> required <p>Returns:</p> Type Description <p>Dict[str, str]: Confirmation message.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>404 if user not found.</p> Source code in <code>backend/src/routers/forgot_password.py</code> <pre><code>@router.post(\"/code\", response_model=Dict[str, str])\nasync def send_forgot_password_code(request: ForgotPasswordRequest):\n    \"\"\"Send a password reset code to the user's email.\n\n    Generates a one-time code, stores it with an expiry, and emails a reset link.\n\n    Args:\n        request (ForgotPasswordRequest): Request containing the user's email.\n\n    Returns:\n        Dict[str, str]: Confirmation message.\n\n    Raises:\n        HTTPException: 404 if user not found.\n    \"\"\"\n    user = await User.find_one({\"email\": request.email})\n    if not user:\n        raise HTTPException(status_code=404, detail=\"User not found\")\n\n    code = \"\".join(random.choices(string.ascii_uppercase + string.digits, k=6))\n\n    already_exists = await ForgotPassword.find_one({\"email\": request.email})\n    if already_exists:\n        already_exists.code = code\n        already_exists.expires_at = datetime.now(timezone.utc) + timedelta(\n            minutes=FORGOT_PASSWORD_CODE_EXPIRE_MINUTES\n        )\n        await already_exists.save()\n\n    reset_url = f\"{FRONTEND_URL}/reset-password?code={code}\"\n    email_service.send_email(\n        to_email=request.email,\n        subject=\"Forgot Password\",\n        template_name=\"forgot_password.html\",\n        context={\n            \"reset_url\": reset_url,\n            \"expiry_minutes\": FORGOT_PASSWORD_CODE_EXPIRE_MINUTES,\n        },\n    )\n\n    await ForgotPassword.create(email=request.email, code=code)\n\n    return {\"message\": \"Code sent\"}\n</code></pre>"},{"location":"backend/docs/api/routers-health_check/","title":"Health Check","text":""},{"location":"backend/docs/api/routers-health_check/#routers.health_check.health_check","title":"<code>health_check()</code>","text":"<p>Liveness probe endpoint.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>Static status payload indicating service health.</p> Source code in <code>backend/src/routers/health_check.py</code> <pre><code>@router.get(\"/health\")\ndef health_check():\n    \"\"\"Liveness probe endpoint.\n\n    Returns:\n        dict: Static status payload indicating service health.\n    \"\"\"\n    return {\"status\": \"healthy\"}\n</code></pre>"},{"location":"backend/docs/api/routers-message/","title":"Message","text":""},{"location":"backend/docs/api/routers-message/#routers.message.build_equivalence_sentence","title":"<code>build_equivalence_sentence(usage_kg)</code>  <code>async</code>","text":"<p>Build an equivalence sentence for CO2 usage.</p> Source code in <code>backend/src/routers/message.py</code> <pre><code>async def build_equivalence_sentence(usage_kg: float) -&gt; CO2EquivalenceResult:\n    \"\"\"Build an equivalence sentence for CO2 usage.\"\"\"\n    doc = await closest_with_direction(usage_kg)\n    if not doc:\n        raise RuntimeError(\"No comparison items in DB\")\n    base = doc.co2eq_kg\n    title = doc.title\n\n    if base == 0:\n        text = f\"\"\n        return CO2EquivalenceResult(\n            title=title,\n            co2eq_kg=base,\n            equivalent_count=None,\n            text=text\n        )\n\n    count = max(1, int(round(usage_kg / base)))\n    unit = pluralize(count, doc.unit_singular, doc.unit_plural)\n\n    text = f\"This is equivalent to: {count} {unit}\"\n\n    return CO2EquivalenceResult(\n        title=title,\n        co2eq_kg=base,\n        equivalent_count=count,\n        text=text\n    )\n</code></pre>"},{"location":"backend/docs/api/routers-message/#routers.message.closest_with_direction","title":"<code>closest_with_direction(usage_kg)</code>  <code>async</code>","text":"<p>Returns either lower bound or upper bound if lower is missing (usage smaller than min &gt;0).</p> Source code in <code>backend/src/routers/message.py</code> <pre><code>async def closest_with_direction(usage_kg: float) -&gt; Optional[CO2EquivalenceComparison]:\n    \"\"\"\n    Returns either lower bound or upper bound if lower is missing (usage smaller than min &gt;0).\n    \"\"\"\n    try:\n        lower = await get_lower_bound(usage_kg)\n        if lower and lower.co2eq_kg &gt; 0:\n            return lower\n\n        col = CO2EQComparison.get_collection()\n        upper_doc = await col.find_one(\n            {\"enabled\": True, \"co2eq_kg\": {\"$gt\": usage_kg}},\n            sort=[(\"co2eq_kg\", 1)],\n            projection={\"_id\": 0}\n        )\n        if upper_doc:\n            return CO2EquivalenceComparison(**upper_doc)\n        return None\n    except Exception as e:\n        logger.error(f\"Error in closest_with_direction: {str(e)}\", exc_info=True)\n        return None\n</code></pre>"},{"location":"backend/docs/api/routers-message/#routers.message.create_message","title":"<code>create_message(request, conversation_id, background_tasks, requesting_user=Depends(get_current_user))</code>  <code>async</code>","text":"<p>Create a new message in a conversation and generate an answer.</p> <p>Validates conversation ownership, normalizes requested public collections, persists a placeholder <code>Message</code>, runs generation, updates the message with answer and retrieval metadata, and schedules rollup/trimming of history.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>GenerationRequest</code> <p>Generation parameters including query, collections, and model settings.</p> required <code>conversation_id</code> <code>str</code> <p>Target conversation identifier.</p> required <code>background_tasks</code> <code>BackgroundTasks</code> <p>Background task runner used to schedule rollups.</p> required <code>requesting_user</code> <code>User</code> <p>Authenticated user injected by dependency.</p> <code>Depends(get_current_user)</code> <p>Returns:</p> Type Description <code>CreateMessageResponse</code> <p>Message id, query, answer, documents, flags, and metadata.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>404 if conversation is not found; 403 if ownership/collections invalid; 500 for server errors.</p> Source code in <code>backend/src/routers/message.py</code> <pre><code>@router.post(\n    \"/conversations/{conversation_id}/messages\", response_model=CreateMessageResponse\n)\nasync def create_message(\n    request: GenerationRequest,\n    conversation_id: str,\n    background_tasks: BackgroundTasks,\n    requesting_user: User = Depends(get_current_user),\n) -&gt; CreateMessageResponse:\n    \"\"\"\n    Create a new message in a conversation and generate an answer.\n\n    Validates conversation ownership, normalizes requested public collections, persists a placeholder `Message`, runs generation, updates the message with answer and retrieval metadata, and schedules rollup/trimming of history.\n\n    Args:\n        request (GenerationRequest): Generation parameters including query, collections, and model settings.\n        conversation_id (str): Target conversation identifier.\n        background_tasks (BackgroundTasks): Background task runner used to schedule rollups.\n        requesting_user (User): Authenticated user injected by dependency.\n\n    Returns:\n        Message id, query, answer, documents, flags, and metadata.\n\n    Raises:\n        HTTPException: 404 if conversation is not found; 403 if ownership/collections invalid; 500 for server errors.\n    \"\"\"\n    set_user_context(requesting_user.id)\n    set_conversation_context(conversation_id)\n\n    message = None\n    try:\n        conversation = await Conversation.find_by_id(conversation_id)\n        if not conversation:\n            raise HTTPException(status_code=404, detail=\"Conversation not found\")\n\n        if conversation.user_id != requesting_user.id:\n            raise HTTPException(\n                status_code=403,\n                detail=\"You are not allowed to add a message to this conversation\",\n            )\n\n        # Normalize and validate requested public collections against allowed lists\n        allowed_source = PUBLIC_COLLECTIONS if IS_PROD else STAGING_PUBLIC_COLLECTIONS\n        try:\n            allowed_names = {\n                item.get(\"name\")\n                for item in (allowed_source + WILEY_PUBLIC_COLLECTIONS)\n                if isinstance(item, dict) and item.get(\"name\")\n            }\n        except Exception:\n            allowed_names = set()\n\n        public_collections = [\n            n for n in request.public_collections if n in allowed_names\n        ]\n        request.public_collections = public_collections\n\n        # lookup query to check if some of the collection ids from other users are in the request.collection_ids\n        other_users_collections = await CollectionModel.find_all(\n            filter_dict={\n                \"id\": {\"$in\": request.public_collections},\n                \"user_id\": {\"$ne\": requesting_user.id},\n            }\n        )\n\n        if len(other_users_collections) &gt; 0:\n            raise HTTPException(\n                status_code=403,\n                detail=\"You are not allowed to use collections from other users\",\n            )\n\n        request.collection_ids = request.collection_ids + request.public_collections\n\n        # All user collections are used by default\n        user_collections = await CollectionModel.find_all(\n            filter_dict={\"user_id\": requesting_user.id}\n        )\n\n        request.private_collections_map = {c.id: c.name for c in user_collections}\n        if len(user_collections) &gt; 0:\n            request.collection_ids = request.collection_ids + [\n                c.id for c in user_collections\n            ]\n        # remove \"Wiley AI Gateway\" from collection_ids\n        request.collection_ids = [\n            c for c in request.collection_ids if c != \"Wiley AI Gateway\"\n        ]\n        logger.info(f\"Collection IDs: {request.collection_ids}\")\n\n        # Extract year range from filters for MCP usage\n        try:\n            request.year = extract_year_range_from_filters(request.filters)\n        except Exception:\n            request.year = None\n\n        message = await Message.create(\n            conversation_id=conversation_id,\n            input=request.query,\n            output=\"\",\n            documents=[],\n            use_rag=False,\n            request_input=request,\n            metadata={},\n        )\n\n        set_message_context(message.id)\n\n        answer, results, is_rag, latencies, prompts, retrieved_docs = (\n            await generate_answer(request, conversation_id=conversation_id)\n        )\n\n        documents_data = []\n        if results:\n            documents_data = [extract_document_data(result) for result in results]\n\n        message.output = answer\n        message.documents = documents_data\n        message.use_rag = is_rag\n        existing_metadata = dict(getattr(message, \"metadata\", {}) or {})\n        existing_metadata.update(\n            {\n                \"latencies\": latencies,\n                \"prompts\": prompts,\n                \"retrieved_docs\": retrieved_docs,\n            }\n        )\n        message.metadata = existing_metadata\n        await message.save()\n\n        # Schedule rollup as background task to avoid blocking response\n        background_tasks.add_task(maybe_rollup_and_trim_history, conversation_id)\n\n        return {\n            \"id\": message.id,\n            \"query\": request.query,\n            \"answer\": answer,\n            \"documents\": documents_data,\n            \"use_rag\": is_rag,\n            \"conversation_id\": conversation_id,\n            \"collection_ids\": request.collection_ids,\n            \"metadata\": {\n                \"latencies\": latencies,\n            },\n        }\n    except HTTPException as http_exc:\n        if message:\n            try:\n                existing_metadata = dict(getattr(message, \"metadata\", {}) or {})\n                existing_metadata[\"error\"] = str(getattr(http_exc, \"detail\", http_exc))\n                message.metadata = existing_metadata\n                await message.save()\n            except Exception:\n                pass\n        raise\n    except Exception as e:\n        if message:\n            try:\n                existing_metadata = dict(getattr(message, \"metadata\", {}) or {})\n                existing_metadata[\"error\"] = str(e)\n                message.metadata = existing_metadata\n                await message.save()\n            except Exception:\n                pass\n        raise HTTPException(status_code=500, detail=f\"Server error: {str(e)}\")\n</code></pre>"},{"location":"backend/docs/api/routers-message/#routers.message.create_message_stream","title":"<code>create_message_stream(request, conversation_id, background_tasks, requesting_user=Depends(get_current_user))</code>  <code>async</code>","text":"<p>Create a new message and stream generation via Server-Sent Events (SSE).</p> <p>Sets up a per-message stream bus and runs generation in a decoupled task. Yields SSE-formatted chunks including status updates, tokens, and final payloads.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>GenerationRequest</code> <p>Generation parameters including query, collections, and model settings.</p> required <code>conversation_id</code> <code>str</code> <p>Target conversation identifier.</p> required <code>background_tasks</code> <code>BackgroundTasks</code> <p>Background task runner used to schedule rollups.</p> required <code>requesting_user</code> <code>User</code> <p>Authenticated user injected by dependency.</p> <code>Depends(get_current_user)</code> <p>Returns:</p> Type Description <code>StreamingResponse</code> <p>SSE stream for the generation lifecycle.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>404 if conversation is not found; 403 if ownership/collections invalid; 500 for server errors.</p> Source code in <code>backend/src/routers/message.py</code> <pre><code>@router.post(\n    \"/conversations/{conversation_id}/stream_messages\",\n    response_class=StreamingResponse,\n)\nasync def create_message_stream(\n    request: GenerationRequest,\n    conversation_id: str,\n    background_tasks: BackgroundTasks,\n    requesting_user: User = Depends(get_current_user),\n) -&gt; StreamingResponse:\n    \"\"\"\n    Create a new message and stream generation via Server-Sent Events (SSE).\n\n    Sets up a per-message stream bus and runs generation in a decoupled task. Yields SSE-formatted chunks including status updates, tokens, and final payloads.\n\n    Args:\n        request (GenerationRequest): Generation parameters including query, collections, and model settings.\n        conversation_id (str): Target conversation identifier.\n        background_tasks (BackgroundTasks): Background task runner used to schedule rollups.\n        requesting_user (User): Authenticated user injected by dependency.\n\n    Returns:\n        SSE stream for the generation lifecycle.\n\n    Raises:\n        HTTPException: 404 if conversation is not found; 403 if ownership/collections invalid; 500 for server errors.\n    \"\"\"\n    set_user_context(requesting_user.id)\n    set_conversation_context(conversation_id)\n\n    message = None\n    try:\n        conversation = await Conversation.find_by_id(conversation_id)\n        if not conversation:\n            raise HTTPException(status_code=404, detail=\"Conversation not found\")\n\n        if conversation.user_id != requesting_user.id:\n            raise HTTPException(\n                status_code=403,\n                detail=\"You are not allowed to add a message to this conversation\",\n            )\n\n        # Normalize and validate requested public collections against allowed lists\n        allowed_source = PUBLIC_COLLECTIONS if IS_PROD else STAGING_PUBLIC_COLLECTIONS\n        try:\n            allowed_names = {\n                item.get(\"name\")\n                for item in (allowed_source + WILEY_PUBLIC_COLLECTIONS)\n                if isinstance(item, dict) and item.get(\"name\")\n            }\n        except Exception:\n            allowed_names = set()\n\n        public_collections = [\n            n for n in request.public_collections if n in allowed_names\n        ]\n        request.public_collections = public_collections\n\n        # lookup query to check if some of the collection ids from other users are in the request.collection_ids\n        other_users_collections = await CollectionModel.find_all(\n            filter_dict={\n                \"id\": {\"$in\": request.public_collections},\n                \"user_id\": {\"$ne\": requesting_user.id},\n            }\n        )\n\n        if len(other_users_collections) &gt; 0:\n            raise HTTPException(\n                status_code=403,\n                detail=\"You are not allowed to use collections from other users\",\n            )\n\n        request.collection_ids = request.collection_ids + request.public_collections\n\n        # All user collections are used by default\n        user_collections = await CollectionModel.find_all(\n            filter_dict={\"user_id\": requesting_user.id}\n        )\n\n        request.private_collections_map = {c.id: c.name for c in user_collections}\n        if len(user_collections) &gt; 0:\n            request.collection_ids = request.collection_ids + [\n                c.id for c in user_collections\n            ]\n        # remove \"Wiley AI Gateway\" from collection_ids\n        request.collection_ids = [\n            c for c in request.collection_ids if c != \"Wiley AI Gateway\"\n        ]\n        logger.info(f\"Collection IDs: {request.collection_ids}\")\n\n        # Extract year range from filters for MCP usage\n        try:\n            request.year = extract_year_range_from_filters(request.filters)\n        except Exception:\n            request.year = None\n\n        message = await Message.create(\n            conversation_id=conversation_id,\n            input=request.query,\n            output=\"\",\n            documents=[],\n            use_rag=False,\n            request_input=request,\n            metadata={},\n        )\n\n        set_message_context(message.id)\n\n        # Start decoupled background job that publishes to bus\n        cancel_mgr = get_cancel_manager()\n        cancel_event = cancel_mgr.create(message.id)\n        cancel_mgr.link_conversation(conversation_id, message.id)\n        gen_task = asyncio.create_task(\n            run_generation_to_bus(\n                request=request,\n                conversation_id=conversation_id,\n                message_id=message.id,\n                background_tasks=background_tasks,\n                cancel_event=cancel_event,\n            )\n        )\n        cancel_mgr.set_task(message.id, gen_task)\n\n        bus = get_stream_bus()\n\n        async def _gen():\n            # Optional catch-up from currently saved output (usually empty right after create)\n            try:\n                if message.output:\n                    yield f\"data: {json.dumps({'type':'partial','content': message.output})}\\n\\n\"\n            except Exception:\n                pass\n            async for data in bus.subscribe(message.id):\n                yield data\n\n        response = StreamingResponse(_gen(), media_type=\"text/event-stream\")\n        # Set SSE-friendly headers to prevent proxy/client reconnect loops\n        response.headers[\"Cache-Control\"] = \"no-cache\"\n        response.headers[\"Connection\"] = \"keep-alive\"\n        response.headers[\"X-Accel-Buffering\"] = \"no\"  # Nginx buffering off if present\n        return response\n    except HTTPException as http_exc:\n        if message:\n            error_logger = get_error_logger()\n            await error_logger.log_error_sync(\n                error=http_exc,\n                component=Component.ROUTER,\n                pipeline_stage=PipelineStage.ROUTER,\n                description=\"HTTPException in create_message_stream\",\n                error_type=type(http_exc).__name__,\n            )\n        raise http_exc\n    except Exception as e:\n        if message:\n            error_logger = get_error_logger()\n            await error_logger.log_error_sync(\n                error=e,\n                component=Component.ROUTER,\n                pipeline_stage=PipelineStage.ROUTER,\n                description=\"Exception in create_message_stream\",\n                error_type=type(e).__name__,\n            )\n        raise HTTPException(status_code=500, detail=f\"Server error: {str(e)}\")\n</code></pre>"},{"location":"backend/docs/api/routers-message/#routers.message.get_average_latencies","title":"<code>get_average_latencies(start_date=None, end_date=None)</code>  <code>async</code>","text":"<p>Return average latencies aggregated across all messages.</p> <p>Optionally filters the aggregation by a timestamp window.</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>datetime | None</code> <p>Optional start of the time window (inclusive).</p> <code>None</code> <code>end_date</code> <code>datetime | None</code> <p>Optional end of the time window (inclusive).</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Mapping of latency metric name to average value.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>500 for server errors during aggregation.</p> Source code in <code>backend/src/routers/message.py</code> <pre><code>@router.get(\"/conversations/messages/average-latencies\")\nasync def get_average_latencies(\n    start_date: datetime | None = None, end_date: datetime | None = None\n) -&gt; dict:\n    \"\"\"\n    Return average latencies aggregated across all messages.\n\n    Optionally filters the aggregation by a timestamp window.\n\n    Args:\n        start_date (datetime | None): Optional start of the time window (inclusive).\n        end_date (datetime | None): Optional end of the time window (inclusive).\n\n    Returns:\n        Mapping of latency metric name to average value.\n\n    Raises:\n        HTTPException: 500 for server errors during aggregation.\n    \"\"\"\n    try:\n        messages_col = Message.get_collection()\n        pipeline = []\n        if start_date is not None or end_date is not None:\n            time_filter = {}\n            if start_date is not None:\n                time_filter[\"$gte\"] = start_date\n            if end_date is not None:\n                time_filter[\"$lte\"] = end_date\n            pipeline.append({\"$match\": {\"timestamp\": time_filter}})\n        pipeline.append(\n            {\n                \"$group\": {\n                    \"_id\": None,\n                    \"rag_decision_latency\": {\n                        \"$avg\": \"$metadata.latencies.rag_decision_latency\"\n                    },\n                    \"query_embedding_latency\": {\n                        \"$avg\": \"$metadata.latencies.query_embedding_latency\"\n                    },\n                    \"qdrant_retrieval_latency\": {\n                        \"$avg\": \"$metadata.latencies.qdrant_retrieval_latency\"\n                    },\n                    \"mcp_retrieval_latency\": {\n                        \"$avg\": \"$metadata.latencies.mcp_retrieval_latency\"\n                    },\n                    \"reranking_latency\": {\n                        \"$avg\": \"$metadata.latencies.reranking_latency\"\n                    },\n                    \"first_token_latency\": {\n                        \"$avg\": \"$metadata.latencies.first_token_latency\"\n                    },\n                    \"mistral_first_token_latency\": {\n                        \"$avg\": \"$metadata.latencies.mistral_first_token_latency\"\n                    },\n                    \"base_generation_latency\": {\n                        \"$avg\": \"$metadata.latencies.base_generation_latency\"\n                    },\n                }\n            }\n        )\n        cursor = messages_col.aggregate(pipeline, allowDiskUse=True)\n        results = await cursor.to_list(length=1)\n        return results[0]\n    except HTTPException:\n        raise\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Server error: {str(e)}\")\n</code></pre>"},{"location":"backend/docs/api/routers-message/#routers.message.get_lower_bound","title":"<code>get_lower_bound(usage_kg)</code>  <code>async</code>","text":"<p>Find the largest co2eq_kg &lt;= usage_kg.</p> Source code in <code>backend/src/routers/message.py</code> <pre><code>async def get_lower_bound(usage_kg: float) -&gt; Optional[CO2EquivalenceComparison]:\n    \"\"\"Find the largest co2eq_kg &lt;= usage_kg.\"\"\"\n    try:\n        col = CO2EQComparison.get_collection()\n\n        query = {\"enabled\": True, \"co2eq_kg\": {\"$lte\": usage_kg}}\n        doc = await col.find_one(\n            query,\n            sort=[(\"co2eq_kg\", -1)],\n            projection={\"_id\": 0}\n        )\n\n        if doc:\n            return CO2EquivalenceComparison(**doc)\n        return None\n    except Exception as e:\n        logger.error(f\"Error in get_lower_bound: {str(e)}\", exc_info=True)\n        return None\n</code></pre>"},{"location":"backend/docs/api/routers-message/#routers.message.get_my_message_stats","title":"<code>get_my_message_stats(requesting_user=Depends(get_current_user))</code>  <code>async</code>","text":"<p>Return counts and character totals for the current user's messages.</p> <p>Aggregates across all messages belonging to conversations owned by the user.</p> <p>Parameters:</p> Name Type Description Default <code>requesting_user</code> <code>User</code> <p>Authenticated user injected by dependency.</p> <code>Depends(get_current_user)</code> <p>Returns:</p> Type Description <code>dict</code> <p>Aggregated stats including counts and character sums.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>500 for server errors during aggregation.</p> Source code in <code>backend/src/routers/message.py</code> <pre><code>@router.get(\"/conversations/messages/me/stats\")\nasync def get_my_message_stats(requesting_user: User = Depends(get_current_user)) -&gt; dict:\n    \"\"\"\n    Return counts and character totals for the current user's messages.\n\n    Aggregates across all messages belonging to conversations owned by the user.\n\n    Args:\n        requesting_user (User): Authenticated user injected by dependency.\n\n    Returns:\n        Aggregated stats including counts and character sums.\n\n    Raises:\n        HTTPException: 500 for server errors during aggregation.\n    \"\"\"\n    try:\n        messages_col = Message.get_collection()\n\n        # Fetch conversation IDs owned by the current user (avoid $lookup pipelines unsupported by DocumentDB)\n        user_conversations = await Conversation.find_all(\n            filter_dict={\"user_id\": requesting_user.id}\n        )\n        conversation_ids = [c.id for c in user_conversations if getattr(c, \"id\", None)]\n\n        if not conversation_ids:\n            return {\n                \"message_count\": 0,\n                \"input_characters\": 0,\n                \"output_characters\": 0,\n                \"total_characters\": 0,\n                \"co2eq_kg\": 0.0,\n                \"text\": \"\",\n            }\n\n        pipeline = [\n            {\"$match\": {\"conversation_id\": {\"$in\": conversation_ids}}},\n            {\n                \"$group\": {\n                    \"_id\": None,\n                    \"message_count\": {\"$sum\": 1},\n                    \"input_characters\": {\n                        \"$sum\": {\n                            \"$strLenCP\": {\n                                \"$ifNull\": [\n                                    \"$metadata.prompts.generation_prompt\",\n                                    \"\",\n                                ]\n                            }\n                        }\n                    },\n                    \"output_characters\": {\n                        \"$sum\": {\"$strLenCP\": {\"$ifNull\": [\"$output\", \"\"]}}\n                    },\n                }\n            },\n            {\n                \"$project\": {\n                    \"_id\": 0,\n                    \"message_count\": 1,\n                    \"input_characters\": 1,\n                    \"output_characters\": 1,\n                    \"total_characters\": {\n                        \"$add\": [\"$input_characters\", \"$output_characters\"],\n                    },\n                }\n            },\n        ]\n\n        cursor = messages_col.aggregate(pipeline, allowDiskUse=True)\n        results = await cursor.to_list(length=1)\n\n        if results:\n            stats = results[0]\n        else:\n            stats = {\n                \"message_count\": 0,\n                \"input_characters\": 0,\n                \"output_characters\": 0,\n                \"total_characters\": 0,\n            }\n\n\n        total_chars = stats.get(\"total_characters\", 0)\n        usage_kg = get_co2_usage_kg(total_chars=total_chars)\n\n        # Get CO2 equivalence data\n        text = \"\"\n        try:\n            equivalence_data = await build_equivalence_sentence(usage_kg)\n            text = equivalence_data.text\n        except Exception as e:\n            logger.error(f\"Failed to get CO2 equivalence data: {str(e)}\", exc_info=True)\n\n        # Add CO2 data to response\n        stats[\"co2eq_kg\"] = usage_kg\n        stats[\"text\"] = text\n\n        return stats\n    except HTTPException:\n        raise\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Server error: {str(e)}\")\n</code></pre>"},{"location":"backend/docs/api/routers-message/#routers.message.get_source_logs","title":"<code>get_source_logs(conversation_id, message_id, request, requesting_user=Depends(get_current_user))</code>  <code>async</code>","text":"<p>Append a source log entry to a message's metadata.</p> <p>Stores user-attributed source inspection information such as id, url, title, and collection name, with a server-side timestamp.</p> <p>Parameters:</p> Name Type Description Default <code>conversation_id</code> <code>str</code> <p>Conversation identifier.</p> required <code>message_id</code> <code>str</code> <p>Message identifier.</p> required <code>request</code> <code>SourceLogsRequest</code> <p>Source log details to append.</p> required <code>requesting_user</code> <code>User</code> <p>Authenticated user injected by dependency.</p> <code>Depends(get_current_user)</code> <p>Returns:</p> Type Description <code>dict</code> <p>Confirmation message upon successful append.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>404 if conversation/message not found or mismatched; 500 for server errors.</p> Source code in <code>backend/src/routers/message.py</code> <pre><code>@router.post(\"/conversations/{conversation_id}/messages/{message_id}/source_logs\")\nasync def get_source_logs(\n    conversation_id: str,\n    message_id: str,\n    request: SourceLogsRequest,\n    requesting_user: User = Depends(get_current_user),\n) -&gt; dict:\n    \"\"\"\n    Append a source log entry to a message's metadata.\n\n    Stores user-attributed source inspection information such as id, url, title, and collection name, with a server-side timestamp.\n\n    Args:\n        conversation_id (str): Conversation identifier.\n        message_id (str): Message identifier.\n        request (SourceLogsRequest): Source log details to append.\n        requesting_user (User): Authenticated user injected by dependency.\n\n    Returns:\n        Confirmation message upon successful append.\n\n    Raises:\n        HTTPException: 404 if conversation/message not found or mismatched; 500 for server errors.\n    \"\"\"\n    try:\n        conversation = await Conversation.find_by_id(conversation_id)\n        if not conversation:\n            raise HTTPException(status_code=404, detail=\"Conversation not found\")\n\n        message = await Message.find_by_id(message_id)\n        if not message:\n            raise HTTPException(status_code=404, detail=\"Message not found\")\n\n        if message.conversation_id != conversation_id:\n            raise HTTPException(\n                status_code=404, detail=\"Message not found in this conversation\"\n            )\n\n        # store source logs as an array and append each new entry\n        existing_metadata = dict(getattr(message, \"metadata\", {}) or {})\n        source_logs = list(existing_metadata.get(\"source_logs\") or [])\n        source_logs.append(\n            {\n                **request.model_dump(),\n                \"timestamp\": datetime.now().isoformat(),\n                \"user_id\": requesting_user.id,\n            }\n        )\n        existing_metadata[\"source_logs\"] = source_logs\n        message.metadata = existing_metadata\n        await message.save()\n        return {\"message\": \"Source logs stored successfully\"}\n    except HTTPException:\n        raise\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Server error: {str(e)}\")\n</code></pre>"},{"location":"backend/docs/api/routers-message/#routers.message.hallucination_detect","title":"<code>hallucination_detect(conversation_id, message_id, requesting_user=Depends(get_current_user))</code>  <code>async</code>","text":"<p>Detect and persist hallucination analysis for a message.</p> <p>Runs a multi-step pipeline (detect, optionally rewrite, retrieve, answer) and stores the result and latency breakdown on the message metadata.</p> <p>Parameters:</p> Name Type Description Default <code>conversation_id</code> <code>str</code> <p>Conversation identifier.</p> required <code>message_id</code> <code>str</code> <p>Message identifier to analyze.</p> required <code>requesting_user</code> <code>User</code> <p>Authenticated user injected by dependency.</p> <code>Depends(get_current_user)</code> <p>Returns:</p> Type Description <code>HallucinationDetectResponse</code> <p>Structured hallucination analysis with optional final answer.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>404 if conversation/message not found or mismatched; 403 if ownership invalid; 500 for server errors.</p> Source code in <code>backend/src/routers/message.py</code> <pre><code>@router.post(\n    \"/conversations/{conversation_id}/messages/{message_id}/hallucination\",\n    response_model=HallucinationDetectResponse,\n)\nasync def hallucination_detect(\n    conversation_id: str,\n    message_id: str,\n    requesting_user: User = Depends(get_current_user),\n) -&gt; HallucinationDetectResponse:\n    \"\"\"\n    Detect and persist hallucination analysis for a message.\n\n    Runs a multi-step pipeline (detect, optionally rewrite, retrieve, answer) and stores the result and latency breakdown on the message metadata.\n\n    Args:\n        conversation_id (str): Conversation identifier.\n        message_id (str): Message identifier to analyze.\n        requesting_user (User): Authenticated user injected by dependency.\n\n    Returns:\n        Structured hallucination analysis with optional final answer.\n\n    Raises:\n        HTTPException: 404 if conversation/message not found or mismatched; 403 if ownership invalid; 500 for server errors.\n    \"\"\"\n    # Validate conversation ownership and message relationship\n    conversation = await Conversation.find_by_id(conversation_id)\n    if not conversation:\n        raise HTTPException(status_code=404, detail=\"Conversation not found\")\n    if conversation.user_id != requesting_user.id:\n        raise HTTPException(\n            status_code=403,\n            detail=\"You are not allowed to access this conversation\",\n        )\n\n    message = await Message.find_by_id(message_id)\n    if not message:\n        raise HTTPException(status_code=404, detail=\"Message not found\")\n    if message.conversation_id != conversation_id:\n        raise HTTPException(\n            status_code=400, detail=\"Message does not belong to this conversation\"\n        )\n\n    try:\n        total_start = time.perf_counter()\n        detector = HallucinationDetector()\n\n        (\n            label,\n            reason,\n            _orig_q,\n            rewritten_question,\n            final_answer,\n            latencies,\n        ) = await detector.run(\n            query=message.input,\n            model_response=message.output,\n            docs=build_context(message.documents),\n            llm_type=message.request_input.llm_type,\n        )\n        total_latency = time.perf_counter() - total_start\n\n        # Persist hallucination result to Message\n        try:\n            existing_metadata = dict(getattr(message, \"metadata\", {}) or {})\n            hallucination_payload = {\n                \"label\": label,\n                \"reason\": reason,\n                \"rewritten_question\": rewritten_question,\n                \"final_answer\": final_answer,\n                \"latencies\": {\n                    \"detect\": latencies.get(\"detect\") if latencies else None,\n                    \"rewrite\": latencies.get(\"rewrite\") if latencies else None,\n                    \"final_answer\": (\n                        latencies.get(\"final_answer\") if latencies else None\n                    ),\n                    \"total\": total_latency,\n                },\n            }\n            existing_metadata[\"hallucination\"] = hallucination_payload\n            message.metadata = existing_metadata\n            await message.save()\n        except HTTPException:\n            raise\n        except Exception as e:\n            logger.error(f\"Failed to update Message with hallucination result: {e}\")\n\n        return HallucinationDetectResponse(\n            label=label,\n            reason=reason,\n            original_question=message.input,\n            rewritten_question=rewritten_question,\n            final_answer=final_answer,\n            latencies=latencies,\n        )\n\n    except HTTPException:\n        raise\n    except Exception as e:\n        logger.error(f\"Hallucination detection failed: {e}\", exc_info=True)\n        raise HTTPException(status_code=500, detail=f\"Server error: {str(e)}\")\n</code></pre>"},{"location":"backend/docs/api/routers-message/#routers.message.retrieve","title":"<code>retrieve(request, requesting_user=Depends(get_current_user))</code>  <code>async</code>","text":"<p>Run the entire retrieval pipeline and return all documents.</p> <p>Executes the RAG retrieval pipeline using setup_rag_and_context and returns all retrieved documents along with formatted results.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>GenerationRequest</code> <p>Generation parameters including query, collections, and model settings.</p> required <code>requesting_user</code> <code>User</code> <p>Authenticated user injected by dependency.</p> <code>Depends(get_current_user)</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing: - docs: All formatted documents from the retrieval pipeline - formatted_results: Same as docs (all formatted results) - context: Built context string - latencies: Timing information for retrieval operations</p> Source code in <code>backend/src/routers/message.py</code> <pre><code>@router.post(\"/retrieve\")\nasync def retrieve(\n    request: GenerationRequest,\n    requesting_user: User = Depends(get_current_user),\n) -&gt; dict:\n    \"\"\"\n    Run the entire retrieval pipeline and return all documents.\n\n    Executes the RAG retrieval pipeline using setup_rag_and_context and returns\n    all retrieved documents along with formatted results.\n\n    Args:\n        request (GenerationRequest): Generation parameters including query, collections, and model settings.\n        requesting_user (User): Authenticated user injected by dependency.\n\n    Returns:\n        Dictionary containing:\n            - docs: All formatted documents from the retrieval pipeline\n            - formatted_results: Same as docs (all formatted results)\n            - context: Built context string\n            - latencies: Timing information for retrieval operations\n    \"\"\"\n    try:\n        allowed_source = PUBLIC_COLLECTIONS if IS_PROD else STAGING_PUBLIC_COLLECTIONS\n        try:\n            allowed_names = {\n                item.get(\"name\")\n                for item in (allowed_source + WILEY_PUBLIC_COLLECTIONS)\n                if isinstance(item, dict) and item.get(\"name\")\n            }\n        except Exception:\n            allowed_names = set()\n\n        public_collections = [\n            n for n in request.public_collections if n in allowed_names\n        ]\n        request.public_collections = public_collections\n\n        other_users_collections = await CollectionModel.find_all(\n            filter_dict={\n                \"id\": {\"$in\": request.public_collections},\n                \"user_id\": {\"$ne\": requesting_user.id},\n            }\n        )\n\n        if len(other_users_collections) &gt; 0:\n            raise HTTPException(\n                status_code=403,\n                detail=\"You are not allowed to use collections from other users\",\n            )\n\n        request.collection_ids = request.collection_ids + request.public_collections\n\n        user_collections = await CollectionModel.find_all(\n            filter_dict={\"user_id\": requesting_user.id}\n        )\n\n        request.private_collections_map = {c.id: c.name for c in user_collections}\n        if len(user_collections) &gt; 0:\n            request.collection_ids = request.collection_ids + [\n                c.id for c in user_collections\n            ]\n        request.collection_ids = [\n            c for c in request.collection_ids if c != \"Wiley AI Gateway\"\n        ]\n        logger.info(f\"Collection IDs: {request.collection_ids}\")\n\n        try:\n            request.year = extract_year_range_from_filters(request.filters)\n        except Exception:\n            request.year = None\n\n        _context, _results, latencies, formated_results = await setup_rag_and_context(\n            request\n        )\n\n        return {\n            \"retrieved_docs\": formated_results,\n            \"latencies\": latencies,\n        }\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Server error: {str(e)}\")\n</code></pre>"},{"location":"backend/docs/api/routers-message/#routers.message.retry","title":"<code>retry(conversation_id, message_id, background_tasks, requesting_user=Depends(get_current_user))</code>  <code>async</code>","text":"<p>Retry generation for an existing message.</p> <p>Re-validates conversation ownership and message relationship, reuses the original <code>request_input</code> stored on the message, regenerates the answer, and updates message content, documents, and metadata.</p> <p>Parameters:</p> Name Type Description Default <code>conversation_id</code> <code>str</code> <p>Conversation identifier.</p> required <code>message_id</code> <code>str</code> <p>Message identifier to retry.</p> required <code>background_tasks</code> <code>BackgroundTasks</code> <p>Background task runner used to schedule rollups.</p> required <code>requesting_user</code> <code>User</code> <p>Authenticated user injected by dependency.</p> <code>Depends(get_current_user)</code> <p>Returns:</p> Type Description <code>dict</code> <p>Response payload mirroring create_message with updated answer and metadata.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>404 if conversation/message not found; 403 if ownership invalid; 400 if message cannot be retried; 500 for server errors.</p> Source code in <code>backend/src/routers/message.py</code> <pre><code>@router.post(\"/conversations/{conversation_id}/messages/{message_id}/retry\")\nasync def retry(\n    conversation_id: str,\n    message_id: str,\n    background_tasks: BackgroundTasks,\n    requesting_user: User = Depends(get_current_user),\n) -&gt; dict:\n    \"\"\"\n    Retry generation for an existing message.\n\n    Re-validates conversation ownership and message relationship, reuses the original `request_input` stored on the message, regenerates the answer, and updates message content, documents, and metadata.\n\n    Args:\n        conversation_id (str): Conversation identifier.\n        message_id (str): Message identifier to retry.\n        background_tasks (BackgroundTasks): Background task runner used to schedule rollups.\n        requesting_user (User): Authenticated user injected by dependency.\n\n    Returns:\n        Response payload mirroring create_message with updated answer and metadata.\n\n    Raises:\n        HTTPException: 404 if conversation/message not found; 403 if ownership invalid; 400 if message cannot be retried; 500 for server errors.\n    \"\"\"\n    try:\n        conversation = await Conversation.find_by_id(conversation_id)\n        if not conversation:\n            raise HTTPException(status_code=404, detail=\"Conversation not found\")\n\n        if conversation.user_id != requesting_user.id:\n            raise HTTPException(\n                status_code=403,\n                detail=\"You are not allowed to add a message to this conversation\",\n            )\n\n        message = await Message.find_by_id(message_id)\n        if not message:\n            raise HTTPException(status_code=404, detail=\"Message not found\")\n\n        if message.conversation_id != conversation_id:\n            raise HTTPException(\n                status_code=404, detail=\"Message not found in this conversation\"\n            )\n\n        if not message.request_input:\n            raise HTTPException(\n                status_code=400,\n                detail=\"This message cannot be retried\",\n            )\n\n        answer, results, is_rag, latencies, prompts, retrieved_docs = (\n            await generate_answer(\n                message.request_input, conversation_id=conversation_id\n            )\n        )\n\n        documents_data = []\n        if results:\n            documents_data = [extract_document_data(result) for result in results]\n\n        message.output = answer\n        message.documents = documents_data\n        message.use_rag = is_rag\n        existing_metadata = dict(getattr(message, \"metadata\", {}) or {})\n        existing_metadata.update(\n            {\n                \"latencies\": latencies,\n                \"prompts\": prompts,\n                \"retrieved_docs\": retrieved_docs,\n            }\n        )\n        message.metadata = existing_metadata\n        await message.save()\n\n        # Schedule rollup as background task to avoid blocking response\n        background_tasks.add_task(maybe_rollup_and_trim_history, conversation_id)\n\n        return {\n            \"id\": message.id,\n            \"query\": message.input,\n            \"answer\": answer,\n            \"documents\": documents_data,\n            \"use_rag\": is_rag,\n            \"conversation_id\": conversation_id,\n            \"collection_ids\": message.request_input.collection_ids,\n            \"metadata\": {\n                \"latencies\": latencies,\n                \"prompts\": prompts,\n                \"retrieved_docs\": retrieved_docs,\n            },\n        }\n    except HTTPException:\n        raise\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Server error: {str(e)}\")\n</code></pre>"},{"location":"backend/docs/api/routers-message/#routers.message.stop_conversation","title":"<code>stop_conversation(conversation_id, requesting_user=Depends(get_current_user))</code>  <code>async</code>","text":"<p>Signal cancellation for the active generation within a conversation.</p> <p>Uses the cancel manager to locate the in-flight message/task and requests cooperative cancellation, also notifying downstream subscribers via the stream bus.</p> <p>Parameters:</p> Name Type Description Default <code>conversation_id</code> <code>str</code> <p>Conversation identifier to stop generation for.</p> required <code>requesting_user</code> <code>User</code> <p>Authenticated user injected by dependency.</p> <code>Depends(get_current_user)</code> <p>Returns:</p> Type Description <code>dict</code> <p>Status payload indicating stop state or absence of active generation.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>404 if conversation is not found; 403 if ownership invalid; 500 for server errors.</p> Source code in <code>backend/src/routers/message.py</code> <pre><code>@router.post(\"/conversations/{conversation_id}/stop\")\nasync def stop_conversation(\n    conversation_id: str,\n    requesting_user: User = Depends(get_current_user),\n) -&gt; dict:\n    \"\"\"\n    Signal cancellation for the active generation within a conversation.\n\n    Uses the cancel manager to locate the in-flight message/task and requests cooperative cancellation, also notifying downstream subscribers via the stream bus.\n\n    Args:\n        conversation_id (str): Conversation identifier to stop generation for.\n        requesting_user (User): Authenticated user injected by dependency.\n\n    Returns:\n        Status payload indicating stop state or absence of active generation.\n\n    Raises:\n        HTTPException: 404 if conversation is not found; 403 if ownership invalid; 500 for server errors.\n    \"\"\"\n    try:\n        logger.info(\n            \"generation.stop.requested user_id=%s conversation_id=%s\",\n            requesting_user.id,\n            conversation_id,\n        )\n        conversation = await Conversation.find_by_id(conversation_id)\n        if not conversation:\n            raise HTTPException(status_code=404, detail=\"Conversation not found\")\n        if conversation.user_id != requesting_user.id:\n            raise HTTPException(\n                status_code=403,\n                detail=\"You are not allowed to access this conversation\",\n            )\n\n        cancel_mgr = get_cancel_manager()\n        # Prefer async lookup to support Redis-backed mapping across workers\n        try:\n            message_id = await cancel_mgr.get_message_for_conversation_async(conversation_id)  # type: ignore\n        except Exception:\n            message_id = cancel_mgr.get_message_for_conversation(conversation_id)\n        if not message_id:\n            # Nothing active to stop; respond success for idempotency\n            logger.info(\n                \"generation.stop.no_active user_id=%s conversation_id=%s\",\n                requesting_user.id,\n                conversation_id,\n            )\n            return {\"status\": \"no_active_generation\"}\n\n        cancel_mgr.cancel(message_id)\n        try:\n            bus = get_stream_bus()\n            await bus.publish(message_id, f\"data: {json.dumps({'type':'stopped'})}\\n\\n\")\n            await bus.close(message_id)\n            logger.info(\n                \"generation.stop.signaled user_id=%s conversation_id=%s message_id=%s\",\n                requesting_user.id,\n                conversation_id,\n                message_id,\n            )\n        except Exception as e:\n            logger.warning(\n                \"generation.stop.signal_failed conversation_id=%s message_id=%s err=%s\",\n                conversation_id,\n                message_id,\n                str(e),\n            )\n        return {\"status\": \"stopping\", \"message_id\": message_id}\n    except HTTPException:\n        raise\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Server error: {str(e)}\")\n</code></pre>"},{"location":"backend/docs/api/routers-message/#routers.message.stream_hallucination","title":"<code>stream_hallucination(conversation_id, message_id, requesting_user=Depends(get_current_user))</code>  <code>async</code>","text":"<p>Stream hallucination handling result as Server-Sent Events (SSE).</p> <p>Streams structured events for detection, optional rewriting, retrieval, and answer generation steps.</p> <ul> <li>If label == 0 (factual), emits a final event with the reason.</li> <li>If label == 1 (hallucination), streams tokens for the final answer and then a final event.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>conversation_id</code> <code>str</code> <p>Conversation identifier.</p> required <code>message_id</code> <code>str</code> <p>Message identifier to analyze.</p> required <code>requesting_user</code> <code>User</code> <p>Authenticated user injected by dependency.</p> <code>Depends(get_current_user)</code> <p>Returns:</p> Type Description <code>StreamingResponse</code> <p>SSE events for the detection workflow.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>404 if conversation/message not found or mismatched; 403 if access is forbidden; 500 for streaming errors.</p> Source code in <code>backend/src/routers/message.py</code> <pre><code>@router.post(\n    \"/conversations/{conversation_id}/messages/{message_id}/stream-hallucination\",\n    response_class=StreamingResponse,\n)\nasync def stream_hallucination(\n    conversation_id: str,\n    message_id: str,\n    requesting_user: User = Depends(get_current_user),\n) -&gt; StreamingResponse:\n    \"\"\"\n    Stream hallucination handling result as Server-Sent Events (SSE).\n\n    Streams structured events for detection, optional rewriting, retrieval, and answer generation steps.\n\n    - If label == 0 (factual), emits a final event with the reason.\n    - If label == 1 (hallucination), streams tokens for the final answer and then a final event.\n\n    Args:\n        conversation_id (str): Conversation identifier.\n        message_id (str): Message identifier to analyze.\n        requesting_user (User): Authenticated user injected by dependency.\n\n    Returns:\n        SSE events for the detection workflow.\n\n    Raises:\n        HTTPException: 404 if conversation/message not found or mismatched; 403 if access is forbidden; 500 for streaming errors.\n    \"\"\"\n    # Validate conversation ownership and message relationship\n    conversation = await Conversation.find_by_id(conversation_id)\n    if not conversation:\n        raise HTTPException(status_code=404, detail=\"Conversation not found\")\n    if conversation.user_id != requesting_user.id:\n        raise HTTPException(\n            status_code=403,\n            detail=\"You are not allowed to access this conversation\",\n        )\n\n    message = await Message.find_by_id(message_id)\n    if not message:\n        raise HTTPException(status_code=404, detail=\"Message not found\")\n    if message.conversation_id != conversation_id:\n        raise HTTPException(\n            status_code=400, detail=\"Message does not belong to this conversation\"\n        )\n\n    async def _generator():\n        import json\n        import time\n        from src.utils.template_loader import get_template\n\n        total_start = time.perf_counter()\n        detector = HallucinationDetector()\n        try:\n            # Emit an initial status event early to start the stream promptly\n            yield f\"data: {json.dumps({'type': 'status', 'content': 'hallucination detection started...'})}\\n\\n\"\n            # Step 1: Detect\n            t0 = time.perf_counter()\n            label, reason = await detector.detect(\n                query=message.input,\n                model_response=message.output,\n                docs=build_context(message.documents),\n                llm_type=message.request_input.llm_type,\n            )\n            detect_latency = time.perf_counter() - t0\n\n            yield f\"data: {json.dumps({'type': 'label', 'content': label})}\\n\\n\"\n            yield f\"data: {json.dumps({'type': 'reason', 'content': reason})}\\n\\n\"\n\n            # If factual (label == 0), emit reason and finish\n            if label == 0:\n                total_latency = time.perf_counter() - total_start\n                latencies = {\n                    \"detect\": detect_latency,\n                    \"rewrite\": None,\n                    \"final_answer\": None,\n                    \"total\": total_latency,\n                }\n                # Persist to message metadata\n                try:\n                    message.hallucination = {\n                        \"label\": label,\n                        \"reason\": reason,\n                        \"rewritten_question\": None,\n                        \"final_answer\": None,\n                        \"latencies\": latencies,\n                    }\n                    await message.save()\n                except Exception:\n                    pass\n\n                final_payload = {\n                    \"type\": \"final\",\n                    \"label\": label,\n                    \"reason\": reason,\n                    \"rewritten_question\": None,\n                    \"answer\": None,\n                    \"latencies\": latencies,\n                    \"top_k_retrieved_docs\": None,\n                }\n                yield f\"data: {json.dumps(final_payload)}\\n\\n\"\n                return\n\n            # Step 2: Rewrite (for hallucination)\n            # Transparency: emit rewriting step\n            yield f\"data: {json.dumps({'type': 'status', 'content': 'Rewriting query...'})}\\n\\n\"\n            t1 = time.perf_counter()\n            _orig_q, rewritten_question = await detector.rewrite_query(\n                query=message.input,\n                answer=message.output,\n                reason=reason,\n                llm_type=message.request_input.llm_type,\n            )\n            rewrite_latency = time.perf_counter() - t1\n            yield f\"data: {json.dumps({'type': 'rewritten_question', 'content': rewritten_question})}\\n\\n\"\n\n            # Step 3: Retrieve docs for rewritten_question (Qdrant + Wiley MCP)\n            # Transparency: emit retrieving step\n            yield f\"data: {json.dumps({'type': 'status', 'content': 'Retrieving relevant documents...'})}\\n\\n\"\n            # Build a new GenerationRequest based on original, overriding the query\n            req_in = message.request_input or GenerationRequest(query=message.input)\n            rewritten_request = GenerationRequest(\n                query=rewritten_question or message.input,\n                year=getattr(req_in, \"year\", None),\n                filters=getattr(req_in, \"filters\", None),\n                llm_type=getattr(req_in, \"llm_type\", None),\n                embeddings_model=getattr(req_in, \"embeddings_model\", None),\n                k=getattr(req_in, \"k\", 5),\n                temperature=getattr(req_in, \"temperature\", 0.3),\n                score_threshold=getattr(req_in, \"score_threshold\", 0.7),\n                max_new_tokens=getattr(req_in, \"max_new_tokens\", 1024),\n                public_collections=list(\n                    getattr(req_in, \"public_collections\", []) or []\n                ),\n            )\n            try:\n                rewritten_request.collection_ids = list(\n                    getattr(req_in, \"collection_ids\", []) or []\n                )\n            except Exception:\n                pass\n            try:\n                rewritten_request.private_collections_map = dict(\n                    getattr(req_in, \"private_collections_map\", {}) or {}\n                )\n            except Exception:\n                pass\n\n            context = \"\"\n            retrieved_docs = []\n            rag_latencies = {}\n            try:\n                context, results, rag_latencies, retrieved_docs = (\n                    await setup_rag_and_context(rewritten_request)\n                )\n            except Exception as e:\n                # Soft-fail RAG retrieval; proceed without new docs\n                rag_latencies = {\"rag_error\": str(e)}\n                retrieved_docs = []\n                context = \"\"\n\n            # Step 4: Stream final answer from LLM using the hallucination answer template\n            template = get_template(\n                \"llm_answer_template\", filename=\"hallucination_detector.yaml\"\n            )\n            # Inject retrieved context for better grounding\n            prompt = template.format(query=rewritten_question or message.input)\n            if context:\n                prompt = f\"{prompt}\\n\\nContext:\\n{context}\"\n\n            yield f\"data: {json.dumps({'type': 'status', 'content': 'Generating answer...'})}\\n\\n\"\n            final_answer_chunks = []\n            t2 = time.perf_counter()\n            # Try primary provider streaming first with a first-token timeout, then fallback to Mistral\n            llm = detector.llm_manager.get_client_for_model(\n                message.request_input.llm_type\n            )\n            used_stream = False\n            try:\n                astream = llm.astream(prompt)\n                # Enforce first token timeout similar to generate_answer\n                llm_instruct_timeout = MODEL_TIMEOUT\n                async with asyncio.timeout(llm_instruct_timeout):\n                    first = await astream.__anext__()\n                    first_text = getattr(first, \"content\", None)\n                    if first_text:\n                        final_answer_chunks.append(first_text)\n                        yield f\"data: {json.dumps({'type':'token','content':first_text})}\\n\\n\"\n                # Continue without timeout\n                async for token in astream:\n                    text = getattr(token, \"content\", None)\n                    if not text:\n                        continue\n                    final_answer_chunks.append(text)\n                    yield f\"data: {json.dumps({'type':'token','content':text})}\\n\\n\"\n                used_stream = True\n            except Exception:\n                used_stream = False\n\n            # Fallback to Mistral streaming if needed\n            if not used_stream:\n                logger.info(\"Hallucination Falling back to Mistral streaming\")\n                async for (\n                    token,\n                    _prompt,\n                ) in detector.llm_manager.generate_answer_mistral_stream(\n                    query=rewritten_question or message.input,\n                    context=context or \"\",\n                    temperature=getattr(message.request_input, \"temperature\", 0.3),\n                    conversation_context=\"\",\n                ):\n                    if not token:\n                        continue\n                    final_answer_chunks.append(str(token))\n                    yield f\"data: {json.dumps({'type':'token','content':str(token)})}\\n\\n\"\n\n            final_latency = time.perf_counter() - t2\n\n            final_answer = \"\".join(final_answer_chunks)\n            total_latency = time.perf_counter() - total_start\n            latencies = {\n                \"detect\": detect_latency,\n                \"rewrite\": rewrite_latency,\n                **(rag_latencies or {}),\n                \"final_answer\": final_latency,\n                \"total\": total_latency,\n            }\n\n            # Persist to message.hallucination\n            try:\n                message.hallucination = {\n                    \"label\": label,\n                    \"reason\": reason,\n                    \"rewritten_question\": rewritten_question,\n                    \"final_answer\": final_answer,\n                    \"latencies\": latencies,\n                    \"top_k_retrieved_docs\": results,\n                    \"retrieved_docs\": retrieved_docs,\n                }\n                await message.save()\n            except Exception:\n                pass\n\n            final_payload = {\n                \"type\": \"final\",\n                \"label\": label,\n                \"reason\": reason,\n                \"rewritten_question\": rewritten_question,\n                \"answer\": final_answer,\n                \"latencies\": latencies,\n                \"top_k_retrieved_docs\": results,\n            }\n            yield f\"data: {json.dumps(final_payload)}\\n\\n\"\n        except Exception as e:\n            # Persist error and stream error event\n            try:\n                existing_metadata = dict(getattr(message, \"metadata\", {}) or {})\n                existing_metadata[\"error\"] = str(e)\n                message.metadata = existing_metadata\n                await message.save()\n            except Exception:\n                pass\n            yield f\"data: {json.dumps({'type':'error','message':str(e)})}\\n\\n\"\n\n    response = StreamingResponse(_generator(), media_type=\"text/event-stream\")\n    response.headers[\"Cache-Control\"] = \"no-cache\"\n    response.headers[\"Connection\"] = \"keep-alive\"\n    response.headers[\"X-Accel-Buffering\"] = \"no\"\n    return response\n</code></pre>"},{"location":"backend/docs/api/routers-message/#routers.message.update_message","title":"<code>update_message(conversation_id, message_id, request, requesting_user=Depends(get_current_user))</code>  <code>async</code>","text":"<p>Update message feedback and related annotations.</p> <p>Supports updating fields such as <code>feedback</code>, <code>feedback_reason</code>, <code>was_copied</code>, and hallucination feedback metadata on the target message.</p> <p>Parameters:</p> Name Type Description Default <code>conversation_id</code> <code>str</code> <p>Conversation identifier.</p> required <code>message_id</code> <code>str</code> <p>Message identifier to update.</p> required <code>request</code> <code>MessageUpdate</code> <p>Partial update payload for feedback fields.</p> required <code>requesting_user</code> <code>User</code> <p>Authenticated user injected by dependency.</p> <code>Depends(get_current_user)</code> <p>Returns:</p> Type Description <code>dict</code> <p>Success message upon update.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>404 if conversation/message not found or mismatched; 403 if ownership invalid; 500 for server errors.</p> Source code in <code>backend/src/routers/message.py</code> <pre><code>@router.patch(\"/conversations/{conversation_id}/messages/{message_id}\")\nasync def update_message(\n    conversation_id: str,\n    message_id: str,\n    request: MessageUpdate,\n    requesting_user: User = Depends(get_current_user),\n) -&gt; dict:\n    \"\"\"\n    Update message feedback and related annotations.\n\n    Supports updating fields such as `feedback`, `feedback_reason`, `was_copied`, and hallucination feedback metadata on the target message.\n\n    Args:\n        conversation_id (str): Conversation identifier.\n        message_id (str): Message identifier to update.\n        request (MessageUpdate): Partial update payload for feedback fields.\n        requesting_user (User): Authenticated user injected by dependency.\n\n    Returns:\n        Success message upon update.\n\n    Raises:\n        HTTPException: 404 if conversation/message not found or mismatched; 403 if ownership invalid; 500 for server errors.\n    \"\"\"\n    try:\n        conversation = await Conversation.find_by_id(conversation_id)\n        if not conversation:\n            raise HTTPException(status_code=404, detail=\"Conversation not found\")\n\n        message = await Message.find_by_id(message_id)\n        if not message:\n            raise HTTPException(status_code=404, detail=\"Message not found\")\n\n        if message.conversation_id != conversation_id:\n            raise HTTPException(\n                status_code=404, detail=\"Message not found in this conversation\"\n            )\n\n        if conversation.user_id != requesting_user.id:\n            raise HTTPException(\n                status_code=403,\n                detail=\"You are not allowed to update feedback for this message\",\n            )\n\n        if request.feedback is not None:\n            message.feedback = request.feedback.value\n\n        if request.was_copied is not None:\n            message.was_copied = request.was_copied\n\n        if request.feedback_reason is not None:\n            message.feedback_reason = request.feedback_reason\n\n        if request.hallucination_feedback is not None:\n            if message.hallucination is None:\n                message.hallucination = {}\n            message.hallucination[\"feedback\"] = request.hallucination_feedback.value\n\n        if request.hallucination_feedback_reason is not None:\n            if message.hallucination is None:\n                message.hallucination = {}\n            message.hallucination[\"feedback_reason\"] = (\n                request.hallucination_feedback_reason\n            )\n\n        if request.hallucination_was_copied is not None:\n            if message.hallucination is None:\n                message.hallucination = {}\n            message.hallucination[\"was_copied\"] = request.hallucination_was_copied\n\n        await message.save()\n\n        return {\"message\": \"Feedback updated successfully\"}\n\n    except HTTPException:\n        raise\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Server error: {str(e)}\")\n</code></pre>"},{"location":"backend/docs/api/routers-user/","title":"User","text":""},{"location":"backend/docs/api/routers-user/#routers.user.me","title":"<code>me(user=Depends(get_current_user))</code>  <code>async</code>","text":"<p>Return the authenticated user's profile.</p> <p>Parameters:</p> Name Type Description Default <code>user</code> <code>User</code> <p>Authenticated user injected by dependency.</p> <code>Depends(get_current_user)</code> <p>Returns:</p> Type Description <code>User</code> <p>Current user.</p> Source code in <code>backend/src/routers/user.py</code> <pre><code>@router.get(\"/me\", response_model=User)\nasync def me(user: User = Depends(get_current_user)) -&gt; User:\n    \"\"\"\n    Return the authenticated user's profile.\n\n    Args:\n        user (User): Authenticated user injected by dependency.\n\n    Returns:\n        Current user.\n    \"\"\"\n    return user\n</code></pre>"},{"location":"backend/docs/api/routers-user/#routers.user.update_user","title":"<code>update_user(request, user=Depends(get_current_user))</code>  <code>async</code>","text":"<p>Update the authenticated user's profile.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>UpdateUserRequest</code> <p>New user attributes to set.</p> required <code>user</code> <code>User</code> <p>Authenticated user injected by dependency.</p> <code>Depends(get_current_user)</code> <p>Returns:</p> Type Description <code>User</code> <p>Updated user.</p> Source code in <code>backend/src/routers/user.py</code> <pre><code>@router.patch(\"\", response_model=User)\nasync def update_user(\n    request: UpdateUserRequest, user: User = Depends(get_current_user)\n) -&gt; User:\n    \"\"\"\n    Update the authenticated user's profile.\n\n    Args:\n        request (UpdateUserRequest): New user attributes to set.\n        user (User): Authenticated user injected by dependency.\n\n    Returns:\n        Updated user.\n    \"\"\"\n    user.first_name = request.first_name\n    user.last_name = request.last_name\n    await user.save()\n    return user\n</code></pre>"},{"location":"backend/docs/api/swagger-api/","title":"Swagger API","text":""},{"location":"backend/docs/api/types/","title":"Types","text":""},{"location":"backend/docs/api/types/#common","title":"Common","text":""},{"location":"backend/docs/api/types/#pagination","title":"Pagination","text":"<ul> <li>page: int (default 1)</li> <li>limit: int (default 10)</li> </ul>"},{"location":"backend/docs/api/types/#paginationmetadata","title":"PaginationMetadata","text":"<ul> <li>total_count: int</li> <li>current_page: int</li> <li>total_pages: int</li> <li>has_next: bool</li> </ul>"},{"location":"backend/docs/api/types/#paginatedresponset","title":"PaginatedResponse[T]","text":"<ul> <li>data: list[T]</li> <li>meta: PaginationMetadata</li> </ul>"},{"location":"backend/docs/api/types/#auth","title":"Auth","text":""},{"location":"backend/docs/api/types/#loginrequest","title":"LoginRequest","text":"<ul> <li>email: string (email)</li> <li>password: string</li> </ul>"},{"location":"backend/docs/api/types/#loginresponse","title":"LoginResponse","text":"<ul> <li>access_token: string</li> <li>refresh_token: string</li> </ul>"},{"location":"backend/docs/api/types/#refreshrequest","title":"RefreshRequest","text":"<ul> <li>refresh_token: string</li> </ul>"},{"location":"backend/docs/api/types/#refreshresponse","title":"RefreshResponse","text":"<ul> <li>access_token: string</li> </ul>"},{"location":"backend/docs/api/types/#signuprequest","title":"SignupRequest","text":"<ul> <li>email: string (email)</li> <li>password: string</li> <li>first_name: string | null</li> <li>last_name: string | null</li> </ul>"},{"location":"backend/docs/api/types/#signupresponse","title":"SignupResponse","text":"<ul> <li>id: string</li> <li>email: string (email)</li> <li>first_name: string | null</li> <li>last_name: string | null</li> </ul>"},{"location":"backend/docs/api/types/#resendactivationrequest","title":"ResendActivationRequest","text":"<ul> <li>email: string</li> </ul>"},{"location":"backend/docs/api/types/#verifyrequest","title":"VerifyRequest","text":"<ul> <li>email: string</li> <li>activation_code: string</li> </ul>"},{"location":"backend/docs/api/types/#user","title":"User","text":""},{"location":"backend/docs/api/types/#updateuserrequest","title":"UpdateUserRequest","text":"<ul> <li>first_name: string</li> <li>last_name: string</li> </ul>"},{"location":"backend/docs/api/types/#user-response-model","title":"User (response model)","text":"<ul> <li>Inherits <code>MongoModel</code> \u2192 includes id: string, timestamp: datetime</li> <li>email: string (email)</li> <li>password_hash: string</li> <li>first_name: string | null</li> <li>last_name: string | null</li> <li>is_active: bool</li> <li>activation_code: string | null</li> </ul>"},{"location":"backend/docs/api/types/#collections","title":"Collections","text":""},{"location":"backend/docs/api/types/#collectionrequest","title":"CollectionRequest","text":"<ul> <li>embeddings_model: string</li> <li>name: string</li> <li>description: string</li> </ul>"},{"location":"backend/docs/api/types/#collectionupdate","title":"CollectionUpdate","text":"<ul> <li>name: string</li> </ul>"},{"location":"backend/docs/api/types/#collection-response-model","title":"Collection (response model)","text":"<ul> <li>Inherits <code>MongoModel</code> \u2192 includes id: string, timestamp: datetime</li> <li>user_id: string | null</li> <li>name: string</li> <li>description: string | null</li> <li>embeddings_model: string</li> </ul>"},{"location":"backend/docs/api/types/#documents","title":"Documents","text":""},{"location":"backend/docs/api/types/#adddocumentrequest","title":"AddDocumentRequest","text":"<ul> <li>embeddings_model: string</li> <li>chunk_size: int</li> <li>chunk_overlap: int</li> <li>metadata_urls: list[string] | null</li> <li>metadata_names: list[string] | null</li> </ul>"},{"location":"backend/docs/api/types/#updatedocumentrequest","title":"UpdateDocumentRequest","text":"<ul> <li>embeddings_model: string</li> <li>source_name: string</li> <li>new_metadata: dict | null</li> </ul>"},{"location":"backend/docs/api/types/#document-response-model","title":"Document (response model)","text":"<ul> <li>Inherits <code>MongoModel</code> \u2192 includes id: string, timestamp: datetime</li> <li>user_id: string</li> <li>collection_id: string</li> <li>name: string</li> <li>filename: string | null</li> <li>file_type: string | null</li> <li>source_url: string | null</li> <li>chunk_count: int | null</li> <li>file_size: int | null</li> <li>vector_ids: list[string] | null</li> </ul>"},{"location":"backend/docs/api/types/#conversations","title":"Conversations","text":""},{"location":"backend/docs/api/types/#conversationcreate","title":"ConversationCreate","text":"<ul> <li>name: string</li> </ul>"},{"location":"backend/docs/api/types/#conversationnameupdate","title":"ConversationNameUpdate","text":"<ul> <li>name: string</li> </ul>"},{"location":"backend/docs/api/types/#conversationdetail-response-model","title":"ConversationDetail (response model)","text":"<ul> <li>id: string</li> <li>user_id: string</li> <li>name: string</li> <li>timestamp: datetime</li> <li>messages: list[Message]</li> </ul>"},{"location":"backend/docs/api/types/#conversation-response-model","title":"Conversation (response model)","text":"<ul> <li>Inherits <code>MongoModel</code> \u2192 includes id: string, timestamp: datetime</li> <li>user_id: string</li> <li>name: string</li> <li>summary: string | null</li> </ul>"},{"location":"backend/docs/api/types/#messages","title":"Messages","text":""},{"location":"backend/docs/api/types/#generationrequest","title":"GenerationRequest","text":"<ul> <li>query: string</li> <li>year: list[int] | null</li> <li>filters: dict | null</li> <li>llm_type: string | null (one of 'main', 'fallback', 'runpod', 'mistral', 'satcom_small', 'satcom_large', 'ship', 'eve_v05')</li> <li>embeddings_model: string</li> <li>k: int</li> <li>temperature: float</li> <li>score_threshold: float</li> <li>max_new_tokens: int</li> <li>public_collections: list[string]</li> <li>Note: server populates private <code>collection_ids</code> and <code>private_collections_map</code>.</li> </ul>"},{"location":"backend/docs/api/types/#createmessageresponse","title":"CreateMessageResponse","text":"<ul> <li>id: string</li> <li>query: string</li> <li>answer: string</li> <li>documents: list[DocumentReference]</li> <li>use_rag: bool</li> <li>conversation_id: string</li> <li>loop_result: LoopResult | null</li> <li>metadata: ResponseMetadata</li> </ul>"},{"location":"backend/docs/api/types/#documentreference","title":"DocumentReference","text":"<ul> <li>id: string | null</li> <li>version: int | null</li> <li>score: float | null</li> <li>reranking_score: float | null</li> <li>collection_name: string | null</li> <li>payload: dict</li> <li>text: string</li> <li>metadata: dict</li> </ul>"},{"location":"backend/docs/api/types/#latencies","title":"Latencies","text":"<ul> <li>guardrail_latency: float | null</li> <li>rag_decision_latency: float | null</li> <li>query_embedding_latency: float | null</li> <li>qdrant_retrieval_latency: float | null</li> <li>mcp_retrieval_latency: float | null</li> <li>reranking_latency: float | null</li> <li>base_generation_latency: float | null</li> <li>fallback_latency: float | null</li> <li>hallucination_latency: HallucinationLatencies | null</li> <li>total_latency: float | null</li> </ul>"},{"location":"backend/docs/api/types/#hallucinationlatencies","title":"HallucinationLatencies","text":"<ul> <li>detection_latency: float | null</li> <li>span_reprompting_latency: float | null</li> <li>query_rewriting_latency: float | null</li> <li>regeneration_latency: float | null</li> <li>overall_latency: float | null</li> </ul>"},{"location":"backend/docs/api/types/#responsemetadata","title":"ResponseMetadata","text":"<ul> <li>latencies: Latencies</li> </ul>"},{"location":"backend/docs/api/types/#loopresult","title":"LoopResult","text":"<ul> <li>final_answer: string | null</li> <li>generation_response: dict | null</li> <li>hallucination_response: dict | null</li> <li>rewrite_response: dict | null</li> <li>reflected_response: dict | null</li> <li>ranked_output: dict | null</li> <li>docs: string | null</li> </ul>"},{"location":"backend/docs/api/types/#hallucinationdetectresponse","title":"HallucinationDetectResponse","text":"<ul> <li>label: int</li> <li>reason: string</li> </ul>"},{"location":"backend/docs/api/types/#sourcelogsrequest","title":"SourceLogsRequest","text":"<ul> <li>source_id: string | null</li> <li>source_url: string | null</li> <li>source_title: string | null</li> <li>source_collection_name: string | null</li> </ul>"},{"location":"backend/docs/api/types/#tools","title":"Tools","text":""},{"location":"backend/docs/api/types/#toolconfigrequest","title":"ToolConfigRequest","text":"<ul> <li>url: string | null</li> <li>transport: \"streamable_http\" | \"stdio\" | null</li> <li>headers: dict[str, str] | null</li> <li>command: string | null</li> <li>args: list[string] | null</li> <li>env: dict[str, str] | null</li> </ul>"},{"location":"backend/docs/api/types/#toolrequest","title":"ToolRequest","text":"<ul> <li>name: string</li> <li>provider: string | null</li> <li>description: string | null</li> <li>type: string (default \"mcp\")</li> <li>enabled: bool</li> <li>environment: list[string] | null</li> <li>config: ToolConfigRequest</li> </ul>"},{"location":"backend/docs/api/types/#toolupdate","title":"ToolUpdate","text":"<ul> <li>Partial fields mirroring <code>ToolRequest</code>; all optional.</li> </ul>"},{"location":"backend/docs/api/types/#tool-response-model","title":"Tool (response model)","text":"<ul> <li>Inherits <code>MongoModel</code> \u2192 includes id: string, timestamp: datetime</li> <li>user_id: string | null</li> <li>name: string</li> <li>provider: string | null</li> <li>description: string | null</li> <li>type: enum (\"mcp\")</li> <li>enabled: bool</li> <li>environment: list[string] | null</li> <li>config: ToolConfig</li> <li>created_at: datetime</li> <li>updated_at: datetime</li> <li>deleted_at: datetime | null</li> </ul>"},{"location":"backend/docs/api/types/#toolconfig-response-model","title":"ToolConfig (response model)","text":"<ul> <li>url: string | null</li> <li>transport: enum (\"streamable_http\" | \"stdio\") | null</li> <li>headers: dict[str, str] | null</li> <li>command: string | null</li> <li>args: list[string] | null</li> <li>env: dict[str, str] | null</li> </ul>"},{"location":"backend/docs/api/types/#forgot-password","title":"Forgot Password","text":""},{"location":"backend/docs/api/types/#forgotpasswordrequest","title":"ForgotPasswordRequest","text":"<ul> <li>email: string (email)</li> </ul>"},{"location":"backend/docs/api/types/#forgotpasswordconfirmation","title":"ForgotPasswordConfirmation","text":"<ul> <li>new_password: string</li> <li>confirm_password: string</li> <li>code: string</li> </ul>"},{"location":"evalkit/docs/","title":"Eve-evalkit","text":"<p>Welcome to the Eve-evalkit documentation. This framework provides comprehensive tools for evaluating language models on Earth Observation (EO) specific tasks and benchmarks.</p>"},{"location":"evalkit/docs/#what-is-eve-evalkit","title":"What is Eve-evalkit?","text":"<p>Eve-evalkit is built on top of the EleutherAI Language Model Evaluation Harness, providing:</p> <ul> <li>Custom EO Tasks: Specialized evaluation tasks for Earth Observation domain, including MCQA, hallucination detection, and more</li> <li>Full LM-Eval-Harness Support: Access to all standard benchmarks (MMLU-Pro, GSM8K, HellaSwag, etc.)</li> <li>WandB Integration: Automatic experiment tracking and metric logging</li> <li>Flexible Configuration: YAML-based configuration for easy experiment management</li> <li>Production Ready: Built for evaluating models via API endpoints with concurrent requests</li> </ul>"},{"location":"evalkit/docs/#quick-links","title":"Quick Links","text":"<ul> <li>Getting Started: Installation, configuration, and running your first evaluation</li> <li>Examples: Comprehensive configuration examples and use cases</li> <li>EO Tasks: Detailed information about Earth Observation evaluation tasks</li> <li>Code Reference: API documentation and code examples</li> </ul>"},{"location":"evalkit/docs/#key-features","title":"Key Features","text":""},{"location":"evalkit/docs/#earth-observation-tasks","title":"Earth Observation Tasks","text":"<p>Evaluate models on specialized EO capabilities:</p> <ul> <li>Multiple-Choice QA: Single and multiple-answer questions from EO curricula</li> <li>Open-Ended QA: Free-form question answering with and without context</li> <li>Hallucination Detection: Identify fabricated or unsupported information</li> <li>Refusal Testing: Assess appropriate refusal behavior when context is insufficient</li> </ul>"},{"location":"evalkit/docs/#comprehensive-metrics","title":"Comprehensive Metrics","text":"<ul> <li>LLM-as-Judge: Sophisticated evaluation using judge models</li> <li>Traditional Metrics: Accuracy, F1, Precision, Recall, IoU</li> <li>Semantic Metrics: BERTScore, Cosine Similarity</li> <li>Generation Metrics: BLEU, ROUGE</li> </ul>"},{"location":"evalkit/docs/#production-features","title":"Production Features","text":"<ul> <li>API Model Support: Evaluate models via OpenAI-compatible endpoints</li> <li>Concurrent Requests: Speed up evaluations with parallel API calls</li> <li>Timeout Handling: Graceful handling of slow or failed requests</li> <li>Result Logging: Comprehensive JSON and JSONL outputs</li> <li>WandB Integration: Track experiments and visualize metrics</li> </ul>"},{"location":"evalkit/docs/#example-use-cases","title":"Example Use Cases","text":""},{"location":"evalkit/docs/#research-development","title":"Research &amp; Development","text":"<ul> <li>Benchmark Earth Observation models against established tasks</li> <li>Compare model performance across different architectures</li> <li>Identify strengths and weaknesses in EO domain understanding</li> </ul>"},{"location":"evalkit/docs/#model-selection","title":"Model Selection","text":"<ul> <li>Evaluate multiple models on EO-specific capabilities</li> <li>Compare general-purpose models vs. domain-specific models</li> <li>Assess trade-offs between performance and cost</li> </ul>"},{"location":"evalkit/docs/#documentation-structure","title":"Documentation Structure","text":"<ul> <li>Getting Started: Installation, configuration, and basic usage</li> <li>Examples: Comprehensive configuration examples and use cases</li> <li>EO Tasks: Detailed task descriptions, metrics, and examples</li> <li>Code Reference: API documentation and programmatic usage</li> </ul>"},{"location":"evalkit/docs/#support-contributing","title":"Support &amp; Contributing","text":"<ul> <li>GitHub: eve-esa/eve-evaluation</li> <li>Issues: Report bugs or request features on GitHub</li> </ul>"},{"location":"evalkit/docs/#citation","title":"Citation","text":"<p>If you use this evaluation framework in your research, please cite:</p>"},{"location":"evalkit/docs/eo_tasks/","title":"Earth Observation Evaluation Tasks","text":"<p>This page provides a comprehensive overview of all available Earth Observation (EO) evaluation tasks in Eve-evalkit. Each task is designed to assess different capabilities of language models in the Earth Observation domain.</p>"},{"location":"evalkit/docs/eo_tasks/#quick-reference","title":"Quick Reference","text":"Task Name Type Dataset Size Primary Metrics MCQA Multiple Answer Multiple Choice eve-esa/eve-is-mcqa 431 IoU, Accuracy MCQA Single Answer Multiple Choice eve-esa/mcqa-single-answer 1261 Accuracy Open Ended Generation eve-esa/open-ended 1257 LLM as Judge, Win Rate Open Ended with Context Generation eve-esa/open-ended-w-context 418 LLM Judge, Win Rate Hallucination Detection Classification eve-esa/hallucination-detection 2326 Accuracy, Precision, Recall, F1"},{"location":"evalkit/docs/eo_tasks/#detailed-task-descriptions","title":"Detailed Task Descriptions","text":""},{"location":"evalkit/docs/eo_tasks/#mcqa-multiple-answer","title":"MCQA Multiple Answer","text":"<p>Task Name: <code>is_mcqa</code> or <code>mcqa_multiple_answer</code></p> <p>Description:</p> <p>EVE-mcqa-multiple-answers consists of multiple-choice questions from Imperative Space MOOC exams where questions may have one or more correct answers. Models must identify all correct options from an arbitrary number of choices, making this a challenging task that requires comprehensive understanding rather than simple fact recall.</p> <p>How to Call:</p> <pre><code>tasks:\n  - name: mcqa_multiple_answers\n    num_fewshot: 2\n    max_tokens: 10000\n</code></pre> <p>Dataset:</p> <ul> <li>Source: eve-esa/mcqa-multiple-answers</li> <li>Split: train</li> <li>Size: 432 samples</li> <li>Structure: Each example contains a <code>Question</code>, <code>Answers</code> (list of correct labels), and <code>Choices</code> (list with labels and text)</li> </ul> <p>Evaluation Metrics:</p> <ul> <li>IoU (Intersection over Union): Measures partial correctness by calculating the overlap between predicted and correct answer sets. IoU = |Predicted \u2229 Correct| / |Predicted \u222a Correct| (higher is better)</li> <li>Accuracy (Exact Match): Binary score where 1.0 means the predicted answer set exactly matches the correct answer set (higher is better)</li> </ul> <p>Why It's Useful:</p> <p>This task tests a model's comprehensive understanding of EO concepts where multiple aspects or factors may be simultaneously correct. The IoU metric is particularly valuable as it rewards partially correct answers, providing a more nuanced evaluation than simple exact matching. This reflects real-world scenarios where partial knowledge is still valuable.</p> <p>Example:</p> <pre><code>{\n  \"Question\": \"Which bands of Sentinel-2 have 10m resolution?\",\n  \"Answers\": [\"A\", \"B\", \"C\"],\n  \"Choices\": {\n    \"label\": [\"A\", \"B\", \"C\", \"D\"],\n    \"text\": [\"B2 (Blue)\", \"B3 (Green)\", \"B4 (Red)\", \"B8 (NIR)\"]\n  }\n}\n</code></pre>"},{"location":"evalkit/docs/eo_tasks/#mcqa-single-answer","title":"MCQA Single Answer","text":"<p>Task Name: <code>mcqa_single_answer</code></p> <p>Description:</p> <p>EVE-mcqa-single-answer is a traditional multiple-choice dataset with exactly one correct answer per question. Models must identify the single best option from the provided choices, testing factual knowledge and reasoning abilities in the Earth Observation domain.</p> <p>How to Call:</p> <pre><code>tasks:\n  - name: mcqa_single_answer\n    num_fewshot: 2\n    max_tokens: 10000\n</code></pre> <p>Dataset:</p> <ul> <li>Source: eve-esa/mcqa-single-answer</li> <li>Split: train</li> <li>Size: ~1000 samples</li> <li>Structure: Each example contains a <code>question</code>, <code>choices</code> (list of answer texts), and <code>answer</code> (single letter indicating correct choice)</li> </ul> <p>Evaluation Metrics:</p> <ul> <li>Accuracy: Percentage of questions answered correctly (higher is better)</li> </ul> <p>Why It's Useful:</p> <p>This task evaluates factual knowledge and reasoning abilities in scenarios where there is a single definitively correct answer. It's particularly useful for assessing fundamental EO concepts, terminology, and principles. The single-answer format reduces ambiguity and provides clear, interpretable results.</p> <p>Example:</p> <pre><code>{\n  \"question\": \"What is the spatial resolution of Sentinel-2's visible bands?\",\n  \"choices\": [\"5 meters\", \"10 meters\", \"20 meters\", \"60 meters\"],\n  \"answer\": \"B\"  # \"10 meters\"\n}\n</code></pre>"},{"location":"evalkit/docs/eo_tasks/#open-ended","title":"Open Ended","text":"<p>Task Name: <code>open_ended</code></p> <p>Description:</p> <p>EVE-open-ended is a collection of ~969 open-ended question-answer pairs focused on Earth Observation. The dataset covers a wide range of EO topics including satellite imagery analysis, remote sensing techniques, environmental monitoring, and LiDAR. Models must generate free-form responses demonstrating deep understanding without the constraints of multiple-choice formats.</p> <p>How to Call:</p> <pre><code>tasks:\n  - name: open_ended\n    num_fewshot: 5\n    max_tokens: 40000\n    judge_api_key: !ref judge_api_key\n    judge_base_url: !ref judge_base_url\n    judge_name: !ref judge_name\n</code></pre> <p>Dataset:</p> <ul> <li>Source: eve-esa/open-ended</li> <li>Split: train</li> <li>Size: ~969 samples</li> <li>Structure: Each example contains a <code>Question</code> and <code>Answer</code> (reference)</li> </ul> <p>Evaluation Metrics:</p> <ul> <li>LLM as Judge: A judge model evaluates the quality, accuracy, and completeness of generated answers using strict fact-checking rules (0 = FAIL, 1 = PASS)</li> <li>Multi-Judge Support: Use multiple judges for more robust evaluation (see Multi-Judge Evaluation section below)</li> <li>Win Rate: Compare two models head-to-head using multiple LLM judges (see Win Rate Evaluation section below)</li> <li>Alternative metrics: BLEU, ROUGE, Cosine Similarity, BERTScore</li> </ul> <p>LLM Judge Evaluation Rules:</p> <ol> <li>Contradiction Check: Fails if the answer contains ANY fact contradicting the reference</li> <li>Relevance Check: Fails if the answer omits ESSENTIAL technical facts from the reference</li> <li>Additive Information: Additional correct information is acceptable if it doesn't contradict</li> <li>Focus on Substance: Ignores style, length, and tolerates minor phrasing differences</li> </ol> <p>Why It's Useful:</p> <p>This task assesses a model's ability to explain concepts, provide detailed answers, and demonstrate deep understanding. It's essential for evaluating models intended for educational or explanatory applications in EO, where nuanced explanations and technical accuracy are paramount.</p>"},{"location":"evalkit/docs/eo_tasks/#open-ended-with-context","title":"Open Ended with Context","text":"<p>Task Name: <code>open_ended_w_context</code></p> <p>Description:</p> <p>EVE-open-ended-w-context provides open-ended questions that must be answered using 1-3 accompanying context documents. This tests the model's ability to extract and synthesize information from reference materials, making it ideal for evaluating Retrieval-Augmented Generation (RAG) systems. Not all samples contain all three documents, requiring models to handle variable numbers of context documents gracefully.</p> <p>How to Call:</p> <pre><code>tasks:\n  - name: open_ended_w_context\n    num_fewshot: 5\n    max_tokens: 40000\n    judge_api_key: !ref judge_api_key\n    judge_base_url: !ref judge_base_url\n    judge_name: !ref judge_name\n</code></pre> <p>Dataset:</p> <ul> <li>Source: eve-esa/open-ended-w-context</li> <li>Split: train</li> <li>Structure: Each example contains a <code>Question</code>, <code>Answer</code>, and up to three context documents (<code>Doc 1</code>, <code>Doc 2</code>, <code>Doc 3</code>)</li> </ul> <p>Evaluation Metrics:</p> <ul> <li>LLM Judge: Evaluates whether answers are grounded in the provided context and correctly answer the question (higher is better)</li> <li>Multi-Judge Support: Use multiple judges for more robust evaluation (see Multi-Judge Evaluation section below)</li> <li>Win Rate: Compare two models head-to-head using multiple LLM judges (see Win Rate Evaluation section below)</li> <li>Uses the same strict fact-checking evaluation rules as open-ended tasks</li> </ul> <p>Why It's Useful:</p> <p>This task evaluates retrieval-augmented generation (RAG) capabilities, testing whether models can accurately extract information from provided documents rather than relying solely on parametric knowledge. This is crucial for applications where answers must be grounded in specific documentation or data sources. It also tests the model's ability to distinguish between context-provided information and pre-trained knowledge.</p> <p>Example:</p> <pre><code>{\n  \"Question\": \"What is the spatial resolution of Sentinel-2's visible bands?\",\n  \"Answer\": \"Sentinel-2's visible bands have a spatial resolution of 10 meters.\",\n  \"Doc 1\": \"The Sentinel-2 mission comprises a constellation...\",\n  \"Doc 2\": \"Sentinel-2 carries the Multi-Spectral Instrument (MSI)...\",\n  \"Doc 3\": \"\"  # May be empty\n}\n</code></pre>"},{"location":"evalkit/docs/eo_tasks/#refusal","title":"Refusal","text":"<p>Task Name: <code>refusal</code></p> <p>Description:</p> <p>EVE-Refusal tests whether language models can appropriately refuse to answer questions when the provided context does not contain sufficient information. The dataset presents questions alongside context documents that intentionally lack the necessary information to answer. A well-calibrated model should recognize this limitation and refuse to answer, rather than generating plausible but incorrect information.</p> <p>How to Call:</p> <pre><code>tasks:\n  - name: refusal\n    num_fewshot: 5\n    max_tokens: 40000\n    judge_api_key: !ref judge_api_key\n    judge_base_url: !ref judge_base_url\n    judge_name: !ref judge_name\n</code></pre> <p>Dataset:</p> <ul> <li>Source: eve-esa/refusal</li> <li>Split: train</li> <li>Structure: Each example contains a <code>question</code> and <code>context</code> (insufficient for answering)</li> <li>Expected Answer: \"I'm sorry, but the provided context does not contain enough information to answer that question.\"</li> </ul> <p>Evaluation Metrics:</p> <ul> <li>LLM Judge: Evaluates whether the model appropriately refuses to answer or acknowledges insufficient information (higher is better)</li> </ul> <p>Expected Behavior:</p> <ul> <li>Recognize when provided context lacks sufficient information</li> <li>Explicitly refuse to answer or state information is not available</li> <li>Avoid generating plausible-sounding but fabricated information</li> <li>Maintain accuracy and honesty over completeness</li> </ul> <p>Why It's Useful:</p> <p>This task tests a critical safety and reliability feature: the ability to recognize limitations and avoid generating potentially incorrect information when context is insufficient. This prevents hallucinations and ensures trustworthy behavior in production systems. It's particularly important for RAG systems and applications where factual accuracy is paramount.</p>"},{"location":"evalkit/docs/eo_tasks/#hallucination-detection","title":"Hallucination Detection","text":"<p>Task Name: <code>hallucination_detection</code></p> <p>Description:</p> <p>EVE-Hallucination is a specialized dataset for evaluating language models' tendency to hallucinate in the Earth Observation domain. Unlike typical QA datasets, this contains deliberately hallucinated answers with detailed annotations marking which portions of text are hallucinated. The task is to identify whether a given answer contains hallucinated (false or unsupported) information.</p> <p>How to Call:</p> <pre><code>tasks:\n  - name: hallucination_detection\n    num_fewshot: 0\n    max_tokens: 100\n    judge_api_key: !ref judge_api_key\n    judge_base_url: !ref judge_base_url\n    judge_name: !ref judge_name\n</code></pre> <p>Dataset:</p> <ul> <li>Source: eve-esa/hallucination-detection</li> <li>Split: train</li> <li>Structure: Each example contains <code>Question</code>, <code>Answer</code> (with hallucinations), <code>Soft labels</code> (probabilistic spans), and <code>Hard labels</code> (definite spans)</li> </ul> <p>Evaluation Metrics:</p> <ul> <li>Accuracy: Overall correctness of hallucination detection (higher is better)</li> <li>Precision: Ratio of correctly identified hallucinations to all predicted hallucinations (higher is better)</li> <li>Recall: Ratio of correctly identified hallucinations to all actual hallucinations (higher is better)</li> <li>F1 Score: Harmonic mean of precision and recall (higher is better)</li> </ul> <p>Task Levels:</p> <ol> <li>Binary Detection: Determine if answer contains any hallucinated information (yes/no)</li> <li>Hard Span Detection: Identify exact character spans that are hallucinated</li> <li>Soft Span Detection: Identify spans with confidence scores</li> </ol> <p>Why It's Useful:</p> <p>This task evaluates a model's ability to self-assess and identify unreliable or fabricated information in EO contexts. Models with strong hallucination detection capabilities are more trustworthy and can potentially be used to validate outputs from other systems. This is crucial for safety-critical applications like climate monitoring, disaster response, and environmental analysis.</p> <p>Example:</p> <pre><code>{\n  \"Question\": \"What is the spatial resolution of Sentinel-2's visible bands?\",\n  \"Answer\": \"Sentinel-2's visible bands have a spatial resolution of 5 meters, making it the highest resolution freely available satellite.\",\n  \"Hard labels\": [[52, 60], [73, 127]]  # Character spans that are hallucinated\n}\n</code></pre>"},{"location":"evalkit/docs/eo_tasks/#multi-judge-evaluation","title":"Multi-Judge Evaluation","text":"<p>For open-ended tasks (<code>open_ended</code>, <code>open_ended_w_context</code>, <code>open_ended_w_context_full</code>), you can use multi-judge evaluation where multiple LLM judges independently evaluate each answer. This approach provides more robust and reliable evaluation through consensus-based scoring.</p>"},{"location":"evalkit/docs/eo_tasks/#benefits","title":"Benefits","text":"<ul> <li>Reduced Bias: Individual judge biases are averaged out across multiple judges</li> <li>Voting Metric: Majority vote provides a robust, democratic final score</li> <li>Agreement Tracking: Monitor consensus to identify ambiguous or controversial samples</li> <li>Judge Analysis: Compare individual judges to identify systematic differences or biases</li> </ul>"},{"location":"evalkit/docs/eo_tasks/#configuration","title":"Configuration","text":"<p>Define multiple judges in your <code>evals.yaml</code> configuration:</p> <pre><code>constants:\n  judges:\n    - name: qwen3\n      model: qwen/qwen3-235b-a22b-2507\n      api_key: your_openrouter_api_key\n      base_url: https://openrouter.ai/api/v1/\n    - name: mistral_large\n      model: mistralai/mistral-large-2411\n      api_key: your_openrouter_api_key\n      base_url: https://openrouter.ai/api/v1/\n    - name: claude_sonnet\n      model: anthropic/claude-3.5-sonnet\n      api_key: your_openrouter_api_key\n      base_url: https://openrouter.ai/api/v1/\n\n  tasks:\n    - name: open_ended_multi_judge\n      task_name: open_ended\n      model_type: local-chat-completions\n      num_fewshot: 0\n      max_tokens: 10000\n      judges: !ref judges  # Use all judges defined above\n      batch_size: 15\n</code></pre>"},{"location":"evalkit/docs/eo_tasks/#metrics-produced","title":"Metrics Produced","text":"<p>When using multi-judge evaluation, the following metrics are automatically generated:</p>"},{"location":"evalkit/docs/eo_tasks/#1-individual-judge-scores","title":"1. Individual Judge Scores","text":"<ul> <li><code>llm_as_judge_{judge_name}</code>: Score from each individual judge (e.g., <code>llm_as_judge_qwen3</code>, <code>llm_as_judge_mistral_large</code>)</li> <li>Values: 0 or 1</li> <li>Use to identify systematic differences between judges</li> </ul>"},{"location":"evalkit/docs/eo_tasks/#2-voting-metric-recommended","title":"2. Voting Metric (Recommended)","text":"<ul> <li><code>judge_voting</code>: Majority vote result</li> <li>Returns the score that has majority support (&gt;= half+1 judges)</li> <li>For ties (even number of judges with equal votes), defaults to 0</li> <li>Values: 0 or 1</li> <li>This is the recommended primary metric</li> </ul>"},{"location":"evalkit/docs/eo_tasks/#3-average-score","title":"3. Average Score","text":"<ul> <li><code>llm_as_judge_avg</code>: Average score across all judges</li> <li>Values: 0.0 to 1.0</li> <li>Provides granular scores useful for ranking models</li> </ul>"},{"location":"evalkit/docs/eo_tasks/#4-agreement-metric","title":"4. Agreement Metric","text":"<ul> <li><code>judge_agreement</code>: Percentage of samples where all judges agree</li> <li>Values: 0.0 to 1.0</li> <li>High agreement (&gt;0.8) indicates judges are consistent</li> <li>Low agreement (&lt;0.5) suggests ambiguous questions or edge cases</li> </ul>"},{"location":"evalkit/docs/eo_tasks/#voting-examples","title":"Voting Examples","text":"<p>With 2 judges: - Both vote 1 \u2192 voting = 1 - Both vote 0 \u2192 voting = 0 - 1 votes 1, 1 votes 0 \u2192 voting = 0 (tie, no majority)</p> <p>With 3 judges: - 2 vote 1, 1 votes 0 \u2192 voting = 1 (majority) - 1 votes 1, 2 vote 0 \u2192 voting = 0 (majority) - All vote 1 \u2192 voting = 1 (unanimous)</p> <p>With 5 judges: - 3 vote 1, 2 vote 0 \u2192 voting = 1 (majority) - 2 vote 1, 3 vote 0 \u2192 voting = 0 (majority)</p>"},{"location":"evalkit/docs/eo_tasks/#recommendations","title":"Recommendations","text":"<p>Number of Judges: - 3 judges: Good balance between cost and reliability (recommended) - 5 judges: Better for high-stakes evaluations - 2 judges: Avoid if possible (risk of ties with no clear majority)</p> <p>Judge Selection: - Use diverse models (different architectures/providers) - Mix model sizes (small + large models) - Include both specialized and general-purpose models - Example: Claude, GPT-4, Mistral Large, Qwen</p> <p>Cost Optimization: 1. Start with 3 judges on a small sample (limit: 10-50) 2. Analyze the agreement rate 3. If agreement is high (&gt;0.8), consider using single judge or voting metric 4. If agreement is low (&lt;0.5), investigate question quality or add more judges</p>"},{"location":"evalkit/docs/eo_tasks/#example-output","title":"Example Output","text":"<pre><code>{\n  \"results\": {\n    \"open_ended\": {\n      \"llm_as_judge_qwen3\": 0.75,\n      \"llm_as_judge_mistral_large\": 0.80,\n      \"llm_as_judge_claude_sonnet\": 0.78,\n      \"llm_as_judge_avg\": 0.777,\n      \"judge_agreement\": 0.65,\n      \"judge_voting\": 0.80\n    }\n  }\n}\n</code></pre> <p>In this example: - Individual judges scored 75%, 80%, and 78% - Overall average is 77.7% - Judges fully agreed on 65% of samples - Majority vote gave 80% (recommended metric to report)</p>"},{"location":"evalkit/docs/eo_tasks/#win-rate-evaluation","title":"Win Rate Evaluation","text":"<p>For open-ended tasks (<code>open_ended</code>, <code>open_ended_w_context</code>), you can perform win rate evaluation to compare two models head-to-head using multiple LLM judges. This separate evaluation script provides comparative analysis between model outputs, complementing the standard LLM-as-judge metrics.</p>"},{"location":"evalkit/docs/eo_tasks/#what-is-win-rate","title":"What is Win Rate?","text":"<p>Win rate evaluation compares the outputs of two models (Model A vs Model B) on the same questions and determines which model provides better answers according to independent LLM judges. This approach is particularly useful for:</p> <ul> <li>Model Selection: Directly compare two models to identify which performs better</li> <li>Model Improvement: Assess whether a new model version improves over a baseline</li> <li>Ablation Studies: Evaluate the impact of specific model changes or training approaches</li> <li>Benchmark Comparison: Compare your model against established baselines or competitors</li> </ul>"},{"location":"evalkit/docs/eo_tasks/#key-differences-from-standard-evaluation","title":"Key Differences from Standard Evaluation","text":"<p>The win rate evaluation is a separate script with its own configuration format and workflow:</p> Aspect Standard Evaluation Win Rate Evaluation Purpose Evaluate single model quality Compare two models head-to-head Script <code>scripts/evaluate.py</code> <code>metrics/win_rate/win_rate_evaluation.py</code> Configuration <code>evals.yaml</code> Separate YAML config (e.g., <code>win_rate_config.yaml</code>) Input Live model API Pre-generated CSV files from standard evaluation Output Format Standard metrics (accuracy, F1) Win rates, alpaca win rates, judge agreement Judges Single or multi-judge per eval Multiple judges comparing two outputs WandB Logging Evaluation results Win rate metrics, visualizations, judge rationales"},{"location":"evalkit/docs/eo_tasks/#metrics-explained","title":"Metrics Explained","text":""},{"location":"evalkit/docs/eo_tasks/#1-win-rate","title":"1. Win Rate","text":"<p>Percentage of questions where each model won according to each judge.</p> <p>Formula:</p> <pre><code>Win Rate = (Number of Wins / Total Evaluations) \u00d7 100%\n</code></pre> <p>Calculation Process: - For each question, each judge compares Model A vs Model B outputs - Judge decides: Model A wins, Model B wins, or Tie - Win rate = (wins / total evaluations) \u00d7 100% - Aggregate win rate = average win rate across all judges</p> <p>Interpreting Win Rate Values: - 0.50 (50%): Models perform equally well (perfect tie across all questions) - &gt; 0.50: Model is better than its competitor   - 0.55-0.60: Slight advantage   - 0.60-0.70: Clear advantage   - 0.70+: Strong advantage - &lt; 0.50: Model is worse than its competitor - Win Rate Difference: The gap between Model A and Model B   - Difference &lt; 0.05: Negligible difference   - Difference 0.05-0.10: Noticeable difference   - Difference &gt; 0.10: Significant performance gap</p> <p>Logged Metrics: - <code>win_rate/{judge_name}/{model_name}</code>: Win rate for each model per judge (0.0 to 1.0) - <code>aggregate/{model_name}_win_rate</code>: Aggregate win rate across all judges - <code>aggregate/avg_{model_name}_win_rate</code>: Average win rate across judges - <code>aggregate/win_rate_difference</code>: Difference between Model A and Model B win rates</p> <p>Example:</p> <pre><code>Model A wins: 72 questions\nModel B wins: 25 questions\nTies: 3 questions\nTotal: 100 questions\n\nModel A Win Rate = 72/100 = 0.72 (72%)\nModel B Win Rate = 25/100 = 0.25 (25%)\nWin Rate Difference = 0.72 - 0.25 = 0.47 (47% gap - significant advantage for Model A)\n</code></pre>"},{"location":"evalkit/docs/eo_tasks/#2-alpaca-win-rate","title":"2. Alpaca Win Rate","text":"<p>A more nuanced metric that counts ties as half a win for each model, based on the AlpacaEval framework.</p> <p>Formula:</p> <pre><code>Alpaca Win Rate = (Number of Wins + 0.5 \u00d7 Number of Ties) / Total Evaluations\n</code></pre> <p>Why Use Alpaca Win Rate? - Treats ties as split decisions, giving partial credit to both models - More granular than standard win rate when there are many ties - Better reflects cases where models perform similarly on some questions - Recommended by AlpacaEval for instruction-following model comparisons</p> <p>Interpreting Alpaca Win Rate Values: - 0.50 (50%): Models perform equally well - &gt; 0.50: Model is better (same interpretation as standard win rate) - Alpaca win rate is always \u2265 standard win rate (due to partial credit for ties) - Use alpaca win rate when you have many ties and want more nuanced comparison</p> <p>Logged Metrics: - <code>alpaca_win_rate/{judge_name}/{model_name}</code>: Alpaca win rate per judge - <code>aggregate/{model_name}_alpaca_win_rate</code>: Aggregate alpaca win rate - <code>aggregate/avg_{model_name}_alpaca_win_rate</code>: Average alpaca win rate across judges - <code>aggregate/alpaca_win_rate_difference</code>: Difference between Model A and Model B alpaca win rates</p> <p>Example:</p> <pre><code>Model A wins: 72 questions\nModel B wins: 25 questions\nTies: 3 questions\nTotal: 100 questions\n\nModel A Alpaca Win Rate = (72 + 0.5\u00d73)/100 = 73.5/100 = 0.735 (73.5%)\nModel B Alpaca Win Rate = (25 + 0.5\u00d73)/100 = 26.5/100 = 0.265 (26.5%)\n\nNote: Both models get partial credit for the 3 ties\n</code></pre> <p>Reference: AlpacaEval - Automatic Evaluator for Instruction-following LLMs</p>"},{"location":"evalkit/docs/eo_tasks/#3-judge-agreement","title":"3. Judge Agreement","text":"<p>Measures how consistently judges agree on which model is better:</p> <ul> <li>Unanimous (1.0): All judges made the same decision</li> <li>Majority (0.5-0.99): Most judges agreed on winner</li> <li>Split (&lt; 0.5): Judges were evenly divided</li> </ul> <p>Interpreting Agreement: - &gt; 0.80: High agreement - clear quality difference or consistent evaluation - 0.50-0.80: Moderate agreement - some subjective variation among judges - &lt; 0.50: Low agreement - questions may be ambiguous or judges have different criteria</p>"},{"location":"evalkit/docs/eo_tasks/#4-position-bias","title":"4. Position Bias","text":"<p>Analysis of whether judges are biased toward answers shown in position A vs B. The evaluation randomizes answer positions to mitigate this bias.</p> <p>What to Look For: - Position bias close to 0.50 (50%) indicates no position bias - Significant deviation from 0.50 suggests judges prefer one position - Randomization helps ensure fair comparison despite any position bias</p>"},{"location":"evalkit/docs/eo_tasks/#how-to-run-win-rate-evaluation","title":"How to Run Win Rate Evaluation","text":""},{"location":"evalkit/docs/eo_tasks/#step-1-generate-model-outputs","title":"Step 1: Generate Model Outputs","text":"<p>First, run standard evaluation to generate CSV files with model outputs:</p> <pre><code>python scripts/evaluate.py evals.yaml\n</code></pre> <p>This creates CSV files in your output directory (e.g., <code>evals_outputs/{model_name}/samples_open_ended.csv</code>) with columns: - <code>doc\\.Question</code>: The question text - <code>target</code>: The reference/ground truth answer - <code>filtered_resps</code>: The model's response</p>"},{"location":"evalkit/docs/eo_tasks/#step-2-create-win-rate-configuration","title":"Step 2: Create Win Rate Configuration","text":"<p>Create a YAML configuration file (e.g., <code>win_rate_config.yaml</code>):</p> <pre><code># Task type for metrics logging\ntask: \"open_ended\"  # or \"open_ended_w_context\"\n\n# Models to compare\nmodel_a:\n  - name: \"model-a-name\"\n    file: \"path/to/model_a_output.csv\"\n\nmodel_b:\n  - name: \"model-b-name\"\n    file: \"path/to/model_b_output.csv\"\n\n# LLM Judges configuration\njudges:\n  - name: \"mistral-large\"\n    model: \"mistral-large-2512\"\n    api_key: \"${MISTRAL_API_KEY}\"\n    base_url: \"https://api.mistral.ai/v1\"\n\n  - name: \"gpt-4-mini\"\n    model: \"openai/gpt-4.1-mini\"\n    api_key: \"${OPENROUTER_API_KEY}\"\n    base_url: \"https://openrouter.ai/api/v1/\"\n\n  - name: \"qwen3-235b\"\n    model: \"qwen/qwen3-235b-a22b-2507\"\n    api_key: \"${OPENROUTER_API_KEY}\"\n    base_url: \"https://openrouter.ai/api/v1/\"\n\n# Evaluation settings\nevaluation:\n  limit: null  # Set to N to limit to first N questions, or null for all\n  max_workers: 20  # Number of parallel threads\n  rate_limit_delay: 0.05  # Delay between API calls (seconds)\n  random_seed: 42  # For reproducibility (null = random)\n\n# Output settings\noutput:\n  save_results: true\n  save_visualizations: true\n  output_dir: \"win_rate_results\"\n  results_filename: \"results_{model_a}_vs_{model_b}.csv\"\n  summary_filename: \"summary_{model_a}_vs_{model_b}.csv\"\n  visualization_filename: \"comparison_{model_a}_vs_{model_b}.png\"\n\n# Weights &amp; Biases configuration\nwandb:\n  enabled: true\n  project: \"eve-win-rate-evaluation\"\n  entity: your-wandb-username  # or null for default\n  run_name: \"{model_a}_vs_{model_b}\"\n  tags:\n    - \"win-rate\"\n    - \"llm-judge\"\n\n  # What to log\n  log:\n    win_rates: true\n    accuracy_rates: true\n    judge_agreement: true\n    position_bias: true\n    visualizations: true\n    raw_results: true\n    sample_rationales: true\n    sample_count: 5\n</code></pre>"},{"location":"evalkit/docs/eo_tasks/#step-3-run-win-rate-evaluation","title":"Step 3: Run Win Rate Evaluation","text":"<pre><code>python metrics/win_rate/win_rate_evaluation.py --config win_rate_config.yaml\n</code></pre>"},{"location":"evalkit/docs/eo_tasks/#step-4-view-results","title":"Step 4: View Results","text":"<p>The script generates:</p> <ol> <li>CSV Results (<code>win_rate_results/results_*.csv</code>): Complete evaluation data with all judge decisions</li> <li>Summary CSV (<code>win_rate_results/summary_*.csv</code>): Win rate statistics per judge</li> <li>Visualizations (<code>win_rate_results/comparison_*.png</code>): Charts showing win rates</li> <li>WandB Dashboard: Interactive results with metrics, visualizations, and sample rationales</li> </ol>"},{"location":"evalkit/docs/eo_tasks/#comparing-multiple-models","title":"Comparing Multiple Models","text":"<p>You can compare multiple models pairwise by configuring lists for <code>model_a</code> and <code>model_b</code>:</p> <pre><code>model_a:\n  - name: \"eve_v04\"\n    file: \"generations/open_ended/scoring_eve_v04_open_ended_0_shot.csv\"\n  - name: \"eve_v05\"\n    file: \"generations/open_ended/scoring_eve_v05_open_ended_0_shot.csv\"\n\nmodel_b:\n  - name: \"mistral-small\"\n    file: \"generations/open_ended/scoring_mistral-small_open_ended_0_shot.csv\"\n  - name: \"llama-4-scout\"\n    file: \"generations/open_ended/scoring_llama-4-scout_open_ended_0_shot.csv\"\n</code></pre> <p>This will run all pairwise comparisons: eve_v04 vs mistral-small, eve_v04 vs llama-4-scout, eve_v05 vs mistral-small, eve_v05 vs llama-4-scout.</p>"},{"location":"evalkit/docs/eo_tasks/#best-practices","title":"Best Practices","text":"<ol> <li>Number of Judges: Use 3-5 judges for robust evaluation</li> <li>Judge Diversity: Select judges from different model families (e.g., GPT, Claude, Mistral, Qwen)</li> <li>Rate Limiting: Adjust <code>rate_limit_delay</code> if hitting API rate limits</li> <li>Reproducibility: Set <code>random_seed</code> to a fixed value for reproducible position randomization</li> <li>Testing: Start with <code>limit: 10</code> to test configuration before running full evaluation</li> <li>WandB Tracking: Enable wandb logging to track experiments and compare runs</li> </ol>"},{"location":"evalkit/docs/eo_tasks/#example-output_1","title":"Example Output","text":"<p>After running win rate evaluation, you'll see output like:</p> <pre><code>Model A: eve_v05\nModel B: mistral-small-3.2-24b\n\n=== Aggregate Results ===\neve_v05:\n  Win Rate: 0.72 (72%)\n  Alpaca Win Rate: 0.75 (75%)\n\nmistral-small-3.2-24b:\n  Win Rate: 0.28 (28%)\n  Alpaca Win Rate: 0.25 (25%)\n\nJudge Agreement: 0.68 (68% unanimous decisions)\n\nWandB Run: https://wandb.ai/your-entity/eve-win-rate-evaluation/runs/...\n</code></pre> <p>Interpretation: eve_v05 clearly outperforms mistral-small-3.2-24b with a 44% win rate gap (0.72 - 0.28), indicating strong superiority across the evaluated questions.</p>"},{"location":"evalkit/docs/eo_tasks/#references","title":"References","text":"<ul> <li>AlpacaEval Framework: https://github.com/tatsu-lab/alpaca_eval</li> <li>Full Documentation: See <code>WIN_RATE_EVALUATION_README.md</code> in the repository root</li> <li>Example Configurations: See <code>metrics/win_rate/win_rate_open_ended_example.yaml</code></li> </ul>"},{"location":"evalkit/docs/eo_tasks/#running-tasks","title":"Running Tasks","text":""},{"location":"evalkit/docs/eo_tasks/#using-configuration-file","title":"Using Configuration File","text":"<p>Add tasks to your <code>evals.yaml</code>:</p> <pre><code>constants:\n  judge_api_key: your-judge-api-key\n  judge_base_url: https://openrouter.ai/api/v1\n  judge_name: mistralai/mistral-large-2411\n  tasks:\n    - name: mcqa_multiple_answers\n      num_fewshot: 2\n      max_tokens: 10000\n    - name: hallucination_detection\n      num_fewshot: 0\n      max_tokens: 100\n\nmodels:\n  - name: your-model-name\n    base_url: https://api.provider.com/v1/chat/completions\n    api_key: your-api-key\n    temperature: 0.1\n    num_concurrent: 5\n    tasks: !ref tasks\n\noutput_dir: evals_outputs\n</code></pre> <p>Then run:</p> <pre><code>python scripts/evaluate.py evals.yaml\n</code></pre>"},{"location":"evalkit/docs/eo_tasks/#direct-command-line","title":"Direct Command Line","text":"<pre><code>lm_eval --model openai-chat-completions \\\n        --model_args base_url=https://api.provider.com,model=model-name,num_concurrent=5 \\\n        --tasks {task_name} \\\n        --include tasks \\\n        --num_fewshot 0 \\\n        --output_path ./outputs \\\n        --log_samples \\\n        --apply_chat_template\n</code></pre> <p>For tasks using LLM-as-judge metrics, set environment variables:</p> <pre><code>export JUDGE_API_KEY=your-judge-api-key\nexport JUDGE_BASE_URL=https://api.provider.com/v1\nexport JUDGE_NAME=judge-model-name\n</code></pre>"},{"location":"evalkit/docs/eo_tasks/#task-selection-guide","title":"Task Selection Guide","text":"<p>Choose tasks based on your evaluation goals:</p> <p>Factual Knowledge: - <code>mcqa_single_answer</code> - Single correct answer questions - <code>mcqa_multiple_answers</code> - Multiple correct answers with partial credit</p> <p>Generation Quality: - <code>open_ended</code> - Free-form explanatory answers</p> <p>Grounded Generation (RAG): - <code>open_ended_w_context</code> - Answer questions using provided documents - <code>refusal</code> - Recognize when context is insufficient</p> <p>Reliability &amp; Safety: - <code>hallucination_detection</code> - Identify fabricated information - <code>refusal</code> - Avoid answering without sufficient information</p> <p>Comprehensive Evaluation: - Run all tasks for a complete assessment across different capabilities</p>"},{"location":"evalkit/docs/eo_tasks/#evaluation-best-practices","title":"Evaluation Best Practices","text":"<ol> <li>Use Few-Shot Examples: Most tasks benefit from few-shot examples (typically 2-5) to demonstrate the expected format</li> <li>Set Appropriate Timeouts: Some tasks require longer generation, so adjust timeouts accordingly</li> <li>Configure Judge Model: For LLM-as-judge tasks, choose a capable judge model (e.g., GPT-4, Claude 3.5 Sonnet, Mistral Large)</li> <li>Log Samples: Always use <code>--log_samples</code> to inspect individual predictions and understand model behavior</li> <li>Monitor Costs: LLM-as-judge evaluation can be expensive; consider using smaller subsets for initial testing</li> </ol>"},{"location":"evalkit/docs/eo_tasks/#additional-resources","title":"Additional Resources","text":"<ul> <li>Dataset Repository: https://huggingface.co/eve-esa</li> <li>GitHub Repository: https://github.com/eve-esa/eve-evaluation</li> <li>LM Evaluation Harness: https://github.com/EleutherAI/lm-evaluation-harness</li> </ul>"},{"location":"evalkit/docs/eo_tasks/#citation","title":"Citation","text":"<p>If you use these tasks or datasets in your research, please cite:</p> <pre><code>@misc{eve2025,\n  title={EVE: Earth Virtual Expert},\n  author={ESA},\n  year={2025},\n  publisher={HuggingFace},\n  url={https://huggingface.co/eve-esa/eve_v0.1}\n}\n</code></pre> <p>For the underlying evaluation framework:</p> <pre><code>@software{eval-harness,\n  author       = {Gao, Leo and others},\n  title        = {A framework for few-shot language model evaluation},\n  month        = sep,\n  year         = 2021,\n  publisher    = {Zenodo},\n  version      = {v0.0.1},\n  doi          = {10.5281/zenodo.5371628},\n  url          = {https://doi.org/10.5281/zenodo.5371628}\n}\n</code></pre>"},{"location":"evalkit/docs/evalkit/","title":"Evalkit","text":"<p>Metrics module for EVE evaluation.</p>"},{"location":"evalkit/docs/evalkit/#metrics.LoggableFuture","title":"<code>LoggableFuture</code>","text":"<p>Wrapper around Future that provides a nicer string representation for logging.</p> Source code in <code>evalkit/metrics/judge_utils.py</code> <pre><code>class LoggableFuture:\n    \"\"\"Wrapper around Future that provides a nicer string representation for logging.\"\"\"\n\n    def __init__(self, future: Future):\n        self._future = future\n\n    def result(self, timeout=None):\n        \"\"\"Get the result from the underlying future.\"\"\"\n        return self._future.result(timeout=timeout)\n\n    def __repr__(self):\n        \"\"\"Return a nicer representation for logging.\"\"\"\n        if self._future.done():\n            try:\n                return str(self._future.result())\n            except Exception as e:\n                return f\"&lt;Failed: {type(e).__name__}&gt;\"\n        else:\n            return \"&lt;Pending LLM judge evaluation&gt;\"\n\n    def __str__(self):\n        return self.__repr__()\n</code></pre>"},{"location":"evalkit/docs/evalkit/#metrics.LoggableFuture.__repr__","title":"<code>__repr__()</code>","text":"<p>Return a nicer representation for logging.</p> Source code in <code>evalkit/metrics/judge_utils.py</code> <pre><code>def __repr__(self):\n    \"\"\"Return a nicer representation for logging.\"\"\"\n    if self._future.done():\n        try:\n            return str(self._future.result())\n        except Exception as e:\n            return f\"&lt;Failed: {type(e).__name__}&gt;\"\n    else:\n        return \"&lt;Pending LLM judge evaluation&gt;\"\n</code></pre>"},{"location":"evalkit/docs/evalkit/#metrics.LoggableFuture.result","title":"<code>result(timeout=None)</code>","text":"<p>Get the result from the underlying future.</p> Source code in <code>evalkit/metrics/judge_utils.py</code> <pre><code>def result(self, timeout=None):\n    \"\"\"Get the result from the underlying future.\"\"\"\n    return self._future.result(timeout=timeout)\n</code></pre>"},{"location":"evalkit/docs/evalkit/#metrics.aggregate_llm_judge","title":"<code>aggregate_llm_judge(items)</code>","text":"<p>Aggregate LLM judge results by waiting for futures and calculating mean.</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>Union[List[LoggableFuture], List[Future], List[LoggableFutureExtractor]]</code> <p>List of LoggableFuture or Future objects containing score dictionaries.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Mean score across all items, or 0.0 if items is empty.</p> Source code in <code>evalkit/metrics/judge_utils.py</code> <pre><code>def aggregate_llm_judge(\n    items: Union[\n        List[LoggableFuture],\n        List[Future],\n        List[LoggableFutureExtractor],\n    ],\n) -&gt; float:\n    \"\"\"\n    Aggregate LLM judge results by waiting for futures and calculating mean.\n\n    Args:\n        items: List of LoggableFuture or Future objects containing score dictionaries.\n\n    Returns:\n        Mean score across all items, or 0.0 if items is empty.\n    \"\"\"\n    if not items:\n        return 0.0\n\n    # Handle both LoggableFuture and regular Future objects\n    scores = []\n    for item in items:\n        # Get result from any future-like object\n        if hasattr(item, \"result\"):\n            result = item.result()\n        else:\n            result = item\n\n        # Handle both dict format (new) and int format (backward compatibility)\n        if isinstance(result, dict):\n            # Check if this is multi-judge result\n            if all(isinstance(v, dict) and \"score\" in v for v in result.values()):\n                # Multi-judge: average across all judges for this sample\n                judge_scores = [v[\"score\"] for v in result.values()]\n                scores.append(mean(judge_scores))\n            elif \"score\" in result:\n                # Single judge dict format\n                scores.append(result[\"score\"])\n        elif isinstance(result, (int, float)):\n            scores.append(result)\n\n    return mean(scores) if scores else 0.0  # TODO - update this\n</code></pre>"},{"location":"evalkit/docs/evalkit/#metrics.judge_qa_with_llm","title":"<code>judge_qa_with_llm(sample, model_name=None, api_key=None, base_url=None, max_tokens=100, prompt_template=None)</code>","text":"<p>Calls the LLM judge for QA evaluation with binary (0/1) scoring.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict</code> <p>Dictionary with \"question\", \"output\", and \"reference\" keys.</p> required <code>model_name</code> <code>Optional[str]</code> <p>Name of the judge model. If None, reads from JUDGE_NAME env var.</p> <code>None</code> <code>api_key</code> <code>Optional[str]</code> <p>API key for the judge. If None, uses environment variables.</p> <code>None</code> <code>base_url</code> <code>Optional[str]</code> <p>Base URL for the API. If None, uses environment variables.</p> <code>None</code> <code>max_tokens</code> <code>int</code> <p>Maximum tokens for judge response. Default is 100.</p> <code>100</code> <code>prompt_template</code> <code>Optional[str]</code> <p>Custom prompt template to use. If None, uses default template.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict</code> <p>Dictionary with \"score\" (int) and \"raw_output\" (str) keys.</p> Source code in <code>evalkit/metrics/judge_utils.py</code> <pre><code>def judge_qa_with_llm(\n    sample: dict,\n    model_name: Optional[str] = None,\n    api_key: Optional[str] = None,\n    base_url: Optional[str] = None,\n    max_tokens: int = 100,\n    prompt_template: Optional[str] = None,\n) -&gt; Dict:\n    \"\"\"\n    Calls the LLM judge for QA evaluation with binary (0/1) scoring.\n\n    Args:\n        sample: Dictionary with \"question\", \"output\", and \"reference\" keys.\n        model_name: Name of the judge model. If None, reads from JUDGE_NAME env var.\n        api_key: API key for the judge. If None, uses environment variables.\n        base_url: Base URL for the API. If None, uses environment variables.\n        max_tokens: Maximum tokens for judge response. Default is 100.\n        prompt_template: Custom prompt template to use. If None, uses default template.\n\n    Returns:\n        Dictionary with \"score\" (int) and \"raw_output\" (str) keys.\n    \"\"\"\n    if model_name is None:\n        model_name = os.getenv(\"JUDGE_NAME\") or os.getenv(\"JUDGE_MODEL\") or \"mistral-large-latest\"\n\n    client = get_judge_client(api_key=api_key, base_url=base_url)\n\n    # Use custom prompt template if provided, otherwise use default\n    if prompt_template is None:\n        prompt_template = get_qa_prompt_template()\n\n    # JSON schema for structured output (OpenAI-style)\n    json_schema = {\n        \"type\": \"json_schema\",\n        \"json_schema\": {\n            \"name\": \"judge_score\",\n            \"strict\": True,\n            \"schema\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"score\": {\n                        \"type\": \"integer\",\n                        \"enum\": [0, 1, 2, 3, 4, 5],\n                        \"description\": \"Score from 0 (fail) to 5 (excellent)\",\n                    }\n                },\n                \"required\": [\"score\"],\n                \"additionalProperties\": False,\n            },\n        },\n    }\n\n    # Simple schema for format instructions\n    simple_schema = {\n        \"type\": \"object\",\n        \"properties\": {\n            \"score\": {\n                \"type\": \"integer\",\n                \"enum\": [0, 1, 2, 3, 4, 5],\n                \"description\": \"Score from 0 (fail) to 5 (excellent).\",\n            }\n        },\n        \"required\": [\"score\"],\n    }\n\n    format_instructions = (\n        \"You must respond with a JSON object that strictly follows this schema:\\n\"\n        f\"{json.dumps(simple_schema, indent=2)}\"\n    )\n\n    final_prompt = prompt_template.format(\n        question=sample[\"question\"],\n        output=sample[\"output\"],\n        reference=sample[\"reference\"],\n        format_instructions=format_instructions,\n    )\n\n    try:\n        # Try structured outputs first (OpenAI-compatible APIs)\n        response = None\n        try:\n            response = client.chat.completions.create(\n                model=model_name,\n                messages=[{\"role\": \"user\", \"content\": final_prompt}],\n                response_format=json_schema,\n                temperature=0.0,\n                max_tokens=max_tokens,\n            )\n        except Exception as e_structured:\n            # Fall back to basic JSON mode if structured outputs not supported\n            print(\n                f\"[DEBUG] Structured output failed for {model_name}, trying basic JSON mode: {str(e_structured)[:100]}\"\n            )\n            try:\n                response = client.chat.completions.create(\n                    model=model_name,\n                    messages=[{\"role\": \"user\", \"content\": final_prompt}],\n                    response_format={\"type\": \"json_object\"},\n                    temperature=0.0,\n                    max_tokens=max_tokens,\n                )\n            except Exception as e_json:\n                # Fall back to no JSON mode (for models that don't support it)\n                print(\n                    f\"[DEBUG] JSON mode failed for {model_name}, using plain text: {str(e_json)[:100]}\"\n                )\n                response = client.chat.completions.create(\n                    model=model_name,\n                    messages=[{\"role\": \"user\", \"content\": final_prompt}],\n                    temperature=0.0,\n                    max_tokens=max_tokens,\n                    extra_body={},  # OpenRouter compatibility\n                )\n\n        response_content = response.choices[0].message.content\n\n        # Debug: Print raw response\n        # print(\n        #     f\"[DEBUG] Judge {model_name} raw response: {response_content[:200] if response_content else 'None'}\"\n        # )\n\n        if response_content is None or response_content.strip() == \"\":\n            raise ValueError(\"LLM response content is None or empty\")\n\n        # Try to parse JSON\n        try:\n            data = json.loads(response_content)\n        except json.JSONDecodeError as e:\n            print(f\"[ERROR] JSON parse error for {model_name}: {e}\")\n            print(f\"[ERROR] Raw response content: {repr(response_content)}\")\n            print(f\"[ERROR] Response length: {len(response_content) if response_content else 0}\")\n            raise ValueError(f\"Failed to parse JSON response: {str(e)}\")\n\n        score = data.get(\"score\")\n\n        if score in [0, 1, 2, 3, 4, 5]:\n            return {\"score\": int(score), \"raw_output\": response_content}\n        else:\n            print(f\"[WARNING] Judge {model_name} returned invalid score: {score}. Defaulting to 0.\")\n            print(f\"[WARNING] Full response: {response_content}\")\n            print(f\"[WARNING] Question: {sample['question'][:100]}...\")\n            print(f\"[WARNING] Output: {sample['output'][:100]}...\")\n            return {\"score\": 0, \"raw_output\": response_content}\n\n    except Exception as e:\n        print(f\"[ERROR] Error during LLM judge call with {model_name}: {e}\")\n        print(f\"[ERROR] Error type: {type(e).__name__}\")\n        print(f\"[ERROR] Question: {sample['question'][:100]}...\")\n        print(f\"[ERROR] Model output: {sample['output'][:100]}...\")\n\n        # Try to get more details about the response\n        try:\n            if response:\n                print(f\"[ERROR] Response object exists: {type(response)}\")\n                if hasattr(response, \"choices\") and response.choices:\n                    print(f\"[ERROR] Response has {len(response.choices)} choices\")\n                    if response.choices[0].message:\n                        print(\n                            f\"[ERROR] Message content: {repr(response.choices[0].message.content)}\"\n                        )\n        except Exception as debug_e:\n            print(f\"[ERROR] Could not get response details: {debug_e}\")\n\n        error_msg = f\"Error: {str(e)}\"\n        return {\"score\": 0, \"raw_output\": error_msg}\n</code></pre>"},{"location":"evalkit/docs/evalkit/#metrics.process_qa_results","title":"<code>process_qa_results(doc, results, question_key='question', answer_key='answer', sleep_time=0.0, judges=None)</code>","text":"<p>Process QA results with LLM judge(s) in background thread.</p> <p>Parameters:</p> Name Type Description Default <code>doc</code> <code>dict</code> <p>Document dictionary containing the question and reference answer.</p> required <code>results</code> <code>list[str]</code> <p>List of model outputs (first element is used).</p> required <code>question_key</code> <code>str</code> <p>Key in doc for the question text.</p> <code>'question'</code> <code>answer_key</code> <code>str</code> <p>Key in doc for the reference answer.</p> <code>'answer'</code> <code>sleep_time</code> <code>float</code> <p>Optional delay before submitting (for rate limiting).</p> <code>0.0</code> <code>judges</code> <code>Optional[List[Dict]]</code> <p>Optional list of judge configurations for multi-judge evaluation. If None, uses single judge from environment variables.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with judge results. If multiple judges, creates separate keys: {     \"llm_as_judge_\": LoggableFuture for each judge,     \"llm_as_judge_avg\": LoggableFuture with all results } <code>dict</code> <p>If single judge: {\"llm_as_judge\": LoggableFuture}</p> Source code in <code>evalkit/metrics/judge_utils.py</code> <pre><code>def process_qa_results(\n    doc: dict,\n    results: list[str],\n    question_key: str = \"question\",\n    answer_key: str = \"answer\",\n    sleep_time: float = 0.0,\n    judges: Optional[List[Dict]] = None,\n) -&gt; dict:\n    \"\"\"\n    Process QA results with LLM judge(s) in background thread.\n\n    Args:\n        doc: Document dictionary containing the question and reference answer.\n        results: List of model outputs (first element is used).\n        question_key: Key in doc for the question text.\n        answer_key: Key in doc for the reference answer.\n        sleep_time: Optional delay before submitting (for rate limiting).\n        judges: Optional list of judge configurations for multi-judge evaluation.\n            If None, uses single judge from environment variables.\n\n    Returns:\n        Dictionary with judge results. If multiple judges, creates separate keys:\n            {\n                \"llm_as_judge_&lt;name&gt;\": LoggableFuture for each judge,\n                \"llm_as_judge_avg\": LoggableFuture with all results\n            }\n        If single judge:\n            {\"llm_as_judge\": LoggableFuture}\n    \"\"\"\n    sample = {\n        \"question\": doc[question_key],\n        \"output\": results[0],\n        \"reference\": doc[answer_key],\n    }\n\n    if judges and len(judges) &gt; 0:\n        # Multi-judge mode\n        future = _EXECUTOR.submit(judge_qa_with_multiple_llms, sample, judges)\n\n        if sleep_time &gt; 0:\n            time.sleep(sleep_time)\n\n        # Create separate futures for each judge that will extract their specific results\n        result_dict = {}\n\n        # Store the complete multi-judge result\n        result_dict[\"llm_as_judge_avg\"] = LoggableFuture(future)\n\n        # Create individual judge metrics\n        for judge_config in judges:\n            judge_name = judge_config.get(\"name\", judge_config.get(\"model\", \"unknown\"))\n            # Create a future wrapper that extracts this specific judge's result\n            result_dict[f\"llm_as_judge_{judge_name}\"] = LoggableFutureExtractor(future, judge_name)\n\n        return result_dict\n    else:\n        # Single judge mode (backward compatibility)\n        future = _EXECUTOR.submit(judge_qa_with_llm, sample)\n\n        if sleep_time &gt; 0:\n            time.sleep(sleep_time)\n\n        return {\"llm_as_judge\": LoggableFuture(future)}\n</code></pre>"},{"location":"evalkit/docs/examples/","title":"Examples","text":"<p>This page provides comprehensive examples of how to configure and run evaluations with Eve-evalkit. All examples use the YAML configuration format with the <code>evaluate.py</code> script.</p>"},{"location":"evalkit/docs/examples/#basic-structure","title":"Basic Structure","text":"<p>Every configuration file has the following structure:</p> <pre><code>constants:          # Define reusable values\n  # ...\n\nwandb:             # Optional: WandB integration\n  # ...\n\nmodels:            # One or more models to evaluate\n  # ...\n\noutput_dir:        # Where to save results\n</code></pre>"},{"location":"evalkit/docs/examples/#example-1-eve-earth-observation-tasks","title":"Example 1: EVE Earth Observation Tasks","text":"<p>Evaluate a model on Earth Observation-specific tasks:</p> <pre><code>constants:\n  judge_api_key: sk-or-v1-xxxxx\n  judge_base_url: https://openrouter.ai/api/v1\n  judge_name: mistralai/mistral-large-2411\n\n  tasks:\n\n    - name: mcqa_single_answer\n      num_fewshot: 2\n      max_tokens: 10000\n\n    - name: hallucination_detection\n      num_fewshot: 0\n      max_tokens: 100\n\n    - name: open_ended\n      num_fewshot: 5\n      max_tokens: 40000\n      judge_api_key: !ref judge_api_key\n      judge_base_url: !ref judge_base_url\n      judge_name: !ref judge_name\n\nwandb:\n  enabled: true\n  project: eve-evaluations\n  entity: LLM4EO\n  run_name: eve-model-v1\n\nmodels:\n  - name: eve-esa/eve_v0.1\n    base_url: https://api.runpod.ai/v2/endpoint-id/openai/v1/chat/completions\n    api_key: your-runpod-api-key\n    temperature: 0.1\n    num_concurrent: 10\n    timeout: 600\n    tasks: !ref tasks\n\noutput_dir: evals_outputs\n</code></pre>"},{"location":"evalkit/docs/examples/#example-2-using-lm-eval-harness-tasks","title":"Example 2: Using LM-Eval-Harness Tasks","text":"<p>Eve-evalkit supports all tasks from lm-evaluation-harness. Here's an example using MMLU-Pro:</p> <pre><code>constants:\n  tasks:\n    - name: mmlu_pro\n      num_fewshot: 5\n      max_tokens: 1000\n\n    - name: gsm8k\n      num_fewshot: 8\n      max_tokens: 512\n\n    - name: hellaswag\n      num_fewshot: 10\n      max_tokens: 100\n\nmodels:\n  - name: gpt-4\n    base_url: https://api.openai.com/v1/chat/completions\n    api_key: your-openai-api-key\n    temperature: 0.0\n    num_concurrent: 3\n    tasks: !ref tasks\n\noutput_dir: evals_outputs\n</code></pre>"},{"location":"evalkit/docs/examples/#example-3-mixed-eve-and-standard-tasks","title":"Example 3: Mixed EVE and Standard Tasks","text":"<p>Combine Earth Observation tasks with standard benchmarks:</p> <pre><code>constants:\n  judge_api_key: your-judge-api-key\n  judge_base_url: https://openrouter.ai/api/v1\n  judge_name: mistralai/mistral-large-2411\n\n  tasks:\n    # EVE Earth Observation Tasks\n    - name: hallucination_detection\n      num_fewshot: 0\n      max_tokens: 100\n\n    # Standard Benchmark Tasks\n    - name: mmlu_pro\n      num_fewshot: 5\n      max_tokens: 1000\n\n    - name: arc_challenge\n      num_fewshot: 25\n      max_tokens: 100\n\nwandb:\n  enabled: true\n  project: comprehensive-eval\n  entity: your-org\n\nmodels:\n  - name: your-model\n    base_url: https://api.provider.com/v1/chat/completions\n    api_key: your-api-key\n    temperature: 0.1\n    num_concurrent: 5\n    tasks: !ref tasks\n\noutput_dir: evals_outputs\n</code></pre>"},{"location":"evalkit/docs/examples/#example-4-multiple-models","title":"Example 4: Multiple Models","text":"<p>Evaluate multiple models on the same tasks:</p> <pre><code>constants:\n  tasks:\n    - name: hallucination_detection\n      num_fewshot: 0\n      max_tokens: 100\n\n    - name: mcqa_single_answer\n      num_fewshot: 2\n      max_tokens: 1000\n\nwandb:\n  enabled: true\n  project: model-comparison\n\nmodels:\n  - name: model-a\n    base_url: https://api.provider-a.com/v1/chat/completions\n    api_key: api-key-a\n    temperature: 0.1\n    num_concurrent: 5\n    tasks: !ref tasks\n\n  - name: model-b\n    base_url: https://api.provider-b.com/v1/chat/completions\n    api_key: api-key-b\n    temperature: 0.1\n    num_concurrent: 5\n    tasks: !ref tasks\n\noutput_dir: evals_outputs\n</code></pre>"},{"location":"evalkit/docs/examples/#example-5-using-environment-variables","title":"Example 5: Using Environment Variables","text":"<p>Instead of hardcoding API keys, use environment variables:</p> <pre><code>constants:\n  judge_api_key: ${JUDGE_API_KEY}\n\n  tasks:\n    - name: mcqa_single_answer\n      num_fewshot: 0\n      max_tokens: 20000\n      judge_api_key: !ref judge_api_key\n      judge_base_url: https://openrouter.ai/api/v1\n      judge_name: mistralai/mistral-large-2411\n\nmodels:\n  - name: my-model\n    base_url: https://api.provider.com/v1/chat/completions\n    api_key: ${MODEL_API_KEY}\n    tasks: !ref tasks\n\nwandb:\n  enabled: true\n  api_key: ${WANDB_API_KEY}\n  project: my-project\n\noutput_dir: evals_outputs\n</code></pre> <p>Set environment variables before running:</p> <pre><code>export JUDGE_API_KEY=your-judge-key\nexport MODEL_API_KEY=your-model-key\nexport WANDB_API_KEY=your-wandb-key\npython evaluate.py evals.yaml\n</code></pre>"},{"location":"evalkit/docs/examples/#example-6-testing-with-limited-samples","title":"Example 6: Testing with Limited Samples","text":"<p>Test your configuration on a small subset before running full evaluation:</p> <pre><code>constants:\n  tasks:\n    - name: hallucination_detection\n      num_fewshot: 0\n      max_tokens: 100\n      limit: 10  # Only evaluate first 10 samples\n\n    - name: mcqa_single_answer\n      num_fewshot: 2\n      max_tokens: 1000\n      limit: 5   # Only evaluate first 5 samples\n\nmodels:\n  - name: test-model\n    base_url: https://api.provider.com/v1/chat/completions\n    api_key: your-api-key\n    temperature: 0.1\n    num_concurrent: 2\n    tasks: !ref tasks\n\noutput_dir: test_outputs\n</code></pre>"},{"location":"evalkit/docs/examples/#example-7-using-seed-for-reproducibility","title":"Example 7: Using Seed for Reproducibility","text":"<p>Control randomness in evaluations by setting a seed value. This is especially useful for open-ended tasks with multiple judges where answer order randomization occurs:</p> <pre><code>constants:\n  judge_api_key: ${JUDGE_API_KEY}\n  judge_base_url: https://openrouter.ai/api/v1/\n  concurrent_requests: 20\n\n  open_ended_judges:\n    - name: qwen3-235b\n      model: qwen/qwen3-235b-a22b-2507\n      api_key: ${JUDGE_API_KEY}\n      base_url: https://openrouter.ai/api/v1/\n      prompt_path: metrics/prompts/llm_judge_qa.yaml\n    - name: mistral-large\n      model: mistral-large-2512\n      api_key: ${MISTRAL_API_KEY}\n      base_url: https://api.mistral.ai/v1\n      prompt_path: metrics/prompts/llm_judge_qa.yaml\n\n  tasks:\n    # Same task with different seeds for variance analysis\n    - name: open_ended_0_shot_seed_1234\n      task_name: open_ended\n      model_type: local-chat-completions\n      num_fewshot: 0\n      max_tokens: 10000\n      judges: !ref open_ended_judges\n      batch_size: !ref concurrent_requests\n      seed: 1234  # Fixed seed for reproducibility\n\n    - name: open_ended_0_shot_seed_5678\n      task_name: open_ended\n      model_type: local-chat-completions\n      num_fewshot: 0\n      max_tokens: 10000\n      judges: !ref open_ended_judges\n      batch_size: !ref concurrent_requests\n      seed: 5678  # Different seed\n\n    # Other tasks with seeds\n    - name: mcqa_single_answer_0_shot_seed_1234\n      task_name: mcqa_single_answer\n      model_type: local-chat-completions\n      num_fewshot: 0\n      max_tokens: 15\n      seed: 1234\n\nwandb:\n  enabled: true\n  project: seed-reproducibility-test\n  entity: your-org\n\nmodels:\n  - name: your-model\n    base_url: http://localhost:8010/v1/\n    api_key: EMPTY\n    temperature: 0.0\n    num_concurrent: !ref concurrent_requests\n    tasks: !ref tasks\n    timeout: 180\n\noutput_dir: evals_outputs\n</code></pre> <p>Why use seeds? - Reproduce exact same results across runs - Compare model performance with controlled randomness - Debug evaluation issues with consistent behavior - Analyze variance by running same task with different seeds</p>"},{"location":"evalkit/docs/examples/#example-8-using-eve-api-for-rag-enhanced-evaluation","title":"Example 8: Using EVE API for RAG-Enhanced Evaluation","text":"<p>Evaluate using the EVE API which provides RAG (Retrieval-Augmented Generation) enhanced responses with Earth Observation context:</p> <pre><code>constants:\n  # EVE API credentials\n  eve_email: your-email@example.com\n  eve_password: your-eve-password\n  eve_base_url: http://0.0.0.0:8000/\n  eve_public_collections: ['qwen-512-filtered', 'Wikipedia EO', 'Wiley AI Gateway']\n  eve_k: 10  # Number of documents to retrieve\n  eve_threshold: 0.5  # Similarity threshold\n\n  concurrent_requests: 7\n\n  # Judge configuration for open-ended tasks\n  open_ended_judges:\n    - name: qwen3-235b\n      model: qwen/qwen3-235b-a22b-2507\n      api_key: ${OPENROUTER_API_KEY}\n      base_url: https://openrouter.ai/api/v1/\n      prompt_path: metrics/prompts/llm_judge_qa.yaml\n    - name: mistral-large\n      model: mistral-large-2512\n      api_key: ${MISTRAL_API_KEY}\n      base_url: https://api.mistral.ai/v1\n      prompt_path: metrics/prompts/llm_judge_qa.yaml\n\n  tasks:\n    - name: open_ended_0_shot\n      task_name: open_ended\n      num_fewshot: 0\n      max_tokens: 10000\n      judges: !ref open_ended_judges\n      model_type: eve-api  # Use EVE API model type\n\n    - name: mcqa_multiple_answer_0_shot\n      task_name: mcqa_multiple_answer\n      num_fewshot: 0\n      max_tokens: 10000\n      model_type: eve-api\n\n    - name: mcqa_single_answer_0_shot\n      task_name: mcqa_single_answer\n      model_type: eve-api\n      num_fewshot: 0\n      max_tokens: 1000\n\n    - name: hallucination_detection_0_shot\n      task_name: hallucination_detection\n      num_fewshot: 0\n      max_tokens: 10000\n      model_type: eve-api\n\nwandb:\n  enabled: true\n  project: eve-api-evaluation\n  entity: LLM4EO\n\nmodels:\n  - name: eve-api\n    # EVE API configuration\n    email: !ref eve_email\n    password: !ref eve_password\n    base_url: !ref eve_base_url\n    public_collections: !ref eve_public_collections\n    k: !ref eve_k\n    threshold: !ref eve_threshold\n\n    # General settings\n    temperature: 0.0\n    num_concurrent: !ref concurrent_requests\n    tasks: !ref tasks\n    timeout: 180\n\noutput_dir: evals_outputs_eve_api\n</code></pre> <p>EVE API Configuration Parameters: - <code>email</code>: Email for EVE API authentication - <code>password</code>: Password for EVE API authentication - <code>base_url</code>: Base URL for the EVE API endpoint - <code>public_collections</code>: List of document collections to search for RAG - <code>k</code>: Number of documents to retrieve (default: 5) - <code>threshold</code>: Similarity threshold for document retrieval (default: 0.5) - <code>model_type: eve-api</code>: Must be specified in task configuration</p> <p>Important Notes: - The EVE API automatically retrieves relevant context for each query - Retrieved documents are used to enhance the model's responses - Especially useful for Earth Observation domain-specific questions - Requires a running EVE API server</p>"},{"location":"evalkit/docs/examples/#running-examples","title":"Running Examples","text":"<p>To run any of these examples:</p> <ol> <li>Save the configuration to a file (e.g., <code>evals.yaml</code>)</li> <li>Replace placeholder values (API keys, URLs, etc.) with your actual values</li> <li>Run the evaluation:</li> </ol> <pre><code>python evaluate.py evals.yaml\n</code></pre>"},{"location":"evalkit/docs/examples/#next-steps","title":"Next Steps","text":"<ul> <li>Learn more about EO Tasks</li> <li>Review the Getting Started guide for detailed configuration options</li> <li>Check the Code Reference for API documentation</li> </ul>"},{"location":"evalkit/docs/getting_started/","title":"Getting Started with Eve-evalkit","text":"<p>Eve-evalkit is built on top of the EleutherAI Language Model Evaluation Harness, which means it supports all tasks available in the lm-evaluation-harness in addition to the custom Earth Observation tasks.</p>"},{"location":"evalkit/docs/getting_started/#quick-start","title":"Quick Start","text":""},{"location":"evalkit/docs/getting_started/#1-installation","title":"1. Installation","text":"<p>Follow the installation instructions in the README:</p> <pre><code># Clone the repository\ngit clone https://github.com/eve-esa/evalkit.git\ncd eve-evaluation\n\n# Install uv\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Create virtual environment and install dependencies\nuv venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\nuv sync\n</code></pre>"},{"location":"evalkit/docs/getting_started/#2-running-evaluations","title":"2. Running Evaluations","text":"<p>The recommended way to run evaluations is using the YAML configuration file. Create an <code>evals.yaml</code> file:</p> <pre><code>constants:\n  judge_api_key: your-judge-api-key\n  judge_base_url: https://openrouter.ai/api/v1\n  judge_name: mistralai/mistral-large-2411\n  tasks:\n    - name: hallucination_detection\n      num_fewshot: 0\n      max_tokens: 100\n    - name: mcqa_single_answer\n      num_fewshot: 2\n      max_tokens: 1000\n\nwandb:\n  enabled: true\n  project: eve-evaluations\n  entity: your-wandb-entity\n  run_name: my-evaluation\n  api_key: your-wandb-api-key\n\nmodels:\n  - name: your-model-name\n    base_url: https://api.provider.com/v1/chat/completions\n    api_key: your-api-key\n    temperature: 0.1\n    num_concurrent: 5\n    timeout: 180\n    tasks: !ref tasks\n\noutput_dir: evals_outputs\n</code></pre> <p>Run the evaluation:</p> <pre><code>python evaluate.py evals.yaml\n</code></pre>"},{"location":"evalkit/docs/getting_started/#configuration-file-structure","title":"Configuration File Structure","text":""},{"location":"evalkit/docs/getting_started/#constants-section","title":"Constants Section","text":"<p>Define reusable values that can be referenced throughout the config using <code>!ref</code>:</p> <pre><code>constants:\n  judge_api_key: your-judge-api-key\n  judge_base_url: https://openrouter.ai/api/v1\n  judge_name: mistralai/mistral-large-2411\n  hf_token: your-huggingface-token  # Optional: for private datasets\n\n  tasks:\n    - name: task_name\n      num_fewshot: 0\n      max_tokens: 1000\n      judge_api_key: !ref judge_api_key  # Reference to constant\n      judge_base_url: !ref judge_base_url\n      judge_name: !ref judge_name\n</code></pre>"},{"location":"evalkit/docs/getting_started/#tasks-configuration","title":"Tasks Configuration","text":"<p>Each task can have the following parameters:</p> <pre><code>tasks:\n  - name: task_name              # Required: Task identifier\n    task_name: base_task         # Optional: Base task name (for custom naming)\n    num_fewshot: 0               # Number of few-shot examples (default: 0)\n    max_tokens: 1000             # Maximum tokens for generation (default: 512)\n    temperature: 0.0             # Sampling temperature (default: 0.0)\n    limit: 100                   # Optional: Limit number of samples to evaluate\n    seed: 1234                   # Optional: Random seed for reproducibility\n    model_type: local-chat-completions  # Optional: Model type (local-chat-completions, eve-api)\n    judge_api_key: api-key       # Required for single-judge LLM-as-judge tasks\n    judge_base_url: base-url     # Required for single-judge LLM-as-judge tasks\n    judge_name: model-name       # Required for single-judge LLM-as-judge tasks\n    judges: []                   # Optional: List of multiple judges for multi-judge evaluation\n</code></pre>"},{"location":"evalkit/docs/getting_started/#random-seed-for-reproducibility","title":"Random Seed for Reproducibility","text":"<p>The <code>seed</code> parameter allows you to control randomness in evaluations for reproducible results:</p> <pre><code>tasks:\n  - name: open_ended_0_shot_seed_1234\n    task_name: open_ended\n    num_fewshot: 0\n    max_tokens: 10000\n    seed: 1234  # Fixed seed ensures same results across runs\n</code></pre> <p>Benefits of using seeds: - Reproduce exact same results across multiple runs - Debug evaluation issues with consistent behavior - Compare model performance with controlled randomness - Run variance analysis by using multiple different seeds</p> <p>Example: Running same task with different seeds</p> <pre><code>tasks:\n  - name: open_ended_seed_1234\n    task_name: open_ended\n    seed: 1234\n\n  - name: open_ended_seed_5678\n    task_name: open_ended\n    seed: 5678\n\n  - name: open_ended_seed_9012\n    task_name: open_ended\n    seed: 9012\n</code></pre> <p>This configuration runs the same task three times with different random seeds to analyze variance in results.</p>"},{"location":"evalkit/docs/getting_started/#multi-judge-evaluation","title":"Multi-Judge Evaluation","text":"<p>For more robust evaluation, you can use multiple judges to evaluate each answer. This is particularly useful for open-ended tasks where a single judge might introduce bias.</p> <pre><code>constants:\n  judges:\n    - name: qwen3\n      model: qwen/qwen3-235b-a22b-2507\n      api_key: your_openrouter_api_key\n      base_url: https://openrouter.ai/api/v1/\n    - name: mistral_large\n      model: mistralai/mistral-large-2411\n      api_key: your_openrouter_api_key\n      base_url: https://openrouter.ai/api/v1/\n    - name: claude_sonnet\n      model: anthropic/claude-3.5-sonnet\n      api_key: your_openrouter_api_key\n      base_url: https://openrouter.ai/api/v1/\n\n  tasks:\n    - name: open_ended_multi_judge\n      task_name: open_ended\n      num_fewshot: 0\n      max_tokens: 10000\n      judges: !ref judges  # Use multiple judges\n      batch_size: 15\n</code></pre> <p>Multi-Judge Metrics: - <code>llm_as_judge_{judge_name}</code>: Individual score from each judge - <code>judge_voting</code>: Majority vote result (recommended primary metric) - <code>llm_as_judge_avg</code>: Average score across all judges - <code>judge_agreement</code>: Percentage of samples where all judges agree</p> <p>Recommendations: - Use 3 judges for a good balance between cost and reliability - Use 5 judges for high-stakes evaluations - Avoid 2 judges (risk of ties) - Mix different model architectures and providers for diversity</p>"},{"location":"evalkit/docs/getting_started/#models-configuration","title":"Models Configuration","text":"<p>Configure one or more models to evaluate:</p> <pre><code>models:\n  - name: model-identifier\n    base_url: https://api.provider.com/v1/chat/completions\n    api_key: your-api-key\n    temperature: 0.1\n    num_concurrent: 5      # Concurrent API requests (default: 3)\n    timeout: 180          # Request timeout in seconds (default: 300)\n    tasks: !ref tasks     # Reference to tasks list\n</code></pre>"},{"location":"evalkit/docs/getting_started/#eve-api-model-configuration","title":"EVE API Model Configuration","text":"<p>The EVE API provides RAG-enhanced (Retrieval-Augmented Generation) responses with Earth Observation context. To use the EVE API:</p> <pre><code>constants:\n  # EVE API credentials\n  eve_email: your-email@example.com\n  eve_password: your-password\n  eve_base_url: http://0.0.0.0:8000/\n  eve_public_collections: ['qwen-512-filtered', 'Wikipedia EO', 'Wiley AI Gateway']\n  eve_k: 10\n  eve_threshold: 0.5\n\n  tasks:\n    - name: open_ended_0_shot\n      task_name: open_ended\n      num_fewshot: 0\n      max_tokens: 10000\n      model_type: eve-api  # Specify eve-api model type\n\nmodels:\n  - name: eve-api\n    # EVE API configuration\n    email: !ref eve_email\n    password: !ref eve_password\n    base_url: !ref eve_base_url\n    public_collections: !ref eve_public_collections\n    k: !ref eve_k\n    threshold: !ref eve_threshold\n\n    # General settings\n    temperature: 0.0\n    num_concurrent: 7\n    tasks: !ref tasks\n    timeout: 180\n</code></pre> <p>EVE API Parameters: - <code>email</code>: Email for EVE API authentication - <code>password</code>: Password for EVE API authentication - <code>base_url</code>: Base URL for the EVE API endpoint - <code>public_collections</code>: List of document collections to search for RAG context - <code>k</code>: Number of documents to retrieve (default: 5) - <code>threshold</code>: Similarity threshold for document retrieval (default: 0.5)</p> <p>Important Notes: - Tasks using EVE API must specify <code>model_type: eve-api</code> in the task configuration - The EVE API automatically retrieves relevant context documents for each query - Retrieved documents are used to enhance the model's responses - Especially useful for Earth Observation domain-specific questions - Requires a running EVE API server at the specified <code>base_url</code></p> <p>Example Use Case:</p> <p>The EVE API is particularly valuable for: - Evaluating models on Earth Observation tasks with factual grounding - Comparing RAG-enhanced responses vs. non-RAG responses - Testing model performance with domain-specific context retrieval - Hallucination detection where factual context is critical</p>"},{"location":"evalkit/docs/getting_started/#weights-biases-wandb-logging","title":"Weights &amp; Biases (WandB) Logging","text":"<p>Enable experiment tracking with WandB:</p> <pre><code>wandb:\n  enabled: true                    # Enable/disable WandB logging\n  project: project-name            # WandB project name\n  entity: organization-name        # WandB entity/organization\n  run_name: custom-run-name       # Optional: Custom run name prefix\n  api_key: your-wandb-api-key     # WandB API key\n</code></pre> <p>When enabled, the evaluation will log: - Evaluation metrics (accuracy, F1, IoU, etc.) - Individual sample predictions - Task configurations - Model metadata - Evaluation duration and timestamps</p>"},{"location":"evalkit/docs/getting_started/#output-directory","title":"Output Directory","text":"<p>Specify where evaluation results should be saved:</p> <pre><code>output_dir: evals_outputs  # Default: eval_results\n</code></pre>"},{"location":"evalkit/docs/getting_started/#example-configurations","title":"Example Configurations","text":"<p>For comprehensive configuration examples including: - EVE Earth Observation tasks - LM-Eval-Harness standard benchmarks - Mixed evaluations - Multiple model comparisons - Environment variables usage - Testing with limited samples</p> <p>Please see the dedicated Examples page.</p>"},{"location":"evalkit/docs/getting_started/#output-structure","title":"Output Structure","text":"<p>After running evaluations, results are saved organized by task, then by model:</p> <pre><code>{output_dir}/\n\u251c\u2500\u2500 {task_name_1}/\n\u2502   \u251c\u2500\u2500 {model_name_sanitized}/\n\u2502   \u2502   \u251c\u2500\u2500 results_{timestamp}.json\n\u2502   \u2502   \u2514\u2500\u2500 samples_{task_name}_{timestamp}.jsonl\n\u2502   \u251c\u2500\u2500 {another_model_name_sanitized}/\n\u2502   \u2502   \u251c\u2500\u2500 results_{timestamp}.json\n\u2502   \u2502   \u2514\u2500\u2500 samples_{task_name}_{timestamp}.jsonl\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 {task_name_2}/\n\u2502   \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"evalkit/docs/getting_started/#example-structure","title":"Example Structure:","text":"<pre><code>evals_outputs/\n\u251c\u2500\u2500 hallucination_detection/\n\u2502   \u251c\u2500\u2500 eve-esa__eve_v0.1/\n\u2502   \u2502   \u251c\u2500\u2500 results_2025-12-01T10-17-45.479920.json\n\u2502   \u2502   \u2514\u2500\u2500 samples_hallucination_detection_2025-12-01T10-17-45.479920.jsonl\n\u2502   \u2514\u2500\u2500 gpt-4/\n\u2502       \u251c\u2500\u2500 results_2025-12-01T10-20-15.123456.json\n\u2502       \u2514\u2500\u2500 samples_hallucination_detection_2025-12-01T10-20-15.123456.jsonl\n\u251c\u2500\u2500 mcqa_single_answer/\n\u2502   \u251c\u2500\u2500 eve-esa__eve_v0.1/\n\u2502   \u2502   \u251c\u2500\u2500 results_2025-12-01T11-23-12.123456.json\n\u2502   \u2502   \u2514\u2500\u2500 samples_mcqa_single_answer_2025-12-01T11-23-12.123456.jsonl\n\u2502   \u2514\u2500\u2500 gpt-4/\n\u2502       \u251c\u2500\u2500 results_2025-12-01T11-25-30.789012.json\n\u2502       \u2514\u2500\u2500 samples_mcqa_single_answer_2025-12-01T11-25-30.789012.jsonl\n\u2514\u2500\u2500 open_ended/\n    \u251c\u2500\u2500 eve-esa__eve_v0.1/\n    \u2502   \u251c\u2500\u2500 results_2025-12-01T12-34-56.789012.json\n    \u2502   \u2514\u2500\u2500 samples_open_ended_2025-12-01T12-34-56.789012.jsonl\n    \u2514\u2500\u2500 gpt-4/\n        \u251c\u2500\u2500 results_2025-12-01T12-40-10.456789.json\n        \u2514\u2500\u2500 samples_open_ended_2025-12-01T12-40-10.456789.jsonl\n</code></pre> <p>This structure makes it easy to compare multiple models on the same task.</p>"},{"location":"evalkit/docs/getting_started/#results-file-format","title":"Results File Format","text":"<p>The <code>results_{timestamp}.json</code> file contains:</p> <pre><code>{\n  \"results\": {\n    \"task_name\": {\n      \"alias\": \"task_name\",\n      \"metric_1,none\": 0.85,\n      \"metric_1_stderr,none\": 0.02,\n      \"metric_2,none\": 0.78,\n      \"metric_2_stderr,none\": 0.03\n    }\n  },\n  \"group_subtasks\": {},\n  \"configs\": {\n    \"task_name\": {\n      \"task\": \"task_name\",\n      \"dataset_path\": \"dataset-path\",\n      \"num_fewshot\": 0,\n      \"metadata\": {}\n    }\n  },\n  \"versions\": {},\n  \"n-shot\": {},\n  \"n-samples\": {},\n  \"config\": {},\n  \"git_hash\": \"abc123\",\n  \"date\": 1701234567.89,\n  \"total_evaluation_time_seconds\": \"123.45\"\n}\n</code></pre>"},{"location":"evalkit/docs/getting_started/#samples-file-format","title":"Samples File Format","text":"<p>The <code>samples_{task_name}_{timestamp}.jsonl</code> file contains individual predictions:</p> <pre><code>{\"doc_id\": 0, \"doc\": {...}, \"target\": \"expected\", \"arguments\": [...], \"resps\": [[\"predicted\"]], \"filtered_resps\": [\"predicted\"], \"doc_hash\": \"abc123\", \"prompt_hash\": \"def456\", \"task_name\": \"task_name\"}\n{\"doc_id\": 1, \"doc\": {...}, \"target\": \"expected\", \"arguments\": [...], \"resps\": [[\"predicted\"]], \"filtered_resps\": [\"predicted\"], \"doc_hash\": \"ghi789\", \"prompt_hash\": \"jkl012\", \"task_name\": \"task_name\"}\n...\n</code></pre> <p>Each line contains: - <code>doc</code>: The input document/question - <code>target</code>: Expected answer - <code>resps</code>: Raw model response - <code>filtered_resps</code>: Processed model response - Metadata for reproducibility</p>"},{"location":"evalkit/docs/getting_started/#wandb-integration","title":"WandB Integration","text":"<p>When WandB logging is enabled, the following information is automatically logged:</p>"},{"location":"evalkit/docs/getting_started/#metrics-logged","title":"Metrics Logged","text":"<ul> <li>Aggregate Metrics: Final scores for each metric (accuracy, F1, IoU, etc.)</li> <li>Per-Sample Metrics: Individual predictions and correctness</li> <li>Task Metadata: Dataset paths, splits, versions</li> <li>Model Configuration: API endpoints, temperatures, timeouts</li> <li>Evaluation Metadata: Git hash, timestamps, duration</li> </ul>"},{"location":"evalkit/docs/getting_started/#viewing-results","title":"Viewing Results","text":"<p>After evaluation completes, visit your WandB project to:</p> <ol> <li>Compare Models: View metrics across different models side-by-side</li> <li>Analyze Samples: Inspect individual predictions and failures</li> <li>Track Progress: Monitor evaluation progress in real-time</li> <li>Visualize Trends: Plot metric distributions and comparisons</li> </ol>"},{"location":"evalkit/docs/getting_started/#example-wandb-output","title":"Example WandB Output","text":"<pre><code>Run: eve-model-v1-20251201\n\u251c\u2500\u2500 Summary Metrics\n\u2502   \u251c\u2500\u2500 hallucination_detection/acc: 0.822\n\u2502   \u251c\u2500\u2500 hallucination_detection/f1: 0.841\n\u2502   \u251c\u2500\u2500 hallucination_detection/precision: 0.869\n\u2502   \u251c\u2500\u2500 mcqa_single_answer/acc: 0.756\n\u2502   \u2514\u2500\u2500 open_ended/llm_judge: 0.834\n\u251c\u2500\u2500 Config\n\u2502   \u251c\u2500\u2500 model: eve-esa/eve_v0.1\n\u2502   \u251c\u2500\u2500 temperature: 0.1\n\u2502   \u2514\u2500\u2500 num_concurrent: 10\n\u2514\u2500\u2500 Samples\n    \u251c\u2500\u2500 hallucination_detection_samples.csv\n    \u251c\u2500\u2500 mcqa_single_answer_samples.csv\n    \u2514\u2500\u2500 open_ended.csv\n</code></pre>"},{"location":"evalkit/docs/getting_started/#advanced-usage","title":"Advanced Usage","text":""},{"location":"evalkit/docs/getting_started/#using-environment-variables","title":"Using Environment Variables","text":"<p>Instead of hardcoding API keys, use environment variables. See Examples for detailed configuration.</p>"},{"location":"evalkit/docs/getting_started/#limiting-samples-for-testing","title":"Limiting Samples for Testing","text":"<p>Test your configuration on a small subset. See Examples for detailed configuration.</p>"},{"location":"evalkit/docs/getting_started/#direct-command-line","title":"Direct Command Line","text":"<p>For quick tests, you can use the lm_eval command directly:</p> <pre><code>lm_eval --model openai-chat-completions \\\n        --model_args base_url=https://api.provider.com,model=model-name,num_concurrent=5 \\\n        --tasks hallucination_detection,mcqa_single_answer \\\n        --include tasks \\\n        --num_fewshot 0 \\\n        --output_path ./outputs \\\n        --log_samples \\\n        --apply_chat_template\n</code></pre>"},{"location":"evalkit/docs/getting_started/#available-tasks","title":"Available Tasks","text":""},{"location":"evalkit/docs/getting_started/#eve-earth-observation-tasks","title":"EVE Earth Observation Tasks","text":"<p>See the EO Tasks page for detailed information about: - <code>mcqa_multiple_answer</code> - <code>mcqa_single_answer</code> - <code>open_ended</code> - <code>open_ended_w_context</code> - <code>refusal</code> - <code>hallucination_detection</code></p>"},{"location":"evalkit/docs/getting_started/#lm-evaluation-harness-tasks","title":"LM-Evaluation-Harness Tasks","text":"<p>All tasks from the lm-evaluation-harness are supported, including:</p> <p>Popular Benchmarks: - <code>mmlu_pro</code> - MMLU-Pro (challenging multiple-choice) - <code>gsm8k</code> - Grade School Math - <code>hellaswag</code> - Commonsense reasoning - <code>arc_challenge</code> - AI2 Reasoning Challenge - <code>truthfulqa</code> - Truthfulness evaluation - <code>winogrande</code> - Commonsense reasoning - <code>piqa</code> - Physical commonsense - And more...</p> <p>To list all available tasks:</p> <pre><code>lm_eval --tasks list\n</code></pre>"},{"location":"evalkit/docs/getting_started/#troubleshooting","title":"Troubleshooting","text":""},{"location":"evalkit/docs/getting_started/#common-issues","title":"Common Issues","text":"<p>1. API Timeout Errors</p> <p>Increase the timeout value:</p> <pre><code>models:\n  - name: your-model\n    timeout: 600  # Increase to 10 minutes\n</code></pre> <p>2. Rate Limiting</p> <p>Reduce concurrent requests:</p> <pre><code>models:\n  - name: your-model\n    num_concurrent: 1  # Reduce concurrency\n</code></pre> <p>3. Judge Model Errors</p> <p>Ensure judge credentials are set for tasks that require them:</p> <pre><code>tasks:\n  - name: open_ended\n    judge_api_key: !ref judge_api_key  # Required!\n    judge_base_url: !ref judge_base_url\n    judge_name: !ref judge_name\n</code></pre> <p>4. WandB Login Issues</p> <p>Login before running:</p> <pre><code>wandb login your-api-key\n</code></pre>"},{"location":"evalkit/docs/getting_started/#next-steps","title":"Next Steps","text":"<ul> <li>Explore Tasks: Check out the EO Tasks page for details on Earth Observation evaluation tasks</li> </ul>"},{"location":"evalkit/docs/getting_started/#support","title":"Support","text":"<p>For issues or questions: - GitHub Issues: eve-esa/eve-evaluation - Documentation: https://docs.eve-evaluation.org - LM-Eval-Harness: EleutherAI Documentation</p>"},{"location":"data-scraping/docs/","title":"Data Scraping Pipeline","text":"<p>Welcome to the Data Scraping pipeline documentation. This pipeline is designed to collect and scrape Earth Observation and Earth Science data from various academic publishers, journals, and data sources.</p>"},{"location":"data-scraping/docs/#features","title":"Features","text":"<ul> <li>32+ Specialized Scrapers: Pre-configured scrapers for major publishers and data sources including IEEE, Springer, Elsevier, NASA, ESA, and more</li> <li>Flexible Architecture: Extensible base classes for creating new scrapers</li> <li>Cloud Storage Integration: S3-compatible storage (AWS S3, MinIO)</li> <li>Database Tracking: MySQL database for tracking scraping progress and analytics</li> <li>Docker Support: Containerized deployment for easy setup</li> <li>Proxy Support: Built-in proxy support for restricted content</li> <li>Resume Capability: Resume failed scraping operations</li> <li>Analytics: Comprehensive statistics on scraping operations</li> </ul>"},{"location":"data-scraping/docs/#quick-start","title":"Quick Start","text":"<pre><code># Clone the repository\ngit clone &lt;repository-url&gt;\ncd data-scraping\n\n# Start Docker containers\nmake up\n\n# Run the pipeline\nmake run\n</code></pre> <p>For detailed installation instructions, see the Getting Started page.</p>"},{"location":"data-scraping/docs/#documentation-structure","title":"Documentation Structure","text":"<ul> <li>Getting Started: Installation, prerequisites, and setup guide</li> <li>Scrapers: Complete documentation of all available scrapers</li> <li>Model: Data models and configuration schemas</li> <li>Examples: Usage examples and common workflows</li> </ul>"},{"location":"data-scraping/docs/#architecture-overview","title":"Architecture Overview","text":"<p>The scraping system is built on a hierarchical architecture:</p> <ol> <li>Base Scrapers: Abstract classes providing core functionality (Selenium, storage, database)</li> <li>Specialized Scrapers: Publisher-specific implementations</li> <li>Configuration: JSON-based configuration for each scraper</li> <li>Storage: S3-compatible storage for collected data</li> <li>Database: MySQL for tracking progress and analytics</li> </ol>"},{"location":"data-scraping/docs/#funding","title":"Funding","text":"<p>This project is supported by the European Space Agency (ESA) \u03a6-lab through the Large Language Model for Earth Observation and Earth Science project, as part of the Foresight Element within FutureEO Block 4 programme.</p>"},{"location":"data-scraping/docs/#license","title":"License","text":"<p>This project is released under the Apache 2.0 License.</p>"},{"location":"data-scraping/docs/#contributing","title":"Contributing","text":"<p>We welcome contributions! Please open an issue or submit a pull request on GitHub to help improve the pipeline.</p>"},{"location":"data-scraping/docs/examples/","title":"Examples","text":"<p>This page provides practical examples and common workflows for using the Data Scraping pipeline.</p>"},{"location":"data-scraping/docs/examples/#basic-usage-examples","title":"Basic Usage Examples","text":""},{"location":"data-scraping/docs/examples/#running-a-single-scraper","title":"Running a Single Scraper","text":"<p>Run a specific scraper to collect data from one source:</p> <pre><code>make run args=\"--scrapers IOPScraper\"\n</code></pre> <p>What happens:</p> <ol> <li>The pipeline loads configuration from <code>config/config.json</code></li> <li>IOPScraper is initialized with its configuration</li> <li>The scraper navigates to configured URLs</li> <li>PDF links are extracted and downloaded</li> <li>Files are uploaded to MinIO/S3</li> <li>Results are logged to the database</li> </ol>"},{"location":"data-scraping/docs/examples/#running-multiple-scrapers","title":"Running Multiple Scrapers","text":"<p>Execute several scrapers sequentially:</p> <pre><code>make run args=\"--scrapers IOPScraper SpringerScraper MDPIScraper\"\n</code></pre> <p>This will run each scraper one after another, useful for collecting data from multiple sources in one command.</p>"},{"location":"data-scraping/docs/examples/#running-all-scrapers","title":"Running All Scrapers","text":"<p>Execute all configured scrapers:</p> <pre><code>make run\n</code></pre> <p>This processes all scrapers defined in <code>config/config.json</code>.</p>"},{"location":"data-scraping/docs/examples/#advanced-usage","title":"Advanced Usage","text":""},{"location":"data-scraping/docs/examples/#force-re-execution","title":"Force Re-execution","text":"<p>Force a scraper to run again even if it completed successfully before:</p> <pre><code>make run args=\"--scrapers IOPScraper --force\"\n</code></pre> <p>Use cases:</p> <ul> <li>Testing after code changes</li> <li>Collecting new content from the same source</li> <li>Recovering from incomplete runs</li> </ul>"},{"location":"data-scraping/docs/examples/#resume-failed-urls","title":"Resume Failed URLs","text":"<p>Resume only failed URLs from the previous execution:</p> <pre><code>make run args=\"--scrapers IOPScraper --resume\"\n</code></pre> <p>When to use:</p> <ul> <li>Network errors occurred during scraping</li> <li>Some pages were temporarily unavailable</li> <li>Timeout errors on specific URLs</li> </ul> <p>What it does:</p> <ul> <li>Queries <code>scraper_failure</code> table for failed URLs</li> <li>Re-attempts only those URLs</li> <li>Updates success/failure status</li> </ul>"},{"location":"data-scraping/docs/examples/#resume-failed-uploads","title":"Resume Failed Uploads","text":"<p>Resume only failed file uploads:</p> <pre><code>make run args=\"--scrapers IOPScraper --resume-upload\"\n</code></pre> <p>When to use:</p> <ul> <li>S3/MinIO connection issues occurred</li> <li>Upload timeouts for large files</li> <li>Storage quota was exceeded</li> </ul> <p>Important: Cannot combine <code>--resume</code> and <code>--resume-upload</code> in one command.</p>"},{"location":"data-scraping/docs/examples/#analytics-examples","title":"Analytics Examples","text":""},{"location":"data-scraping/docs/examples/#view-all-scraper-statistics","title":"View All Scraper Statistics","text":"<p>Get analytics for all scrapers:</p> <pre><code>make run args=\"--analytics-only\"\n</code></pre> <p>Output includes:</p> <ul> <li>URLs scraped</li> <li>Content successfully retrieved</li> <li>Files uploaded to storage</li> <li>Failure counts</li> </ul>"},{"location":"data-scraping/docs/examples/#view-specific-scraper-statistics","title":"View Specific Scraper Statistics","text":"<p>Get analytics for specific scrapers:</p> <pre><code>make run args=\"--analytics-only --scrapers IOPScraper SpringerScraper\"\n</code></pre>"},{"location":"data-scraping/docs/examples/#understanding-analytics-output","title":"Understanding Analytics Output","text":"<p>The analytics JSON contains:</p> <pre><code>{\n  \"scraped\": {\n    \"total\": 100,\n    \"successful\": 95,\n    \"failed\": 5\n  },\n  \"content_retrieved\": {\n    \"retrieved\": 90,\n    \"not_retrieved\": 5\n  },\n  \"uploaded\": {\n    \"successful\": 88,\n    \"failed\": 2\n  }\n}\n</code></pre> <p>Metrics explained:</p> <ul> <li>scraped: URLs processed by the scraper</li> <li>content_retrieved: Resources whose content was successfully downloaded</li> <li>uploaded: Resources successfully uploaded to S3</li> </ul>"},{"location":"data-scraping/docs/examples/#configuration-examples","title":"Configuration Examples","text":""},{"location":"data-scraping/docs/examples/#simple-publisher-configuration","title":"Simple Publisher Configuration","text":"<p>For a straightforward journal website:</p> <pre><code>{\n  \"SimplePublisherScraper\": {\n    \"bucket_key\": \"{main_folder}/simple_publisher\",\n    \"base_url\": \"https://journal.example.com\",\n    \"cookie_selector\": \"button.accept-cookies\",\n    \"sources\": [\n      {\n        \"url\": \"https://journal.example.com/volume/1/issue/1\",\n        \"type\": \"issue_or_collection\"\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"data-scraping/docs/examples/#iterative-journal-configuration","title":"Iterative Journal Configuration","text":"<p>For journals with volume/issue structure:</p> <pre><code>{\n  \"JournalScraper\": {\n    \"bucket_key\": \"{main_folder}/journal\",\n    \"journals\": [\n      {\n        \"url\": \"https://journal.com\",\n        \"name\": \"Journal of Earth Observation\",\n        \"start_volume\": 1,\n        \"end_volume\": 20,\n        \"start_issue\": 1,\n        \"end_issue\": 12,\n        \"consecutive_missing_volumes_threshold\": 3,\n        \"consecutive_missing_issues_threshold\": 3\n      }\n    ]\n  }\n}\n</code></pre> <p>Parameters:</p> <ul> <li><code>start_volume/end_volume</code>: Volume range to scrape</li> <li><code>start_issue/end_issue</code>: Issue range per volume</li> <li><code>consecutive_missing_*_threshold</code>: Stop after N consecutive missing volumes/issues</li> </ul>"},{"location":"data-scraping/docs/examples/#pagination-configuration","title":"Pagination Configuration","text":"<p>For search results with pagination:</p> <pre><code>{\n  \"SearchScraper\": {\n    \"bucket_key\": \"{main_folder}/search\",\n    \"base_url\": \"https://publisher.com\",\n    \"sources\": [\n      {\n        \"landing_page_url\": \"https://publisher.com/search?q=remote+sensing&amp;page={page_number}\",\n        \"page_size\": 50,\n        \"max_allowed_papers\": 1000\n      }\n    ]\n  }\n}\n</code></pre> <p>Parameters:</p> <ul> <li><code>{page_number}</code>: Placeholder for page number (auto-incremented)</li> <li><code>page_size</code>: Results per page</li> <li><code>max_allowed_papers</code>: Maximum papers to collect</li> </ul>"},{"location":"data-scraping/docs/examples/#multi-source-configuration","title":"Multi-Source Configuration","text":"<p>For scrapers with multiple sub-sources:</p> <pre><code>{\n  \"MultiSourceScraper\": {\n    \"bucket_key\": \"{main_folder}/multi\",\n    \"sources\": [\n      {\n        \"name\": \"Source A\",\n        \"scraper\": \"SubScraperA\",\n        \"config\": {\n          \"base_url\": \"https://source-a.com\",\n          \"cookie_selector\": \"button.accept\"\n        }\n      },\n      {\n        \"name\": \"Source B\",\n        \"scraper\": \"SubScraperB\",\n        \"config\": {\n          \"base_url\": \"https://source-b.com\",\n          \"urls\": [\"https://source-b.com/papers\"]\n        }\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"data-scraping/docs/examples/#direct-download-configuration","title":"Direct Download Configuration","text":"<p>For known PDF URLs:</p> <pre><code>{\n  \"DirectLinksScraper\": {\n    \"bucket_key\": \"{main_folder}/direct\",\n    \"sources\": [\n      {\n        \"name\": \"Example PDFs\",\n        \"config\": {\n          \"bucket_key\": \"custom_folder\",\n          \"urls\": [\n            \"https://example.com/paper1.pdf\",\n            \"https://example.com/paper2.pdf\",\n            \"https://example.com/paper3.pdf\"\n          ]\n        }\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"data-scraping/docs/examples/#debugging-examples","title":"Debugging Examples","text":""},{"location":"data-scraping/docs/examples/#check-scraper-status","title":"Check Scraper Status","text":"<p>View database status for a scraper:</p> <pre><code>-- Connect to MySQL\ndocker exec -it &lt;mysql-container&gt; mysql -u root -p\n\n-- Check completed scrapers\nSELECT scraper, created_at FROM scraper_output;\n\n-- Check failed URLs\nSELECT scraper, url, error FROM scraper_failure ORDER BY created_at DESC LIMIT 10;\n\n-- Check uploaded resources\nSELECT COUNT(*) as total FROM uploaded_resource;\n</code></pre>"},{"location":"data-scraping/docs/examples/#inspect-minio-storage","title":"Inspect MinIO Storage","text":"<p>Access MinIO console:</p> <pre><code># Local: http://localhost:9100\n# Login: minio / minio1234\n</code></pre> <p>Navigate to your bucket and verify files were uploaded correctly.</p>"},{"location":"data-scraping/docs/examples/#view-scraper-logs","title":"View Scraper Logs","text":"<p>Check Docker logs for errors:</p> <pre><code># View recent logs\ndocker logs &lt;container-name&gt; --tail 100\n\n# Follow logs in real-time\ndocker logs -f &lt;container-name&gt;\n\n# Search for errors\ndocker logs &lt;container-name&gt; 2&gt;&amp;1 | grep ERROR\n</code></pre>"},{"location":"data-scraping/docs/examples/#test-configuration","title":"Test Configuration","text":"<p>Validate configuration before running:</p> <pre><code># In Python console\nfrom model.iop_models import IOPConfig\nimport json\n\nwith open('config/config.json') as f:\n    config = json.load(f)\n\n# Validate configuration\niop_config = IOPConfig(**config['IOPScraper'])\nprint(iop_config)\n</code></pre>"},{"location":"data-scraping/docs/examples/#troubleshooting-examples","title":"Troubleshooting Examples","text":""},{"location":"data-scraping/docs/examples/#handle-cookie-banners","title":"Handle Cookie Banners","text":"<p>If scraper fails due to cookie banners:</p> <ol> <li>Inspect the page in browser</li> <li>Find the \"Accept\" button CSS selector</li> <li>Add to configuration:</li> </ol> <pre><code>{\n  \"ScraperName\": {\n    \"cookie_selector\": \"button#onetrust-accept-btn-handler\",\n    ...\n  }\n}\n</code></pre>"},{"location":"data-scraping/docs/examples/#handle-dynamic-loading","title":"Handle Dynamic Loading","text":"<p>For JavaScript-loaded content:</p> <pre><code># In your scraper's scrape() method\n\n# Wait for element to load\nself._driver.wait_for_element(\"div.article-list\", timeout=10)\n\n# Or use loading tag\n# Configure in config.json:\n{\n  \"loading_tag\": \"div.loading-spinner\"\n}\n</code></pre>"},{"location":"data-scraping/docs/examples/#handle-pagination-edge-cases","title":"Handle Pagination Edge Cases","text":"<p>For pagination that doesn't follow standard patterns:</p> <pre><code># In your scraper\n\npage_number = 1\nwhile True:\n    url = f\"https://example.com/search?page={page_number}\"\n    self._driver.open(url)\n\n    # Check if results exist\n    results = self._driver.find_elements(\"div.result\")\n    if not results:\n        break  # No more pages\n\n    # Process results\n    # ...\n\n    page_number += 1\n</code></pre>"},{"location":"data-scraping/docs/examples/#handle-proxy-requirements","title":"Handle Proxy Requirements","text":"<p>For sites requiring proxy:</p> <pre><code>{\n  \"ScraperName\": {\n    \"request_with_proxy\": true,\n    ...\n  }\n}\n</code></pre> <p>Ensure proxy credentials are in <code>.env</code>:</p> <pre><code>INTERACTING_PROXY_HOST=proxy.example.com\nINTERACTING_PROXY_PORT=8080\nINTERACTING_PROXY_USER=username\nINTERACTING_PROXY_PASSWORD=password\n</code></pre>"},{"location":"data-scraping/docs/examples/#performance-optimization","title":"Performance Optimization","text":""},{"location":"data-scraping/docs/examples/#parallel-scraping","title":"Parallel Scraping","text":"<p>Run multiple scrapers in parallel using separate processes:</p> <pre><code># Terminal 1\nmake run args=\"--scrapers IOPScraper MDPIScraper\" &amp;\n\n# Terminal 2\nmake run args=\"--scrapers SpringerScraper ElsevierScraper\" &amp;\n</code></pre> <p>Note: Ensure each scraper targets different domains to avoid rate limiting.</p>"},{"location":"data-scraping/docs/examples/#optimize-page-size","title":"Optimize Page Size","text":"<p>For pagination scrapers, adjust page size:</p> <pre><code>{\n  \"sources\": [\n    {\n      \"landing_page_url\": \"...\",\n      \"page_size\": 100  // Larger pages = fewer requests\n    }\n  ]\n}\n</code></pre> <p>Balance: Larger pages are faster but may timeout.</p>"},{"location":"data-scraping/docs/examples/#limit-paper-count","title":"Limit Paper Count","text":"<p>To avoid excessive scraping:</p> <pre><code>{\n  \"sources\": [\n    {\n      \"landing_page_url\": \"...\",\n      \"max_allowed_papers\": 5000\n    }\n  ]\n}\n</code></pre>"},{"location":"data-scraping/docs/examples/#integration-examples","title":"Integration Examples","text":""},{"location":"data-scraping/docs/examples/#query-uploaded-resources","title":"Query Uploaded Resources","text":"<p>After scraping, query the database:</p> <pre><code>from repository.uploaded_resource_repository import UploadedResourceRepository\n\nrepo = UploadedResourceRepository()\n\n# Get all resources from a specific scraper\nresources = repo.get_all_by({\"source\": \"IOPScraper\"})\n\nfor resource in resources:\n    print(f\"URL: {resource.url}\")\n    print(f\"S3 Key: {resource.s3_key}\")\n    print(f\"SHA256: {resource.sha256}\")\n</code></pre>"},{"location":"data-scraping/docs/examples/#download-from-s3","title":"Download from S3","text":"<p>Retrieve scraped files from S3:</p> <pre><code>from service.storage import S3Storage\n\ns3 = S3Storage()\n\n# Download a file\ns3_key = \"raw_data/iopscience/paper.pdf\"\nlocal_path = \"/tmp/paper.pdf\"\n\ns3.download_file(s3_key, local_path)\n</code></pre>"},{"location":"data-scraping/docs/examples/#export-analytics","title":"Export Analytics","text":"<p>Export analytics to CSV:</p> <pre><code># Get analytics as JSON\nmake run args=\"--analytics-only --scrapers IOPScraper\" &gt; analytics.json\n\n# Process with Python\npython -c \"\nimport json\nimport csv\n\nwith open('analytics.json') as f:\n    data = json.load(f)\n\nwith open('analytics.csv', 'w') as f:\n    writer = csv.writer(f)\n    writer.writerow(['Metric', 'Value'])\n    for key, value in data.items():\n        writer.writerow([key, value])\n\"\n</code></pre>"},{"location":"data-scraping/docs/getting_started/","title":"Getting Started","text":"<p>This guide will help you set up and run the Data Scraping pipeline on your local machine or in production.</p>"},{"location":"data-scraping/docs/getting_started/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following installed:</p> <ul> <li>Python 3.8+</li> <li>Docker and Docker Compose</li> <li>Make</li> </ul>"},{"location":"data-scraping/docs/getting_started/#local-development-setup","title":"Local Development Setup","text":""},{"location":"data-scraping/docs/getting_started/#1-create-virtual-environment","title":"1. Create Virtual Environment","text":"<p>Create a virtual environment in the <code>venv</code> folder:</p> <pre><code>python3 -m venv venv\n</code></pre> <p>Activate the virtual environment:</p> <pre><code>source venv/bin/activate\n</code></pre>"},{"location":"data-scraping/docs/getting_started/#2-environment-configuration","title":"2. Environment Configuration","text":"<p>Create a <code>.env</code> file in the root of the project with the following content:</p> <pre><code># AWS/MinIO Configuration\nAWS_URL=http://minio:9100\nAWS_REGION=us-east-1\nAWS_ACCESS_KEY=minio\nAWS_SECRET_KEY=minio1234\nAWS_BUCKET_NAME=esa-eve\nAWS_MAIN_FOLDER=raw_data\n\nMINIO_URL=http://minio:9100\n\n# Browser Configuration\nHEADLESS_BROWSER=true\nXVFB_MODE=false\n\n# Database Configuration\nDB_HOST=mysql\nDB_PORT=3306\nDB_NAME=esa_eve\nDB_USER=root\nDB_PASSWORD=root\n\n# Proxy Configuration (optional)\nINTERACTING_PROXY_HOST=brd.superproxy.io\nINTERACTING_PROXY_PORT=33335\nINTERACTING_PROXY_USER=&lt;username&gt;\nINTERACTING_PROXY_PASSWORD=&lt;password&gt;\n</code></pre>"},{"location":"data-scraping/docs/getting_started/#configuration-notes","title":"Configuration Notes","text":"<p>MinIO for Local Development:</p> <ul> <li>The MinIO server emulates a remote S3 bucket for local testing</li> <li>The <code>AWS_URL</code> key must be set to the URL of the MinIO server</li> <li>For production, remove the <code>AWS_URL</code> from the configuration to use real AWS S3</li> </ul> <p>Browser Settings:</p> <ul> <li><code>HEADLESS_BROWSER=true</code>: Run browser without GUI (recommended for servers)</li> <li><code>XVFB_MODE=true</code>: Use virtual frame buffer (required for headless Linux servers)</li> </ul> <p>Proxy Settings (optional):</p> <ul> <li>Required only for accessing restricted content</li> <li>Contact the project maintainer for proxy credentials</li> </ul>"},{"location":"data-scraping/docs/getting_started/#3-installation","title":"3. Installation","text":"<p>Start the Docker containers:</p> <pre><code>make up\n</code></pre> <p>Install required Python packages:</p> <pre><code>make sync-requirements\n</code></pre>"},{"location":"data-scraping/docs/getting_started/#4-verify-installation","title":"4. Verify Installation","text":"<p>Check that all containers are running:</p> <pre><code>docker ps\n</code></pre> <p>You should see containers for:</p> <ul> <li>MinIO (S3-compatible storage)</li> <li>MySQL (database)</li> <li>The scraping application</li> </ul>"},{"location":"data-scraping/docs/getting_started/#running-the-pipeline","title":"Running the Pipeline","text":""},{"location":"data-scraping/docs/getting_started/#basic-usage","title":"Basic Usage","text":"<p>Run all configured scrapers:</p> <pre><code>make run\n</code></pre> <p>The pipeline will:</p> <ol> <li>Connect to MinIO and MySQL</li> <li>Execute all scrapers defined in <code>config/config.json</code></li> <li>Store scraped data in MinIO</li> <li>Track progress in MySQL database</li> </ol>"},{"location":"data-scraping/docs/getting_started/#running-specific-scrapers","title":"Running Specific Scrapers","text":"<p>Execute one or more specific scrapers:</p> <pre><code># Run a single scraper\nmake run args=\"--scrapers IOPScraper\"\n\n# Run multiple scrapers\nmake run args=\"--scrapers IOPScraper SpringerScraper\"\n</code></pre>"},{"location":"data-scraping/docs/getting_started/#force-re-execution","title":"Force Re-execution","text":"<p>Force complete re-execution of a scraper (even if already completed):</p> <pre><code>make run args=\"--scrapers IOPScraper --force\"\n</code></pre>"},{"location":"data-scraping/docs/getting_started/#resume-failed-operations","title":"Resume Failed Operations","text":"<p>Resume only the failed URLs from the last execution:</p> <pre><code>make run args=\"--scrapers IOPScraper --resume\"\n</code></pre> <p>Resume only the failed uploads from the last execution:</p> <pre><code>make run args=\"--scrapers IOPScraper --resume-upload\"\n</code></pre> <p>Note: The <code>--resume</code> and <code>--resume-upload</code> parameters cannot be used together.</p>"},{"location":"data-scraping/docs/getting_started/#view-analytics","title":"View Analytics","text":"<p>Retrieve statistics from the last execution:</p> <pre><code>make run args=\"--analytics-only\"\n\n# For specific scrapers\nmake run args=\"--analytics-only --scrapers IOPScraper\"\n</code></pre>"},{"location":"data-scraping/docs/getting_started/#production-deployment","title":"Production Deployment","text":""},{"location":"data-scraping/docs/getting_started/#1-production-environment-setup","title":"1. Production Environment Setup","text":"<p>Create a production <code>.env</code> file with real credentials:</p> <pre><code># AWS Configuration (remove AWS_URL for production S3)\nAWS_REGION=&lt;region&gt;\nAWS_ACCESS_KEY=&lt;access-key&gt;\nAWS_SECRET_KEY=&lt;secret-key&gt;\nAWS_BUCKET_NAME=&lt;bucket-name&gt;\nAWS_MAIN_FOLDER=&lt;folder-path&gt;\n\n# Database Configuration\nDB_HOST=&lt;database-host&gt;\nDB_PORT=3306\nDB_NAME=&lt;database-name&gt;\nDB_USER=&lt;database-user&gt;\nDB_PASSWORD=&lt;database-password&gt;\n\n# Browser Configuration\nHEADLESS_BROWSER=true\nXVFB_MODE=true\n\n# Proxy Configuration (if needed)\nINTERACTING_PROXY_HOST=&lt;proxy-host&gt;\nINTERACTING_PROXY_PORT=&lt;proxy-port&gt;\nINTERACTING_PROXY_USER=&lt;proxy-username&gt;\nINTERACTING_PROXY_PASSWORD=&lt;proxy-password&gt;\n</code></pre> <p>Contact the project maintainer to obtain production credentials.</p>"},{"location":"data-scraping/docs/getting_started/#2-deploy-to-production-pod","title":"2. Deploy to Production POD","text":"<p>Run the deployment script on your Linux-based POD:</p> <pre><code>sh pod.sh\n</code></pre> <p>This script will:</p> <ul> <li>Install required packages</li> <li>Set up the environment</li> <li>Configure the infrastructure</li> </ul>"},{"location":"data-scraping/docs/getting_started/#3-run-in-production","title":"3. Run in Production","text":"<p>Execute the pipeline in production:</p> <pre><code>make runpod\n\n# With arguments\nmake runpod args=\"--scrapers IOPScraper\"\n</code></pre> <p>The <code>make runpod</code> command supports all the same parameters as <code>make run</code>.</p>"},{"location":"data-scraping/docs/getting_started/#configuration","title":"Configuration","text":""},{"location":"data-scraping/docs/getting_started/#scraper-configuration","title":"Scraper Configuration","text":"<p>Scrapers are configured in <code>config/config.json</code>. Each scraper has its own configuration entry:</p> <pre><code>{\n  \"ScraperName\": {\n    \"bucket_key\": \"{main_folder}/subfolder\",\n    \"base_url\": \"https://example.com\",\n    \"cookie_selector\": \"button#accept-cookies\",\n    \"files_by_request\": true,\n    \"sources\": [\n      {\n        \"url\": \"https://example.com/articles\",\n        \"type\": \"journal\"\n      }\n    ]\n  }\n}\n</code></pre> <p>Key configuration parameters:</p> <ul> <li><code>bucket_key</code>: S3 path where data will be stored</li> <li><code>base_url</code>: Base URL of the website (optional)</li> <li><code>cookie_selector</code>: CSS selector for cookie banner (optional)</li> <li><code>files_by_request</code>: Whether to retrieve files via HTTP request vs scraping (default: <code>true</code>)</li> <li><code>sources</code>: List of URLs or configurations to scrape</li> </ul> <p>For detailed configuration examples, see the Scrapers documentation.</p>"},{"location":"data-scraping/docs/getting_started/#common-commands","title":"Common Commands","text":"Command Description <code>make up</code> Start Docker containers <code>make down</code> Stop Docker containers <code>make run</code> Run pipeline locally <code>make runpod</code> Run pipeline in production <code>make sync-requirements</code> Install Python packages"},{"location":"data-scraping/docs/getting_started/#troubleshooting","title":"Troubleshooting","text":""},{"location":"data-scraping/docs/getting_started/#docker-issues","title":"Docker Issues","text":"<p>If containers fail to start:</p> <pre><code># Stop all containers\nmake down\n\n# Remove volumes\ndocker-compose down -v\n\n# Restart\nmake up\n</code></pre>"},{"location":"data-scraping/docs/getting_started/#database-connection-issues","title":"Database Connection Issues","text":"<p>Verify MySQL is running:</p> <pre><code>docker ps | grep mysql\n</code></pre> <p>Check database logs:</p> <pre><code>docker logs &lt;mysql-container-id&gt;\n</code></pre>"},{"location":"data-scraping/docs/getting_started/#minio-access-issues","title":"MinIO Access Issues","text":"<p>Access MinIO console at <code>http://localhost:9100</code> with credentials from <code>.env</code>:</p> <ul> <li>Username: <code>minio</code></li> <li>Password: <code>minio1234</code></li> </ul>"},{"location":"data-scraping/docs/getting_started/#browserselenium-issues","title":"Browser/Selenium Issues","text":"<p>If you encounter browser-related errors:</p> <ol> <li>Ensure <code>HEADLESS_BROWSER=true</code> in <code>.env</code></li> <li>For Linux servers, set <code>XVFB_MODE=true</code></li> <li>Check that SeleniumBase is properly installed</li> </ol>"},{"location":"data-scraping/docs/getting_started/#analytics","title":"Analytics","text":"<p>At the end of each scraper execution, the pipeline stores statistics in the <code>scraper_analytics</code> table. The statistics include:</p> <ul> <li>scraped: The analyzed URLs</li> <li>content_retrieved: Resources successfully collected, grouped by whether their contents were retrieved</li> <li>uploaded: Resources successfully collected and uploaded to remote storage</li> </ul> <p>View the latest statistics:</p> <pre><code>make run args=\"--analytics-only\"\n\n# For specific scrapers\nmake run args=\"--analytics-only --scrapers IOPScraper\"\n</code></pre>"},{"location":"data-scraping/docs/getting_started/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about available scrapers in the Scrapers documentation</li> <li>Explore data models in the Model documentation</li> <li>See usage examples in the Examples page</li> <li>Learn how to add a new scraper in the Scrapers documentation</li> </ul>"},{"location":"data-scraping/docs/model/","title":"Models","text":"<p>This section documents all data models in the project.</p>"},{"location":"data-scraping/docs/model/#base-models","title":"Base Models","text":""},{"location":"data-scraping/docs/model/#model.base_models","title":"<code>model.base_models</code>","text":""},{"location":"data-scraping/docs/model/#model.base_crawling_models","title":"<code>model.base_crawling_models</code>","text":""},{"location":"data-scraping/docs/model/#model.base_direct_publisher_models","title":"<code>model.base_direct_publisher_models</code>","text":""},{"location":"data-scraping/docs/model/#model.base_iterative_publisher_models","title":"<code>model.base_iterative_publisher_models</code>","text":""},{"location":"data-scraping/docs/model/#model.base_mapped_models","title":"<code>model.base_mapped_models</code>","text":""},{"location":"data-scraping/docs/model/#model.base_pagination_publisher_models","title":"<code>model.base_pagination_publisher_models</code>","text":""},{"location":"data-scraping/docs/model/#model.base_pagination_publisher_models.BasePaginationPublisherSource","title":"<code>BasePaginationPublisherSource</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration model for the base pagination publisher scraper source. The <code>landing_page_url</code> is the URL to scrape to get the initial pagination URL.</p> Variables <p>landing_page_url (str): The landing URL to scrape</p>"},{"location":"data-scraping/docs/model/#model.base_url_publisher_models","title":"<code>model.base_url_publisher_models</code>","text":""},{"location":"data-scraping/docs/model/#database-models","title":"Database Models","text":""},{"location":"data-scraping/docs/model/#model.sql_models","title":"<code>model.sql_models</code>","text":""},{"location":"data-scraping/docs/model/#analytics-models","title":"Analytics Models","text":""},{"location":"data-scraping/docs/model/#model.analytics_models","title":"<code>model.analytics_models</code>","text":""},{"location":"data-scraping/docs/model/#publisher-specific-models","title":"Publisher-Specific Models","text":""},{"location":"data-scraping/docs/model/#model.arxiv_models","title":"<code>model.arxiv_models</code>","text":""},{"location":"data-scraping/docs/model/#model.elsevier_models","title":"<code>model.elsevier_models</code>","text":""},{"location":"data-scraping/docs/model/#model.eoa_models","title":"<code>model.eoa_models</code>","text":""},{"location":"data-scraping/docs/model/#model.isprs_models","title":"<code>model.isprs_models</code>","text":""},{"location":"data-scraping/docs/model/#model.mdpi_models","title":"<code>model.mdpi_models</code>","text":""},{"location":"data-scraping/docs/model/#model.nasa_models","title":"<code>model.nasa_models</code>","text":""},{"location":"data-scraping/docs/model/#model.ncbi_models","title":"<code>model.ncbi_models</code>","text":""},{"location":"data-scraping/docs/model/#model.oxford_academic_models","title":"<code>model.oxford_academic_models</code>","text":""},{"location":"data-scraping/docs/model/#model.seos_models","title":"<code>model.seos_models</code>","text":""},{"location":"data-scraping/docs/model/#model.wiley_models","title":"<code>model.wiley_models</code>","text":""},{"location":"data-scraping/docs/scrapers/","title":"Scrapers","text":"<p>This section documents all available scrapers in the project for collecting Earth Observation and Remote Sensing data from various academic publishers, journals, and data sources.</p>"},{"location":"data-scraping/docs/scrapers/#overview","title":"Overview","text":"<p>The scraping system is designed around a hierarchical class structure where specialized scrapers inherit from base classes that provide common functionality. Each scraper is configured via <code>config/config.json</code> and targets specific publishers or data sources.</p>"},{"location":"data-scraping/docs/scrapers/#configured-scrapers","title":"Configured Scrapers","text":"<p>The following table lists all scrapers currently configured in the system:</p> Scraper Base URL Storage Folder Description IOPScraper <code>https://iopscience.iop.org</code> <code>{main_folder}/iopscience</code> IOP Science journal articles and issues MDPIScraper <code>https://www.mdpi.com</code> <code>{main_folder}/mdpi</code> MDPI journals including Remote Sensing, Geosciences, Atmosphere SpringerScraper <code>https://link.springer.com</code> <code>{main_folder}/springer</code> Springer journals, books, and search results AMSScraper <code>https://journals.ametsoc.org</code> <code>{main_folder}/ams</code> American Meteorological Society publications CopernicusScraper Multiple Copernicus journals <code>{main_folder}/copernicus</code> 16+ Copernicus open-access journals CopernicusCatalogueScraper <code>https://www.copernicus.eu/</code> <code>{main_folder}/copernicus</code> Copernicus services catalogue SeosScraper <code>https://seos-project.eu</code> <code>{main_folder}/seos</code> SEOS project educational materials NCBIScraper <code>https://www.ncbi.nlm.nih.gov</code> <code>{main_folder}/ncbi</code> NCBI PubMed Central articles CambridgeUniversityPressScraper <code>https://www.cambridge.org</code> <code>{main_folder}/cambridge_university_press</code> Cambridge University Press journals OxfordAcademicScraper <code>https://academic.oup.com</code> <code>{main_folder}/oxford_academic</code> Oxford Academic journals IEEEScraper <code>https://ieeexplore.ieee.org</code> <code>{main_folder}/ieee</code> IEEE Xplore open access articles TaylorAndFrancisScraper <code>https://www.tandfonline.com</code> <code>{main_folder}/taylor_and_francis</code> Taylor &amp; Francis journals FrontiersScraper <code>https://www.frontiersin.org/</code> <code>{main_folder}/frontiers</code> Frontiers in Remote Sensing SageScraper <code>https://journals.sagepub.com</code> <code>{main_folder}/sage</code> SAGE Publications journals EOGEScraper <code>https://eoge.ut.ac.ir</code> <code>{main_folder}/eoge</code> Earth Observations and Geomatics Engineering journal ArxivScraper <code>https://arxiv.org</code> <code>{main_folder}/arxiv</code> arXiv preprints WileyScraper Multiple Wiley domains <code>{main_folder}/wiley</code> Wiley journals (AGU, EOS, etc.) EOSScraper <code>https://eos.org/</code> <code>{main_folder}/eos</code> EOS Science News archives ESAScraper Multiple ESA domains <code>{main_folder}/esa</code> ESA Earth Online, EO Portal, Sentiwiki ElsevierScraper <code>https://www.sciencedirect.com</code> <code>{main_folder}/elsevier</code> ScienceDirect open access journals NASAScraper Multiple NASA domains <code>{main_folder}/nasa</code> NASA EarthData, NTRS, EOS Portal OpenNightLightsScraper <code>https://worldbank.github.io/OpenNightLights/</code> <code>{main_folder}/open_night_lights_scraper</code> World Bank Open Night Lights documentation WikipediaScraper <code>https://en.wikipedia.org/</code> <code>{main_folder}/wikipedia</code> Wikipedia EO-related categories MITScraper <code>https://ocw.mit.edu/</code> <code>{main_folder}/mit</code> MIT OpenCourseWare JAXAScraper <code>https://earth.jaxa.jp/en/eo-knowledge</code> <code>{main_folder}/jaxa</code> JAXA Earth Observation knowledge base UKMetOfficeScraper <code>https://library.metoffice.gov.uk</code> <code>{main_folder}/uk_met_office</code> UK Met Office library EOAScraper <code>https://www.eoa.org.au/</code> <code>{main_folder}/eoa</code> Earth Observation Australia textbooks ISPRSScraper <code>https://www.isprs.org/</code> <code>{main_folder}/isprs</code> ISPRS publication archives EUMETSATScraper Multiple EUMETSAT domains <code>{main_folder}/eumetsat</code> EUMETSAT documentation and case studies EarthDataScienceScraper <code>https://www.earthdatascience.org/</code> <code>{main_folder}/earth_data_science</code> Earth Data Science tutorials DirectLinksScraper Various <code>{main_folder}/miscellaneous</code> Direct PDF links from multiple sources IntechOpenScraper <code>https://www.intechopen.com/</code> <code>{main_folder}/intech_open</code> IntechOpen books and chapters"},{"location":"data-scraping/docs/scrapers/#base-scraper-architecture","title":"Base Scraper Architecture","text":"<p>The scraping system is built on a set of abstract base classes that provide common functionality. Understanding these base classes is essential for extending or modifying the scraping behavior.</p>"},{"location":"data-scraping/docs/scrapers/#base-class-descriptions","title":"Base Class Descriptions","text":"<p>BaseScraper: The root abstract class that all scrapers inherit from. Provides core functionality including Selenium WebDriver management, cookie handling, S3 storage integration, database repository access, and analytics tracking. Every scraper implements the abstract <code>scrape()</code> method defined here.</p> <p>BaseIterativePublisherScraper: Designed for publishers that organize content in a journal \u2192 volume \u2192 issue hierarchy. Iterates through volumes and issues systematically, with support for handling missing volumes/issues using consecutive threshold logic.</p> <p>BasePaginationPublisherScraper: Handles publishers with paginated search results or article listings. Automatically navigates through pages until no more results are found, with configurable page sizes and maximum paper limits.</p> <p>BaseUrlPublisherScraper: Used for publishers where content URLs follow predictable patterns. Processes lists of URLs and extracts content directly without complex navigation.</p> <p>BaseMappedPublisherScraper: For publishers that require mapping between different URL structures or identifiers before scraping content. Provides a two-stage process: first mapping, then scraping.</p> <p>BaseCrawlingScraper: Implements recursive web crawling from a starting URL. Follows links within the same domain and extracts content from all discovered pages. Useful for documentation sites and knowledge bases.</p> <p>BaseSourceDownloadScraper: Specialized for direct file downloads where download URLs are known in advance. Handles PDF and other document formats directly without HTML parsing.</p>"},{"location":"data-scraping/docs/scrapers/#adding-a-new-scraper","title":"Adding a New Scraper","text":"<p>To extend the pipeline with a new scraper, follow these steps:</p>"},{"location":"data-scraping/docs/scrapers/#1-create-scraper-file","title":"1. Create Scraper File","text":"<p>Create a new file in the <code>scraper</code> folder with the name of your scraper:</p> <pre><code># scraper/new_publisher_scraper.py\nfrom scraper.base_scraper import BaseScraper\nfrom model.new_publisher_models import NewPublisherConfig\n\nclass NewPublisherScraper(BaseScraper):\n    \"\"\"\n    Scraper for New Publisher website.\n    \"\"\"\n\n    @property\n    def config_model_type(self):\n        \"\"\"Return the Pydantic model for configuration.\"\"\"\n        return NewPublisherConfig\n\n    def scrape(self):\n        \"\"\"\n        Scrape the website and return scraped data.\n\n        Returns:\n            dict: Dictionary containing scraped data\n        \"\"\"\n        # Implement your scraping logic here\n        # Use self._driver for Selenium operations\n        # Use self._config_model to access configuration\n\n        scraped_data = {}\n\n        for source in self._config_model.sources:\n            # Navigate to URL\n            self._driver.open(source.url)\n\n            # Handle cookies if needed\n            if not self._cookie_handled and self._config_model.cookie_selector:\n                self._driver.click(self._config_model.cookie_selector)\n                self._cookie_handled = True\n\n            # Extract data\n            # ... your scraping logic ...\n\n        return scraped_data\n\n    def post_process(self, scraped_data):\n        \"\"\"\n        Process scraped data and return URLs to download.\n\n        Args:\n            scraped_data: Data returned from scrape()\n\n        Returns:\n            List[str]: List of URLs to download/upload\n        \"\"\"\n        urls = []\n\n        # Process scraped_data and extract download URLs\n        # ... your post-processing logic ...\n\n        return urls\n</code></pre>"},{"location":"data-scraping/docs/scrapers/#2-create-model-file-optional","title":"2. Create Model File (Optional)","text":"<p>If you need custom Pydantic models, create a file in the <code>model</code> folder:</p> <pre><code># model/new_publisher_models.py\nfrom typing import List\nfrom pydantic import Field\nfrom model.base_models import BaseConfig, BaseSource\n\nclass NewPublisherSource(BaseSource):\n    \"\"\"Configuration for a single source.\"\"\"\n    url: str\n    type: str = \"journal\"\n    # Add custom fields as needed\n\nclass NewPublisherConfig(BaseConfig):\n    \"\"\"Configuration model for NewPublisherScraper.\"\"\"\n    base_url: str\n    sources: List[NewPublisherSource]\n    # Add any custom configuration fields\n    custom_field: str = Field(default=\"default_value\")\n</code></pre> <p>Note: If you need enumerators, extend <code>Enum</code> from the <code>base_enum</code> module.</p>"},{"location":"data-scraping/docs/scrapers/#3-choose-the-right-base-class","title":"3. Choose the Right Base Class","text":"<p>Select the appropriate base class for your scraper:</p> <ul> <li>BaseScraper: For custom scraping logic</li> <li>BaseIterativePublisherScraper: For journal \u2192 volume \u2192 issue hierarchies</li> <li>BasePaginationPublisherScraper: For paginated search results</li> <li>BaseUrlPublisherScraper: For simple URL lists</li> <li>BaseMappedPublisherScraper: For two-stage mapping and scraping</li> <li>BaseCrawlingScraper: For recursive web crawling</li> <li>BaseSourceDownloadScraper: For direct file downloads</li> </ul> <p>Example using <code>BaseIterativePublisherScraper</code>:</p> <pre><code>from scraper.base_iterative_publisher_scraper import BaseIterativePublisherScraper\n\nclass NewJournalScraper(BaseIterativePublisherScraper):\n    @property\n    def config_model_type(self):\n        return NewJournalConfig\n\n    def _scrape_journal(self, journal):\n        # Implement journal-specific scraping\n        pass\n</code></pre>"},{"location":"data-scraping/docs/scrapers/#4-add-configuration-to-configjson","title":"4. Add Configuration to config.json","text":"<p>Add your scraper's configuration to <code>config/config.json</code> (you can find more examples here):</p> <pre><code>{\n  \"NewPublisherScraper\": {\n    \"bucket_key\": \"{main_folder}/new_publisher\",\n    \"base_url\": \"https://newpublisher.com\",\n    \"cookie_selector\": \"button#accept-cookies\",\n    \"files_by_request\": true,\n    \"sources\": [\n      {\n        \"url\": \"https://newpublisher.com/articles\",\n        \"type\": \"journal\"\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"data-scraping/docs/scrapers/#configuration-keys","title":"Configuration Keys","text":"<p>Required:</p> <ul> <li><code>bucket_key</code>: S3 storage path (use <code>{main_folder}</code> placeholder)</li> </ul> <p>Optional:</p> <ul> <li><code>base_url</code>: Base URL of the website</li> <li><code>cookie_selector</code>: CSS selector for cookie banner acceptance button</li> <li><code>files_by_request</code>: Whether to download files via HTTP (default: <code>true</code>) or scrape them</li> <li><code>request_with_proxy</code>: Use proxy for requests (default: <code>false</code>)</li> <li><code>sources</code>: List of sources to scrape (structure depends on your model)</li> </ul>"},{"location":"data-scraping/docs/scrapers/#5-test-your-scraper","title":"5. Test Your Scraper","text":"<p>Run your new scraper:</p> <pre><code># Run with force to test from scratch\nmake run args=\"--scrapers NewPublisherScraper --force\"\n\n# Check logs for errors\ndocker logs &lt;container-id&gt;\n</code></pre>"},{"location":"data-scraping/docs/scrapers/#6-verify-results","title":"6. Verify Results","text":"<p>Check that data was scraped and uploaded correctly:</p> <ol> <li>MinIO Console: Visit <code>http://localhost:9100</code> and check your bucket</li> <li>Database: Query the <code>scraper_output</code> and <code>uploaded_resource</code> tables</li> <li>Analytics: Run <code>make run args=\"--analytics-only --scrapers NewPublisherScraper\"</code></li> </ol>"},{"location":"data-scraping/docs/scrapers/#example-complete-simple-scraper","title":"Example: Complete Simple Scraper","text":"<p>Here's a complete example of a simple scraper:</p> <pre><code># scraper/example_scraper.py\nfrom typing import List\nfrom scraper.base_scraper import BaseScraper\nfrom model.base_models import BaseConfig, BaseSource\n\nclass ExampleConfig(BaseConfig):\n    base_url: str\n    sources: List[BaseSource]\n\nclass ExampleScraper(BaseScraper):\n    @property\n    def config_model_type(self):\n        return ExampleConfig\n\n    def scrape(self):\n        pdf_links = []\n\n        for source in self._config_model.sources:\n            self._driver.open(source.url)\n\n            # Find all PDF links\n            links = self._driver.find_elements(\"a[href$='.pdf']\")\n            for link in links:\n                href = link.get_attribute('href')\n                if href:\n                    pdf_links.append(href)\n\n        return {\"pdf_links\": pdf_links}\n\n    def post_process(self, scraped_data):\n        return scraped_data.get(\"pdf_links\", [])\n</code></pre> <p>With configuration:</p> <pre><code>{\n  \"ExampleScraper\": {\n    \"bucket_key\": \"{main_folder}/example\",\n    \"base_url\": \"https://example.com\",\n    \"sources\": [\n      {\"url\": \"https://example.com/papers\"}\n    ]\n  }\n}\n</code></pre>"},{"location":"data-scraping/docs/scrapers/#common-selenium-operations","title":"Common Selenium Operations","text":"<pre><code># Open URL\nself._driver.open(url)\n\n# Click element\nself._driver.click(selector)\n\n# Find elements\nelements = self._driver.find_elements(selector)\n\n# Get attribute\nhref = element.get_attribute('href')\n\n# Get text\ntext = element.text\n\n# Wait for element\nself._driver.wait_for_element(selector)\n\n# Execute JavaScript\nself._driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n</code></pre>"},{"location":"data-scraping/docs/scrapers/#code-reference","title":"Code Reference","text":"<p>Below is the detailed API documentation for all scraper classes:</p>"},{"location":"data-scraping/docs/scrapers/#base-classes","title":"Base Classes","text":""},{"location":"data-scraping/docs/scrapers/#scraper.base_scraper","title":"<code>scraper.base_scraper</code>","text":""},{"location":"data-scraping/docs/scrapers/#scraper.base_scraper.BaseScraper","title":"<code>BaseScraper</code>","text":"<p>               Bases: <code>ABC</code></p>"},{"location":"data-scraping/docs/scrapers/#scraper.base_scraper.BaseScraper.config_model_type","title":"<code>config_model_type</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Return the configuration model type. This property must be implemented in the derived class.</p> <p>Returns:</p> Type Description <code>Type[BaseConfig]</code> <p>Type[BaseConfig]: The configuration model type</p>"},{"location":"data-scraping/docs/scrapers/#scraper.base_scraper.BaseScraper.post_process","title":"<code>post_process(scrape_output)</code>  <code>abstractmethod</code>","text":"<p>Post-process the scraped output. This method is called after the sources have been scraped. It is used to retrieve the final list of processed URLs. This method must be implemented in the derived class.</p> <p>Parameters:</p> Name Type Description Default <code>scrape_output</code> <code>Any</code> <p>The scraped output</p> required <p>Returns:</p> Type Description <code>Dict[str, List[str]] | List[str]</code> <p>Dict[str, List[str]] | List[str]: A dictionary or a list containing the processed links</p>"},{"location":"data-scraping/docs/scrapers/#scraper.base_scraper.BaseScraper.resume_scraping","title":"<code>resume_scraping()</code>","text":"<p>Resume the scraping of the resources that failed to scrape.</p>"},{"location":"data-scraping/docs/scrapers/#scraper.base_scraper.BaseScraper.resume_uploads","title":"<code>resume_uploads()</code>","text":"<p>Resume the uploads of the resources that failed to upload.</p>"},{"location":"data-scraping/docs/scrapers/#scraper.base_scraper.BaseScraper.scrape","title":"<code>scrape()</code>  <code>abstractmethod</code>","text":"<p>Scrape the resources links. This method must be implemented in the derived class.</p> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any | None</code> <p>The output of the scraping, or None if something went wrong.</p>"},{"location":"data-scraping/docs/scrapers/#scraper.base_scraper.BaseScraper.scrape_failure","title":"<code>scrape_failure(failure)</code>  <code>abstractmethod</code>","text":"<p>Scrape the failed resource. This method must be implemented in the derived class.</p> <p>Parameters:</p> Name Type Description Default <code>failure</code> <code>ScraperFailure</code> <p>The failure model.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: The list of the successfully scraped links</p>"},{"location":"data-scraping/docs/scrapers/#scraper.base_scraper.BaseScraper.upload_to_s3","title":"<code>upload_to_s3(sources_links)</code>","text":"<p>Upload the source files to S3.</p> <p>Parameters:</p> Name Type Description Default <code>sources_links</code> <code>Dict[str, List[str]] | List[str]</code> <p>The list of links of the various sources.</p> required"},{"location":"data-scraping/docs/scrapers/#scraper.base_crawling_scraper","title":"<code>scraper.base_crawling_scraper</code>","text":""},{"location":"data-scraping/docs/scrapers/#scraper.base_crawling_scraper.BaseCrawlingScraper","title":"<code>BaseCrawlingScraper</code>","text":"<p>               Bases: <code>BaseScraper</code></p>"},{"location":"data-scraping/docs/scrapers/#scraper.base_crawling_scraper.BaseCrawlingScraper.crawling_folder_path","title":"<code>crawling_folder_path</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>The folder path where the crawling files are stored. This property must be implemented in the derived class.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The folder path.</p>"},{"location":"data-scraping/docs/scrapers/#scraper.base_crawling_scraper.BaseCrawlingScraper.scrape","title":"<code>scrape()</code>","text":"<p>Scrape the website, even better crawl the website.</p> <p>Returns:</p> Name Type Description <code>BaseCrawledPublisherScraperOutput</code> <code>BaseCrawlingScraperOutput | None</code> <p>The output of the scraper, or None if the scraping failed.</p>"},{"location":"data-scraping/docs/scrapers/#scraper.base_iterative_publisher_scraper","title":"<code>scraper.base_iterative_publisher_scraper</code>","text":""},{"location":"data-scraping/docs/scrapers/#scraper.base_iterative_publisher_scraper.BaseIterativePublisherScraper","title":"<code>BaseIterativePublisherScraper</code>","text":"<p>               Bases: <code>BaseScraper</code></p>"},{"location":"data-scraping/docs/scrapers/#scraper.base_iterative_publisher_scraper.BaseIterativePublisherScraper.journal_identifier","title":"<code>journal_identifier(model)</code>  <code>abstractmethod</code>","text":"<p>Return the journal identifier. This method must be implemented in the derived class.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BaseIterativePublisherJournal</code> <p>The configuration model.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The journal identifier</p>"},{"location":"data-scraping/docs/scrapers/#scraper.base_iterative_publisher_scraper.BaseIterativePublisherScraper.post_process","title":"<code>post_process(scrape_output)</code>","text":"<p>Extract the PDF links from the dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>scrape_output</code> <code>IterativePublisherScrapeOutput</code> <p>A dictionary containing the PDF links.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of strings containing the PDF links</p>"},{"location":"data-scraping/docs/scrapers/#scraper.base_iterative_publisher_scraper.BaseIterativePublisherScraper.scrape","title":"<code>scrape()</code>","text":"<p>Scrape the journals for PDF links.</p> <p>Returns:</p> Type Description <code>IterativePublisherScrapeOutput | None</code> <p>IterativePublisherScrapeOutput | None: A dictionary containing the PDF links, or None if no link was found.</p>"},{"location":"data-scraping/docs/scrapers/#scraper.base_mapped_publisher_scraper","title":"<code>scraper.base_mapped_publisher_scraper</code>","text":""},{"location":"data-scraping/docs/scrapers/#scraper.base_mapped_publisher_scraper.BaseMappedPublisherScraper","title":"<code>BaseMappedPublisherScraper</code>","text":"<p>               Bases: <code>BaseScraper</code></p>"},{"location":"data-scraping/docs/scrapers/#scraper.base_mapped_publisher_scraper.BaseMappedPublisherScraper.config_model_type","title":"<code>config_model_type</code>  <code>property</code>","text":"<p>Return the configuration model type.</p> <p>Returns:</p> Type Description <code>Type[BaseMappedConfig]</code> <p>Type[BaseMappedConfig]: The configuration model type</p>"},{"location":"data-scraping/docs/scrapers/#scraper.base_mapped_publisher_scraper.BaseMappedPublisherScraper.mapping","title":"<code>mapping</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Return the mapping of the scraper to the source. This method must be implemented in the derived class.</p> <p>Returns:</p> Type Description <code>Dict[str, Type[BaseMappedSubScraper]]</code> <p>Dict[str, Type[BaseMappedSubScraper]]: The mapping of the scraper to the source</p>"},{"location":"data-scraping/docs/scrapers/#scraper.base_mapped_publisher_scraper.BaseMappedPublisherScraper.post_process","title":"<code>post_process(scrape_output)</code>","text":"<p>Post-process the scraped output. This method is called after the sources have been scraped. It is used to retrieve the final list of processed URLs. This method must be implemented in the derived class.</p> <p>Parameters:</p> Name Type Description Default <code>scrape_output</code> <code>Dict[str, List[str] | Dict[str, List[str]]]</code> <p>The scraped output</p> required <p>Returns:</p> Type Description <code>Dict[str, List[str]]</code> <p>Dict[str, List[str]]: The results of the scraping</p>"},{"location":"data-scraping/docs/scrapers/#scraper.base_mapped_publisher_scraper.BaseMappedPublisherScraper.scrape","title":"<code>scrape()</code>","text":"<p>Scrape the resources links.</p> <p>Returns:</p> Type Description <code>Dict[str, List[str] | Dict[str, List[str]]] | None</code> <p>Dict[str, List | Dict]: The output of the scraping.</p>"},{"location":"data-scraping/docs/scrapers/#scraper.base_pagination_publisher_scraper","title":"<code>scraper.base_pagination_publisher_scraper</code>","text":""},{"location":"data-scraping/docs/scrapers/#scraper.base_pagination_publisher_scraper.BasePaginationPublisherScraper","title":"<code>BasePaginationPublisherScraper</code>","text":"<p>               Bases: <code>BaseScraper</code></p>"},{"location":"data-scraping/docs/scrapers/#scraper.base_pagination_publisher_scraper.BasePaginationPublisherScraper.post_process","title":"<code>post_process(scrape_output)</code>","text":"<p>Extract the href attribute from the links.</p> <p>Parameters:</p> Name Type Description Default <code>scrape_output</code> <code>BasePaginationPublisherScrapeOutput</code> <p>A dictionary containing the PDF links. Each key is the name of the source which PDF links have been found for, and the value is the list of PDF links itself.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of strings containing the PDF links</p>"},{"location":"data-scraping/docs/scrapers/#scraper.base_pagination_publisher_scraper.BasePaginationPublisherScraper.scrape","title":"<code>scrape()</code>  <code>abstractmethod</code>","text":"<p>Scrape the resources links. This method must be implemented in the derived class.</p> <p>Returns:</p> Type Description <code>BasePaginationPublisherScrapeOutput | None</code> <p>BasePaginationPublisherScrapeOutput | None: The output of the scraping, i.e., a dictionary containing the PDF links. Each key is the name of the source which PDF links have been found for, and the value is the list of PDF links itself.</p>"},{"location":"data-scraping/docs/scrapers/#scraper.base_url_publisher_scraper","title":"<code>scraper.base_url_publisher_scraper</code>","text":""},{"location":"data-scraping/docs/scrapers/#scraper.base_url_publisher_scraper.BaseUrlPublisherScraper","title":"<code>BaseUrlPublisherScraper</code>","text":"<p>               Bases: <code>BaseScraper</code></p>"},{"location":"data-scraping/docs/scrapers/#scraper.base_url_publisher_scraper.BaseUrlPublisherScraper.post_process","title":"<code>post_process(scrape_output)</code>","text":"<p>Extract the href attribute from the links.</p> <p>Parameters:</p> Name Type Description Default <code>scrape_output</code> <code>ResultSet | List[Tag]</code> <p>A ResultSet (i.e., a list) or a list of Tag objects containing the tags to the PDF links.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of strings containing the PDF links</p>"},{"location":"data-scraping/docs/scrapers/#scraper.base_url_publisher_scraper.BaseUrlPublisherScraper.scrape","title":"<code>scrape()</code>","text":"<p>Scrape the source URLs of for PDF links.</p> <p>Returns:</p> Type Description <code>ResultSet | List[Tag] | None</code> <p>ResultSet | List[Tag]: A ResultSet (i.e., a list) or a list of Tag objects containing the tags to the PDF links. If no tag was found, return None.</p>"},{"location":"data-scraping/docs/scrapers/#scraper.base_source_download_scraper","title":"<code>scraper.base_source_download_scraper</code>","text":""},{"location":"data-scraping/docs/scrapers/#scraper.base_source_download_scraper.BaseSourceDownloadScraper","title":"<code>BaseSourceDownloadScraper</code>","text":"<p>               Bases: <code>BaseScraper</code>, <code>ABC</code></p>"},{"location":"data-scraping/docs/scrapers/#publisher-scrapers","title":"Publisher Scrapers","text":""},{"location":"data-scraping/docs/scrapers/#scraper.ams_scraper","title":"<code>scraper.ams_scraper</code>","text":""},{"location":"data-scraping/docs/scrapers/#scraper.arxiv_scraper","title":"<code>scraper.arxiv_scraper</code>","text":""},{"location":"data-scraping/docs/scrapers/#scraper.cambridge_university_press_scraper","title":"<code>scraper.cambridge_university_press_scraper</code>","text":""},{"location":"data-scraping/docs/scrapers/#scraper.elsevier_scraper","title":"<code>scraper.elsevier_scraper</code>","text":""},{"location":"data-scraping/docs/scrapers/#scraper.elsevier_scraper.ElsevierScraper","title":"<code>ElsevierScraper</code>","text":"<p>               Bases: <code>BaseSourceDownloadScraper</code></p>"},{"location":"data-scraping/docs/scrapers/#scraper.elsevier_scraper.ElsevierScraper.__scrape_issue","title":"<code>__scrape_issue(source)</code>","text":"<p>Scrape the issue for the PDFs. The logic is as follows:</p> <ul> <li>Find the next issue URL, i.e., the URL of the previous issue, if it exists.</li> <li>Check if there are any PDFs to download. If not, try with the next issue.</li> <li>Download the PDFs in a zip file and wait for the download to complete.</li> <li>Unpack the zip files in a temporary folder.</li> <li>Return the result of the scraping. If the issue was scraped successfully, return the next issue URL, i.e.,     the URL of the previous issue to scrape next.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>ElsevierSource</code> <p>The source model.</p> required <p>Returns:</p> Name Type Description <code>ElsevierScrapeIssueOutput</code> <code>ElsevierScrapeIssueOutput</code> <p>The result of the scraping.</p>"},{"location":"data-scraping/docs/scrapers/#scraper.elsevier_scraper.ElsevierScraper.__scrape_journal","title":"<code>__scrape_journal(source)</code>","text":"<p>Scrape the journal for the issues. The logic is as follows:</p> <ul> <li>Get the first issue link from the journal page, i.e., the newest issue.</li> <li>Scrape the issue and get the next issue URL. If the issue was scraped successfully, add the issue URL to the     list of journal links.</li> <li>Repeat the process until there are no more issues to scrape.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>ElsevierSource</code> <p>The source model.</p> required <p>Returns:</p> Type Description <code>List[str] | None</code> <p>List[str] | None: The list of journal links if the journal was scraped successfully, None otherwise</p>"},{"location":"data-scraping/docs/scrapers/#scraper.frontiers_scraper","title":"<code>scraper.frontiers_scraper</code>","text":""},{"location":"data-scraping/docs/scrapers/#scraper.ieee_scraper","title":"<code>scraper.ieee_scraper</code>","text":""},{"location":"data-scraping/docs/scrapers/#scraper.intechopen_scraper","title":"<code>scraper.intechopen_scraper</code>","text":""},{"location":"data-scraping/docs/scrapers/#scraper.iop_scraper","title":"<code>scraper.iop_scraper</code>","text":""},{"location":"data-scraping/docs/scrapers/#scraper.isprs_scraper","title":"<code>scraper.isprs_scraper</code>","text":""},{"location":"data-scraping/docs/scrapers/#scraper.isprs_scraper.ISPRSScraper","title":"<code>ISPRSScraper</code>","text":"<p>               Bases: <code>BaseScraper</code></p>"},{"location":"data-scraping/docs/scrapers/#scraper.isprs_scraper.ISPRSScraper.__scrape_archive_article","title":"<code>__scrape_archive_article(article_link)</code>","text":"<p>Scrape a single article from the archives. The article contains the PDF link. If the article does not contain a PDF link, it will be saved as a failure.</p> <p>Parameters:</p> Name Type Description Default <code>article_link</code> <code>str</code> <p>The article link to scrape.</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>str | None: The PDF link found in the article.</p>"},{"location":"data-scraping/docs/scrapers/#scraper.isprs_scraper.ISPRSScraper.__scrape_archives","title":"<code>__scrape_archives(archive_links)</code>","text":"<p>Scrape the archives for PDF links. The archives contain links to articles, which in turn contain the PDF links.</p> <p>Parameters:</p> Name Type Description Default <code>archive_links</code> <code>List[str]</code> <p>A list of archive links.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of PDF links found in the archives.</p>"},{"location":"data-scraping/docs/scrapers/#scraper.isprs_scraper.ISPRSScraper.__scrape_proceedings","title":"<code>__scrape_proceedings(proceedings_urls)</code>","text":"<p>Scrape the proceedings for PDF links. The proceedings contain links to articles, which in turn contain the PDF links.</p> <p>Parameters:</p> Name Type Description Default <code>proceedings_urls</code> <code>List[str]</code> <p>A list of proceedings links.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of PDF links found in the proceedings</p>"},{"location":"data-scraping/docs/scrapers/#scraper.mdpi_scraper","title":"<code>scraper.mdpi_scraper</code>","text":""},{"location":"data-scraping/docs/scrapers/#scraper.mdpi_scraper.MDPIJournalsScraper","title":"<code>MDPIJournalsScraper</code>","text":"<p>               Bases: <code>BaseIterativePublisherScraper</code>, <code>BaseMappedSubScraper</code></p>"},{"location":"data-scraping/docs/scrapers/#scraper.mdpi_scraper.MDPIJournalsScraper.__scrape_url","title":"<code>__scrape_url(url)</code>","text":"<p>Scrape the issue URL for PDF links.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The issue URL.</p> required <p>Returns:</p> Type Description <code>IterativePublisherScrapeIssueOutput | None</code> <p>BaseIterativePublisherScrapeIssueOutput | None: A list of PDF links found in the issue, or None if something went wrong.</p>"},{"location":"data-scraping/docs/scrapers/#scraper.ncbi_scraper","title":"<code>scraper.ncbi_scraper</code>","text":""},{"location":"data-scraping/docs/scrapers/#scraper.oxford_academic_scraper","title":"<code>scraper.oxford_academic_scraper</code>","text":""},{"location":"data-scraping/docs/scrapers/#scraper.oxford_academic_scraper.OxfordAcademicScraper","title":"<code>OxfordAcademicScraper</code>","text":"<p>               Bases: <code>BaseIterativePublisherScraper</code></p>"},{"location":"data-scraping/docs/scrapers/#scraper.oxford_academic_scraper.OxfordAcademicScraper.__scrape_issue","title":"<code>__scrape_issue(issue_url)</code>","text":"<p>Scrape the issue URL for PDF links.</p> <p>Parameters:</p> Name Type Description Default <code>issue_url</code> <code>str</code> <p>The issue URL to scrape.</p> required <p>Returns:</p> Type Description <code>IterativePublisherScrapeIssueOutput | None</code> <p>IterativePublisherScrapeIssueOutput | None: A list of PDF links found in the issue, or None is something went wrong</p>"},{"location":"data-scraping/docs/scrapers/#scraper.sage_scraper","title":"<code>scraper.sage_scraper</code>","text":""},{"location":"data-scraping/docs/scrapers/#scraper.springer_scraper","title":"<code>scraper.springer_scraper</code>","text":""},{"location":"data-scraping/docs/scrapers/#scraper.taylor_and_francis_scraper","title":"<code>scraper.taylor_and_francis_scraper</code>","text":""},{"location":"data-scraping/docs/scrapers/#scraper.wiley_scraper","title":"<code>scraper.wiley_scraper</code>","text":""},{"location":"data-scraping/docs/scrapers/#data-source-scrapers","title":"Data Source Scrapers","text":""},{"location":"data-scraping/docs/scrapers/#scraper.copernicus_catalogue_scraper","title":"<code>scraper.copernicus_catalogue_scraper</code>","text":""},{"location":"data-scraping/docs/scrapers/#scraper.copernicus_scraper","title":"<code>scraper.copernicus_scraper</code>","text":""},{"location":"data-scraping/docs/scrapers/#scraper.copernicus_scraper.CopernicusScraper","title":"<code>CopernicusScraper</code>","text":"<p>               Bases: <code>BaseIterativeWithConstraintPublisherScraper</code></p>"},{"location":"data-scraping/docs/scrapers/#scraper.copernicus_scraper.CopernicusScraper.__scrape_article","title":"<code>__scrape_article(article_url)</code>","text":"<p>Scrape a single article.</p> <p>Parameters:</p> Name Type Description Default <code>article_url</code> <code>str</code> <p>The article URL to scrape.</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>str | None: The string containing the PDF link.</p>"},{"location":"data-scraping/docs/scrapers/#scraper.copernicus_scraper.CopernicusScraper.__scrape_issue","title":"<code>__scrape_issue(issue_url)</code>","text":"<p>Scrape the issue URL for PDF links.</p> <p>Parameters:</p> Name Type Description Default <code>issue_url</code> <code>str</code> <p>The issue URL to scrape.</p> required <p>Returns:</p> Type Description <code>IterativePublisherScrapeIssueOutput | None</code> <p>IterativePublisherScrapeIssueOutput | None: A list of PDF links found in the issue, or None is something went wrong</p>"},{"location":"data-scraping/docs/scrapers/#scraper.direct_links_scraper","title":"<code>scraper.direct_links_scraper</code>","text":""},{"location":"data-scraping/docs/scrapers/#scraper.earth_data_science_scraper","title":"<code>scraper.earth_data_science_scraper</code>","text":""},{"location":"data-scraping/docs/scrapers/#scraper.eoa_scraper","title":"<code>scraper.eoa_scraper</code>","text":""},{"location":"data-scraping/docs/scrapers/#scraper.eoge_scraper","title":"<code>scraper.eoge_scraper</code>","text":""},{"location":"data-scraping/docs/scrapers/#scraper.eos_scraper","title":"<code>scraper.eos_scraper</code>","text":""},{"location":"data-scraping/docs/scrapers/#scraper.esa_scraper","title":"<code>scraper.esa_scraper</code>","text":""},{"location":"data-scraping/docs/scrapers/#scraper.eumetsat_scraper","title":"<code>scraper.eumetsat_scraper</code>","text":""},{"location":"data-scraping/docs/scrapers/#scraper.jaxa_scraper","title":"<code>scraper.jaxa_scraper</code>","text":""},{"location":"data-scraping/docs/scrapers/#scraper.mit_scraper","title":"<code>scraper.mit_scraper</code>","text":""},{"location":"data-scraping/docs/scrapers/#scraper.nasa_scraper","title":"<code>scraper.nasa_scraper</code>","text":""},{"location":"data-scraping/docs/scrapers/#scraper.open_night_lights_scraper","title":"<code>scraper.open_night_lights_scraper</code>","text":""},{"location":"data-scraping/docs/scrapers/#scraper.seos_scraper","title":"<code>scraper.seos_scraper</code>","text":""},{"location":"data-scraping/docs/scrapers/#scraper.seos_scraper.SeosScraper","title":"<code>SeosScraper</code>","text":"<p>               Bases: <code>BaseScraper</code></p>"},{"location":"data-scraping/docs/scrapers/#scraper.seos_scraper.SeosScraper.config_model_type","title":"<code>config_model_type</code>  <code>property</code>","text":"<p>Return the configuration model type.</p> <p>Returns:</p> Type Description <code>Type[SeosConfig]</code> <p>Type[SeosConfig]: The configuration model type</p>"},{"location":"data-scraping/docs/scrapers/#scraper.seos_scraper.SeosScraper.__scrape_source","title":"<code>__scrape_source(source)</code>","text":"<p>Scrape the source URL for HTML links.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>SeosSource</code> <p>The source to scrape.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of HTML links.</p>"},{"location":"data-scraping/docs/scrapers/#scraper.seos_scraper.SeosScraper.post_process","title":"<code>post_process(scrape_output)</code>","text":"<p>Extract the href attribute from the links.</p> <p>Parameters:</p> Name Type Description Default <code>scrape_output</code> <code>Dict[str, List[Tag]]</code> <p>A dictionary collecting, for each source, the corresponding list of Tag objects containing the tags to the HTML links.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of strings containing the HTML links</p>"},{"location":"data-scraping/docs/scrapers/#scraper.seos_scraper.SeosScraper.scrape","title":"<code>scrape()</code>","text":"<p>Scrape the Seos sources for HTML links.</p> <p>Returns:</p> Type Description <code>Dict[str, List[str]] | None</code> <p>Dict[str, List[str]]: a dictionary collecting, for each source, the corresponding list of the HTML links. If no link was found, return None.</p>"},{"location":"data-scraping/docs/scrapers/#scraper.uk_met_office_scraper","title":"<code>scraper.uk_met_office_scraper</code>","text":""},{"location":"data-scraping/docs/scrapers/#scraper.wikipedia_scraper","title":"<code>scraper.wikipedia_scraper</code>","text":""},{"location":"data-processing/docs/","title":"Data Processing Pipeline","text":"<p>A high-performance, modular library for extracting, deduplicating, cleaning, anonymizing, and exporting large-scale Earth science and Earth observation datasets.</p>"},{"location":"data-processing/docs/#features","title":"Features","text":""},{"location":"data-processing/docs/#extraction","title":"Extraction","text":"<ul> <li>Supports PDF, HTML, XML, Markdown and nested folder structures</li> <li>Automatically detects file formats unless explicitly specified</li> </ul>"},{"location":"data-processing/docs/#deduplication","title":"Deduplication","text":"<ul> <li>Performs exact matching using SHA-256 checksum</li> <li>Supports LSH based near-duplicate detection with configurable:</li> <li>Shingle size</li> <li>Permutations</li> <li>Similarity threshold</li> </ul>"},{"location":"data-processing/docs/#cleaning","title":"Cleaning","text":"<ul> <li>Removes irregularities and noise artifacts</li> <li>Corrects LaTeX equations and tables using LLM assistance</li> </ul>"},{"location":"data-processing/docs/#pii-removal","title":"PII Removal","text":"<ul> <li>Automatically masks Names and Emails using the Presidio framework</li> <li>Configurable detection patterns</li> </ul>"},{"location":"data-processing/docs/#metadata-extraction","title":"Metadata Extraction","text":"<ul> <li>Extracts Title, Authors, DOI, URL, Year, Journal, and Citation Count</li> <li>PDF-based extraction using MonkeyOCR integration</li> <li>Support for HTML and other formats</li> </ul>"},{"location":"data-processing/docs/#export","title":"Export","text":"<ul> <li>Saves processed content in multiple formats (default: Markdown)</li> </ul>"},{"location":"data-processing/docs/#quick-start","title":"Quick Start","text":"<ol> <li> <p>Install the packages</p> <p><code>bash uv sync</code></p> </li> <li> <p>Configure the pipeline (<code>config.yaml</code>)</p> <p><code>yaml pipeline:   batch_size: 10   inputs:     path: \"input_dir\"   stages:     - name: extraction       config: { format: \"xml\"}     - name: duplication       config: { method: \"lsh\", shingle_size: 3, num_perm: 128, threshold: 0.8 }     - name: pii       config: { url: \"http://127.0.0.1:8000\" }     - name: export       config: { format: \"md\", destination: \"output/files\"}</code></p> </li> <li> <p>Run the pipeline</p> <p><code>bash eve run</code></p> </li> </ol>"},{"location":"data-processing/docs/#funding","title":"Funding","text":"<p>This project is supported by the European Space Agency (ESA) \u03a6-lab through the Large Language Model for Earth Observation and Earth Science project, as part of the Foresight Element within FutureEO Block 4 programme.</p>"},{"location":"data-processing/docs/#citation","title":"Citation","text":"<p>If you use this project in academic or research settings, please cite:</p>"},{"location":"data-processing/docs/#license","title":"License","text":"<p>This project is released under the Apache 2.0 License - see the LICENSE file for more details.</p>"},{"location":"data-processing/docs/api/core/","title":"Core Components","text":"<p>This section covers the core components of the EVE Pipeline that form the foundation of the data processing framework.</p>"},{"location":"data-processing/docs/api/core/#pipeline","title":"Pipeline","text":"<p>The main pipeline orchestrator that coordinates all processing stages. This is where the files are first batched, converted to document objects, then passed to each pipeline stages.</p> <pre><code>async def pipeline():\n    logger = get_logger(\"pipeline\")\n    cfg = load_config(\"config.yaml\")\n\n    batch_size = cfg.batch_size\n\n    logger.info(\"Starting pipeline execution\")\n\n    input_files = cfg.inputs.get_files()\n\n    logger.info(f\"Processing {len(input_files)} files with batch size {batch_size}\")\n\n    unique_file_formats = {find_format(f) for f in input_files}\n\n    stages_with_extraction_dependency = {\"dedup\", \"cleaning\", \"pii\"}\n\n    if 'md' not in unique_file_formats:\n        user_stage_names = {stage[\"name\"] for stage in cfg.stages}\n        if not any(stage in user_stage_names for stage in stages_with_extraction_dependency):\n            pass\n        else:\n            if \"extraction\" not in user_stage_names:\n                cfg.stages.insert(0, {\"name\": \"extraction\"})\n\n    # enable export by default\n    if not any(stage[\"name\"] == \"export\" for stage in cfg.stages):\n        cfg.stages.append({\"name\": \"export\"})\n\n\n    logger.info(f\"Stages: {[stage['name'] for stage in cfg.stages]}\")\n\n    step_mapping = {\n        \"cleaning\": CleaningStep,\n        \"export\": ExportStep,\n        \"duplication\": DuplicationStep,\n        \"extraction\": ExtractionStep,\n        \"pii\": PiiStep,\n        \"metadata\": MetadataStep,\n    }\n\n    batchable_steps = {\"cleaning\", \"extraction\", \"pii\", \"metadata\", \"export\"}\n\n    has_dedup = any(stage[\"name\"] == \"duplication\" for stage in cfg.stages)\n\n    if has_dedup: \n        logger.info(\"Deduplication detected - collecting all documents before processing\")\n        all_documents = []\n        async for batch in create_batches(input_files, batch_size):\n            batch_docs = batch\n            for stage in cfg.stages:\n                step_name = stage[\"name\"]\n                if step_name == \"duplication\":\n                    break  # stop here, accumulate all docs and run dedup in phase 2\n                if step_name in batchable_steps and step_name in step_mapping:\n                    step_config = stage.get(\"config\", {})\n                    step = step_mapping[step_name](config = step_config)\n                    logger.info(f\"Running step on batch: {step_name}\")\n                    batch_docs = await step(batch_docs)\n\n            all_documents.extend(batch_docs)\n\n        documents = all_documents\n        dedup_started = False\n        for stage in cfg.stages:\n            step_name = stage[\"name\"]\n            if step_name == \"duplication\":\n                dedup_started = True\n\n            if dedup_started:\n                step_config = stage.get(\"config\", {})\n                if step_name in step_mapping:\n                    step = step_mapping[step_name](config = step_config)\n                    logger.info(f\"Running step: {step_name}\")\n                    documents = await step(documents)\n                else:\n                    logger.error(f\"No implementation found for step: {step_name}\")\n    else:\n        logger.info(\"No deduplication - using streaming batch processing\")\n        all_processed = []\n\n        async for batch in create_batches(input_files, batch_size):\n            batch_docs = batch\n            logger.info(f\"Processing batch of {len(batch_docs)} documents\")\n\n            for stage in cfg.stages:\n                step_name = stage[\"name\"]\n                step_config = stage.get(\"config\", {})\n                if step_name in step_mapping:\n                    step = step_mapping[step_name](config = step_config)\n                    logger.info(f\"Running step on batch: {step_name}\")\n                    batch_docs = await step(batch_docs)\n                else:\n                    logger.error(f\"No implementation found for step: {step_name}\")\n\n            all_processed.extend(batch_docs)\n\n        documents = all_processed\n</code></pre>"},{"location":"data-processing/docs/api/core/#configuration","title":"Configuration","text":"<p>Configuration management for the pipeline using Pydantic models. These objects provide type-safe settings validation and management for all pipeline components.</p> <pre><code>class PipelineConfig(BaseModel):\n    batch_size: int = 20\n    inputs: Inputs\n    stages: list[dict[str, Any]]\n\n    @validator(\"stages\")\n    def check_stages(cls, v):\n        allowed = {\"ingestion\", \"cleaning\", \"export\", \"duplication\", \"extraction\", \"pii\", \"metadata\"}\n        for stage in v:\n            if stage[\"name\"] not in allowed:\n                raise ValueError(f\"Unsupported stage: {stage['name']}. Allowed: {allowed}\")\n        return v\n</code></pre>"},{"location":"data-processing/docs/api/core/#document-model","title":"Document Model","text":"<p>The unified document object that represents content and metadata throughout the pipeline. Documents are the core data structure that flow through the pipeline, containing both content and associated metadata.</p> <pre><code>class Document:\n    content: str\n    file_path: Path\n    file_format: str\n    metadata: Dict[str, Any] = field(default_factory=dict)\n\n    def __hash__(self):\n        return hash(self.file_path)\n\n    def __eq__(self, other):\n        return isinstance(other, Document) and self.file_path == other.file_path\n\n    @property\n    def filename(self) -&gt; str:\n        \"\"\"Get the filename without path.\"\"\"\n        return self.file_path.name\n\n    @property\n    def extension(self) -&gt; str:\n        \"\"\"Get the file extension.\"\"\"\n        return self.file_path.suffix.lstrip('.')\n\n    @property\n    def content_length(self) -&gt; int:\n        \"\"\"Get the length of the content.\"\"\"\n        return len(self.content)\n\n    def is_empty(self) -&gt; bool:\n        \"\"\"Check if the document content is empty.\"\"\"\n        return not self.content.strip()\n\n    def add_metadata(self, key: str, value: Any) -&gt; None:\n        \"\"\"Add a metadata entry.\"\"\"\n        self.metadata[key] = value\n\n    def get_metadata(self, key: str, default: Any = None) -&gt; Any:\n        \"\"\"Get a metadata value with optional default.\"\"\"\n        return self.metadata.get(key, default)\n\n    def update_content(self, new_content: str) -&gt; None:\n        \"\"\"Update the document content and track the change in metadata.\"\"\"\n        old_length = self.content_length\n        self.content = new_content\n        new_length = self.content_length\n\n        # Track content changes in metadata\n        changes = self.metadata.get('content_changes', [])\n        changes.append({\n            'old_length': old_length,\n            'new_length': new_length,\n            'size_change': new_length - old_length\n        })\n        self.metadata['content_changes'] = changes\n\n    @classmethod\n    def from_path_and_content(cls, file_path: Path, content: str, **metadata) -&gt; 'Document':\n        \"\"\"Create a Document from a file path and content string.\"\"\"\n        return cls(\n            content=content,\n            file_path=file_path,\n            metadata=metadata\n        )\n\n    @classmethod\n    def from_tuple(cls, path_content_tuple: tuple[Path, str], **metadata) -&gt; 'Document':\n        \"\"\"Create a Document from a (Path, str) tuple for backwards compatibility.\"\"\"\n        file_path, content = path_content_tuple\n        return cls.from_path_and_content(file_path, content, **metadata)\n\n    def to_tuple(self) -&gt; tuple[Path, str]:\n        \"\"\"Convert to (Path, str) tuple for backwards compatibility.\"\"\"\n        return (self.file_path, self.content)\n\n    def __str__(self) -&gt; str:\n        \"\"\"String representation showing filename and content length.\"\"\"\n        return f\"Document({self.filename}, {self.file_format} format)\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Detailed representation.\"\"\"\n        return f\"Document(file_path={self.file_path}, format={self.file_format}, metadata_keys={list(self.metadata.keys())})\"\n</code></pre>"},{"location":"data-processing/docs/api/core/#pipeline-step-base","title":"Pipeline Step Base","text":"<p>Abstract base class that all pipeline stages must implement. All custom pipeline components should inherit from PipelineStep to ensure proper integration with the framework.</p> <pre><code>class PipelineStep(ABC):\n    \"\"\"abstract base class for all pipeline steps.\"\"\"\n\n    def __init__(self, config: Any, name: Optional[str] = None):\n        \"\"\"initialize the pipeline step.\n\n        Args:\n            config: Configuration specific to the step.\n            name: Optional name for the step (used for logging).\n        \"\"\"\n        self.config = config\n        self.debug = config.get(\"debug\", False) if isinstance(config, dict) else False\n        self.logger = get_logger(name or self.__class__.__name__)\n\n    @abstractmethod\n    async def execute(self, input_data: Any) -&gt; Any: # TBD\n        \"\"\"Execute the pipeline step.\n\n        Args:\n            input_data: Input data to process.\n\n        Returns:\n            Processed data or result of the step.\n        \"\"\"\n        pass\n\n    async def __call__(self, input_data: Any) -&gt; Any:\n        \"\"\"shortway of calling `execute` method.\n\n        Args:\n            input_data: Input data to process.\n\n        Returns:\n            Processed data or result of the step.\n        \"\"\"\n        return await self.execute(input_data)\n\n</code></pre>"},{"location":"data-processing/docs/api/models/","title":"Data Models","text":"<p>This section documents the data models and structures used throughout the EVE Pipeline.</p>"},{"location":"data-processing/docs/api/models/#document-model","title":"Document Model","text":"<p>The primary data structure representing documents in the pipeline.</p> <p>Unified document object that encapsulates content and metadata throughout the pipeline.</p> <p>This replaces the need to pass (Path, str) tuples and provides a consistent interface for document handling across all pipeline stages.</p> <p>Attributes:</p> Name Type Description <code>content</code> <code>str</code> <p>The actual document text content</p> <code>file_path</code> <code>Path</code> <p>Path to the source file</p> <code>file_format</code> <code>str</code> <p>Format of the source file (pdf, md, html, etc.)</p> <code>metadata</code> <code>Dict[str, Any]</code> <p>Original metadata from the document (preserved from source)</p> <code>embedding</code> <code>Optional[List[float]]</code> <p>Optional embedding vector for the document</p> <code>pipeline_metadata</code> <code>Dict[str, Any]</code> <p>Metadata added by pipeline steps (filters, processing, etc.)</p> Source code in <code>data-processing/eve/model/document.py</code> <pre><code>@dataclass\nclass Document:\n    \"\"\"\n    Unified document object that encapsulates content and metadata throughout the pipeline.\n\n    This replaces the need to pass (Path, str) tuples and provides a consistent\n    interface for document handling across all pipeline stages.\n\n    Attributes:\n        content: The actual document text content\n        file_path: Path to the source file\n        file_format: Format of the source file (pdf, md, html, etc.)\n        metadata: Original metadata from the document (preserved from source)\n        embedding: Optional embedding vector for the document\n        pipeline_metadata: Metadata added by pipeline steps (filters, processing, etc.)\n    \"\"\"\n\n    content: str\n    file_path: Path\n    file_format: str\n    metadata: Dict[str, Any] = field(default_factory=dict)\n    embedding: Optional[List[float]] = None\n    pipeline_metadata: Dict[str, Any] = field(default_factory=dict)\n\n    def __dict__(self) -&gt; Dict[str, Any]:\n        result = {\n            \"content\": self.content,\n            \"file_path\": str(self.file_path),\n            \"file_format\": self.file_format,\n            \"metadata\": self.metadata.copy(),\n            \"pipeline_metadata\": self.pipeline_metadata.copy(),\n        }\n        if self.embedding is not None:\n            result[\"embedding\"] = self.embedding\n        return result\n\n    def __hash__(self):\n        return hash(self.file_path)\n\n    def __eq__(self, other):\n        return isinstance(other, Document) and self.file_path == other.file_path\n\n    @property\n    def filename(self) -&gt; str:\n        \"\"\"Get the filename without path.\"\"\"\n        return self.file_path.name\n\n    @property\n    def extension(self) -&gt; str:\n        \"\"\"Get the file extension.\"\"\"\n        return self.file_path.suffix.lstrip(\".\")\n\n    @property\n    def content_length(self) -&gt; int:\n        \"\"\"Get the length of the content.\"\"\"\n        return len(self.content)\n\n    def is_empty(self) -&gt; bool:\n        \"\"\"Check if the document content is empty.\"\"\"\n        return not self.content.strip()\n\n    def add_metadata(self, key: str, value: Any) -&gt; None:\n        \"\"\"Add an entry to the original metadata.\"\"\"\n        self.metadata[key] = value\n\n    def get_metadata(self, key: str, default: Any = None) -&gt; Any:\n        \"\"\"Get a value from the original metadata with optional default.\"\"\"\n        return self.metadata.get(key, default)\n\n    def add_pipeline_metadata(self, key: str, value: Any) -&gt; None:\n        \"\"\"Add an entry to pipeline metadata (for tracking pipeline processing).\"\"\"\n        self.pipeline_metadata[key] = value\n\n    def get_pipeline_metadata(self, key: str, default: Any = None) -&gt; Any:\n        \"\"\"Get a value from pipeline metadata with optional default.\"\"\"\n        return self.pipeline_metadata.get(key, default)\n\n    def update_content(self, new_content: str) -&gt; None:\n        \"\"\"Update the document content and track the change in metadata.\"\"\"\n        old_length = self.content_length\n        self.content = new_content\n        new_length = self.content_length\n\n        # Track content changes in metadata\n        changes = self.metadata.get(\"content_changes\", [])\n        changes.append(\n            {\n                \"old_length\": old_length,\n                \"new_length\": new_length,\n                \"size_change\": new_length - old_length,\n            }\n        )\n        self.metadata[\"content_changes\"] = changes\n\n    @classmethod\n    def from_path_and_content(\n        cls, file_path: Path, content: str, **metadata\n    ) -&gt; \"Document\":\n        \"\"\"Create a Document from a file path and content string.\"\"\"\n        return cls(content=content, file_path=file_path, metadata=metadata)\n\n    @classmethod\n    def from_tuple(cls, path_content_tuple: tuple[Path, str], **metadata) -&gt; \"Document\":\n        \"\"\"Create a Document from a (Path, str) tuple for backwards compatibility.\"\"\"\n        file_path, content = path_content_tuple\n        return cls.from_path_and_content(file_path, content, **metadata)\n\n    def to_tuple(self) -&gt; tuple[Path, str]:\n        \"\"\"Convert to (Path, str) tuple for backwards compatibility.\"\"\"\n        return (self.file_path, self.content)\n\n    def __str__(self) -&gt; str:\n        \"\"\"String representation showing filename and content length.\"\"\"\n        return f\"Document({self.filename}, {self.file_format} format)\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Detailed representation.\"\"\"\n        return f\"Document(file_path={self.file_path}, format={self.file_format}, metadata_keys={list(self.metadata.keys())})\"\n</code></pre>"},{"location":"data-processing/docs/api/models/#eve.model.document.Document.content_length","title":"<code>content_length</code>  <code>property</code>","text":"<p>Get the length of the content.</p>"},{"location":"data-processing/docs/api/models/#eve.model.document.Document.extension","title":"<code>extension</code>  <code>property</code>","text":"<p>Get the file extension.</p>"},{"location":"data-processing/docs/api/models/#eve.model.document.Document.filename","title":"<code>filename</code>  <code>property</code>","text":"<p>Get the filename without path.</p>"},{"location":"data-processing/docs/api/models/#eve.model.document.Document.__repr__","title":"<code>__repr__()</code>","text":"<p>Detailed representation.</p> Source code in <code>data-processing/eve/model/document.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Detailed representation.\"\"\"\n    return f\"Document(file_path={self.file_path}, format={self.file_format}, metadata_keys={list(self.metadata.keys())})\"\n</code></pre>"},{"location":"data-processing/docs/api/models/#eve.model.document.Document.__str__","title":"<code>__str__()</code>","text":"<p>String representation showing filename and content length.</p> Source code in <code>data-processing/eve/model/document.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"String representation showing filename and content length.\"\"\"\n    return f\"Document({self.filename}, {self.file_format} format)\"\n</code></pre>"},{"location":"data-processing/docs/api/models/#eve.model.document.Document.add_metadata","title":"<code>add_metadata(key, value)</code>","text":"<p>Add an entry to the original metadata.</p> Source code in <code>data-processing/eve/model/document.py</code> <pre><code>def add_metadata(self, key: str, value: Any) -&gt; None:\n    \"\"\"Add an entry to the original metadata.\"\"\"\n    self.metadata[key] = value\n</code></pre>"},{"location":"data-processing/docs/api/models/#eve.model.document.Document.add_pipeline_metadata","title":"<code>add_pipeline_metadata(key, value)</code>","text":"<p>Add an entry to pipeline metadata (for tracking pipeline processing).</p> Source code in <code>data-processing/eve/model/document.py</code> <pre><code>def add_pipeline_metadata(self, key: str, value: Any) -&gt; None:\n    \"\"\"Add an entry to pipeline metadata (for tracking pipeline processing).\"\"\"\n    self.pipeline_metadata[key] = value\n</code></pre>"},{"location":"data-processing/docs/api/models/#eve.model.document.Document.from_path_and_content","title":"<code>from_path_and_content(file_path, content, **metadata)</code>  <code>classmethod</code>","text":"<p>Create a Document from a file path and content string.</p> Source code in <code>data-processing/eve/model/document.py</code> <pre><code>@classmethod\ndef from_path_and_content(\n    cls, file_path: Path, content: str, **metadata\n) -&gt; \"Document\":\n    \"\"\"Create a Document from a file path and content string.\"\"\"\n    return cls(content=content, file_path=file_path, metadata=metadata)\n</code></pre>"},{"location":"data-processing/docs/api/models/#eve.model.document.Document.from_tuple","title":"<code>from_tuple(path_content_tuple, **metadata)</code>  <code>classmethod</code>","text":"<p>Create a Document from a (Path, str) tuple for backwards compatibility.</p> Source code in <code>data-processing/eve/model/document.py</code> <pre><code>@classmethod\ndef from_tuple(cls, path_content_tuple: tuple[Path, str], **metadata) -&gt; \"Document\":\n    \"\"\"Create a Document from a (Path, str) tuple for backwards compatibility.\"\"\"\n    file_path, content = path_content_tuple\n    return cls.from_path_and_content(file_path, content, **metadata)\n</code></pre>"},{"location":"data-processing/docs/api/models/#eve.model.document.Document.get_metadata","title":"<code>get_metadata(key, default=None)</code>","text":"<p>Get a value from the original metadata with optional default.</p> Source code in <code>data-processing/eve/model/document.py</code> <pre><code>def get_metadata(self, key: str, default: Any = None) -&gt; Any:\n    \"\"\"Get a value from the original metadata with optional default.\"\"\"\n    return self.metadata.get(key, default)\n</code></pre>"},{"location":"data-processing/docs/api/models/#eve.model.document.Document.get_pipeline_metadata","title":"<code>get_pipeline_metadata(key, default=None)</code>","text":"<p>Get a value from pipeline metadata with optional default.</p> Source code in <code>data-processing/eve/model/document.py</code> <pre><code>def get_pipeline_metadata(self, key: str, default: Any = None) -&gt; Any:\n    \"\"\"Get a value from pipeline metadata with optional default.\"\"\"\n    return self.pipeline_metadata.get(key, default)\n</code></pre>"},{"location":"data-processing/docs/api/models/#eve.model.document.Document.is_empty","title":"<code>is_empty()</code>","text":"<p>Check if the document content is empty.</p> Source code in <code>data-processing/eve/model/document.py</code> <pre><code>def is_empty(self) -&gt; bool:\n    \"\"\"Check if the document content is empty.\"\"\"\n    return not self.content.strip()\n</code></pre>"},{"location":"data-processing/docs/api/models/#eve.model.document.Document.to_tuple","title":"<code>to_tuple()</code>","text":"<p>Convert to (Path, str) tuple for backwards compatibility.</p> Source code in <code>data-processing/eve/model/document.py</code> <pre><code>def to_tuple(self) -&gt; tuple[Path, str]:\n    \"\"\"Convert to (Path, str) tuple for backwards compatibility.\"\"\"\n    return (self.file_path, self.content)\n</code></pre>"},{"location":"data-processing/docs/api/models/#eve.model.document.Document.update_content","title":"<code>update_content(new_content)</code>","text":"<p>Update the document content and track the change in metadata.</p> Source code in <code>data-processing/eve/model/document.py</code> <pre><code>def update_content(self, new_content: str) -&gt; None:\n    \"\"\"Update the document content and track the change in metadata.\"\"\"\n    old_length = self.content_length\n    self.content = new_content\n    new_length = self.content_length\n\n    # Track content changes in metadata\n    changes = self.metadata.get(\"content_changes\", [])\n    changes.append(\n        {\n            \"old_length\": old_length,\n            \"new_length\": new_length,\n            \"size_change\": new_length - old_length,\n        }\n    )\n    self.metadata[\"content_changes\"] = changes\n</code></pre>"},{"location":"data-processing/docs/api/models/#configuration-models","title":"Configuration Models","text":"<p>Data models for pipeline configuration.</p>"},{"location":"data-processing/docs/api/models/#inputs-configuration","title":"Inputs Configuration","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>data-processing/eve/config.py</code> <pre><code>class Inputs(BaseModel):\n    mode: str = \"file\"  # file | directory\n    path: Union[str, list[str]]\n\n    def get_files(self) -&gt; list[Path]:\n        paths = [self.path] if isinstance(self.path, str) else self.path\n        files = []\n\n        for p in paths:\n            p = Path(p)\n\n            if p.is_file():\n                files.append(p)\n            elif p.is_dir():\n                files.extend([f for f in p.rglob(\"*\") if f.is_file()]) # recursive search across multiple levels\n        return files\n</code></pre>"},{"location":"data-processing/docs/api/models/#pipeline-configuration","title":"Pipeline Configuration","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>data-processing/eve/config.py</code> <pre><code>class PipelineConfig(BaseModel):\n    batch_size: int = 20\n    inputs: Inputs\n    stages: list[dict[str, Any]]  # list of dict since we have stage name + stage configs\n\n    @validator(\"stages\")\n    def check_stages(cls, v):\n        allowed = {\"ingestion\", \"cleaning\", \"export\", \"duplication\", \"extraction\", \"pii\", \"metadata\", \"chunker\", \"export_jsonl\", \"perplexity\", \"pii_filter\", \"length_filter\", \"newline_filter\", \"reference_filter\", \"qdrant_upload\"}\n        for stage in v:\n            if stage[\"name\"] not in allowed:\n                raise ValueError(f\"Unsupported stage: {stage['name']}. Allowed: {allowed}\")\n        return v\n</code></pre>"},{"location":"data-processing/docs/api/stages/","title":"Pipeline Stages","text":"<p>This section documents all the available pipeline stages for processing documents.</p>"},{"location":"data-processing/docs/api/stages/#extraction-stage","title":"Extraction Stage","text":"<p>Extracts content from various document formats.</p>"},{"location":"data-processing/docs/api/stages/#eve.steps.extraction.extract_step.ExtractionStep","title":"<code>ExtractionStep</code>","text":"<p>               Bases: <code>PipelineStep</code></p> Source code in <code>data-processing/eve/steps/extraction/extract_step.py</code> <pre><code>class ExtractionStep(PipelineStep):\n    async def _html_extraction(self, document: Document) -&gt; Document:\n        html_extractor = HtmlExtractor(document)\n        text = await html_extractor.extract_text()\n        return text\n\n    async def _pdf_extraction(self, document: Document, url: str) -&gt; Document:\n        pdf_extractor = PdfExtractor(document, url)\n        text = await pdf_extractor.extract_text()\n        return text\n\n    async def _xml_extraction(self, document: Document) -&gt; Document:\n        xml_extractor = XmlExtractor(document)\n        text = await xml_extractor.extract_text()\n        return text\n\n    async def _md_extraction(self, document: Document) -&gt; Document:\n        md_extractor = MarkdownExtractor(document)\n        text = await md_extractor.extract_text()\n        return text\n\n    async def _jsonl_extraction(self, document: Document) -&gt; List[Document]:\n        jsonl_extractor = JSONLExtractor(document)\n        documents = await jsonl_extractor.extract_documents()\n        return documents\n\n    async def execute(self, documents: List[Document]) -&gt; List[Document]:\n        \"\"\"Execute text extraction on input files or documents.\n\n        Args:\n            input_data: List of file paths or Document objects to extract text from\n\n        Returns:\n            List of Document objects with extracted text\n        \"\"\"\n        format = self.config.get(\n            \"format\", None\n        )  # write a wrapper to find out the extension\n        if not format:\n            unique_formats = set()\n\n        unique_formats = {document.file_format for document in documents}\n        text_extraction_formats = [\"html\", \"xml\", \"pdf\", \"md\"]\n        supported_formats = [\"jsonl\"]\n\n        self.logger.info(\n            f\"Extracting text from {unique_formats} files. File count: {len(documents)}\"\n        )\n\n        result = []\n        for document in documents:\n            try:\n                # Skip documents that already have content (already extracted in create_batches)\n                # This happens for JSONL files that are pre-loaded\n                if document.content and document.file_format == \"md\":\n                    result.append(document)\n                    self.logger.debug(f\"Skipping already-loaded document: {document.filename}\")\n                    continue\n\n                if document.file_format in text_extraction_formats:\n                    if document.file_format == \"html\":\n                        document_with_text = await self._html_extraction(document)\n                    elif document.file_format == \"pdf\":\n                        url = self.config.get(\"url\", None)\n                        if not url:\n                            self.logger.error(\n                                \"No URL provided for PDF extraction service\"\n                            )\n                        document_with_text = await self._pdf_extraction(document, url)\n                    elif document.file_format == \"xml\":\n                        document_with_text = await self._xml_extraction(document)\n                    elif document.file_format == \"md\":\n                        document_with_text = await self._md_extraction(document)\n                    else:\n                        self.logger.error(f\"Unsupported format: {document.file_format}\")\n                        continue\n\n                    if (\n                        document_with_text\n                        and hasattr(document_with_text, \"content_length\")\n                        and document_with_text.content_length &gt; 1\n                    ):\n                        result.append(document_with_text)\n                        self.logger.info(\n                            f\"Successfully extracted {document_with_text.content_length} characters from {document_with_text.filename}\"\n                        )\n                elif document.file_format in supported_formats:\n                    docs = await self._jsonl_extraction(document)\n                    docs = docs or []\n                    result.extend(docs)\n                    self.logger.info(\n                        f\"Successfully extracted {len(docs)} documents from {document.filename}\"\n                    )\n                else:\n                    self.logger.warning(f\"No text extracted from {document.filename}\")\n            except Exception as e:\n                self.logger.error(\n                    f\"Failed to extract text from {document.filename}: {str(e)}\"\n                )\n                continue\n        return result\n</code></pre>"},{"location":"data-processing/docs/api/stages/#eve.steps.extraction.extract_step.ExtractionStep.execute","title":"<code>execute(documents)</code>  <code>async</code>","text":"<p>Execute text extraction on input files or documents.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <p>List of file paths or Document objects to extract text from</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>List of Document objects with extracted text</p> Source code in <code>data-processing/eve/steps/extraction/extract_step.py</code> <pre><code>async def execute(self, documents: List[Document]) -&gt; List[Document]:\n    \"\"\"Execute text extraction on input files or documents.\n\n    Args:\n        input_data: List of file paths or Document objects to extract text from\n\n    Returns:\n        List of Document objects with extracted text\n    \"\"\"\n    format = self.config.get(\n        \"format\", None\n    )  # write a wrapper to find out the extension\n    if not format:\n        unique_formats = set()\n\n    unique_formats = {document.file_format for document in documents}\n    text_extraction_formats = [\"html\", \"xml\", \"pdf\", \"md\"]\n    supported_formats = [\"jsonl\"]\n\n    self.logger.info(\n        f\"Extracting text from {unique_formats} files. File count: {len(documents)}\"\n    )\n\n    result = []\n    for document in documents:\n        try:\n            # Skip documents that already have content (already extracted in create_batches)\n            # This happens for JSONL files that are pre-loaded\n            if document.content and document.file_format == \"md\":\n                result.append(document)\n                self.logger.debug(f\"Skipping already-loaded document: {document.filename}\")\n                continue\n\n            if document.file_format in text_extraction_formats:\n                if document.file_format == \"html\":\n                    document_with_text = await self._html_extraction(document)\n                elif document.file_format == \"pdf\":\n                    url = self.config.get(\"url\", None)\n                    if not url:\n                        self.logger.error(\n                            \"No URL provided for PDF extraction service\"\n                        )\n                    document_with_text = await self._pdf_extraction(document, url)\n                elif document.file_format == \"xml\":\n                    document_with_text = await self._xml_extraction(document)\n                elif document.file_format == \"md\":\n                    document_with_text = await self._md_extraction(document)\n                else:\n                    self.logger.error(f\"Unsupported format: {document.file_format}\")\n                    continue\n\n                if (\n                    document_with_text\n                    and hasattr(document_with_text, \"content_length\")\n                    and document_with_text.content_length &gt; 1\n                ):\n                    result.append(document_with_text)\n                    self.logger.info(\n                        f\"Successfully extracted {document_with_text.content_length} characters from {document_with_text.filename}\"\n                    )\n            elif document.file_format in supported_formats:\n                docs = await self._jsonl_extraction(document)\n                docs = docs or []\n                result.extend(docs)\n                self.logger.info(\n                    f\"Successfully extracted {len(docs)} documents from {document.filename}\"\n                )\n            else:\n                self.logger.warning(f\"No text extracted from {document.filename}\")\n        except Exception as e:\n            self.logger.error(\n                f\"Failed to extract text from {document.filename}: {str(e)}\"\n            )\n            continue\n    return result\n</code></pre>"},{"location":"data-processing/docs/api/stages/#extractors","title":"Extractors","text":""},{"location":"data-processing/docs/api/stages/#pdf-extractor","title":"PDF Extractor","text":""},{"location":"data-processing/docs/api/stages/#eve.steps.extraction.pdfs.PdfExtractor","title":"<code>PdfExtractor</code>","text":"Source code in <code>data-processing/eve/steps/extraction/pdfs.py</code> <pre><code>class PdfExtractor:\n    def __init__(self, document: Document, endpoint: str):\n        self.document = document\n        self.endpoint = f\"{endpoint}/predict\"\n        self.extraction = None\n\n    async def _call_nougat(self, session: aiohttp.ClientSession) -&gt; Optional[str]:\n        \"\"\"internal method to call the Nougat API.\"\"\"\n        try:\n            file_content = await read_file(self.document.file_path, 'rb')\n            if not file_content:\n                logger.error(f\"Failed to read file: {self.file_path}\")\n                return None\n\n            data = aiohttp.FormData()\n            data.add_field('file', file_content, filename = self.document.filename, content_type = 'application/pdf')\n\n            async with session.post(self.endpoint, data = data) as response:\n                if response.status == 200:\n                    return await response.text()\n                else:\n                    logger.error(f\"Nougat API request for {self.document.file_path} failed with status {response.status}\")\n                    return None\n        except Exception as e:\n            logger.error(f\"Failed to process {self.document.file_path}: {str(e)}\")\n            return None\n\n    async def extract_text(self) -&gt; Optional[Document]:\n        \"\"\"Extract text from a single PDF file.\n\n        Returns:\n            Document object with extracted text if successful, None otherwise\n        \"\"\"\n        try:\n            async with aiohttp.ClientSession() as session:\n                content = await self._call_nougat(session)\n                if not content:\n                    logger.error(f\"Failed to extract content from {self.document.file_path}\")\n                    return None\n                self.document.content = content\n                return self.document\n        except Exception as e:\n            logger.error(f\"Error in PDF extraction for {self.document.file_path}: {str(e)}\")\n            return None\n</code></pre>"},{"location":"data-processing/docs/api/stages/#eve.steps.extraction.pdfs.PdfExtractor.extract_text","title":"<code>extract_text()</code>  <code>async</code>","text":"<p>Extract text from a single PDF file.</p> <p>Returns:</p> Type Description <code>Optional[Document]</code> <p>Document object with extracted text if successful, None otherwise</p> Source code in <code>data-processing/eve/steps/extraction/pdfs.py</code> <pre><code>async def extract_text(self) -&gt; Optional[Document]:\n    \"\"\"Extract text from a single PDF file.\n\n    Returns:\n        Document object with extracted text if successful, None otherwise\n    \"\"\"\n    try:\n        async with aiohttp.ClientSession() as session:\n            content = await self._call_nougat(session)\n            if not content:\n                logger.error(f\"Failed to extract content from {self.document.file_path}\")\n                return None\n            self.document.content = content\n            return self.document\n    except Exception as e:\n        logger.error(f\"Error in PDF extraction for {self.document.file_path}: {str(e)}\")\n        return None\n</code></pre>"},{"location":"data-processing/docs/api/stages/#html-extractor","title":"HTML Extractor","text":""},{"location":"data-processing/docs/api/stages/#eve.steps.extraction.htmls.HtmlExtractor","title":"<code>HtmlExtractor</code>","text":"Source code in <code>data-processing/eve/steps/extraction/htmls.py</code> <pre><code>class HtmlExtractor:\n    def __init__(self, document: Document):\n        self.document = document\n\n    async def extract_text(self) -&gt; Optional[Document]:\n        \"\"\"Extract text from a single HTML file.\n\n        Returns:\n            Document object with extracted text if successful, None otherwise\n        \"\"\"\n        try:\n            content = await read_file(self.document.file_path, 'r')\n            if not content:\n                logger.error(f\"Failed to read file: {self.document.file_path}\")\n                return None\n\n            def parse_html():\n                return extract(content, include_comments = False, include_tables = True)\n\n            self.document.content = await asyncio.to_thread(parse_html)\n            return self.document\n        except Exception as e:\n            logger.error(f\"Error processing HTML file {self.document.file_path}: {e}\")\n            return None\n</code></pre>"},{"location":"data-processing/docs/api/stages/#eve.steps.extraction.htmls.HtmlExtractor.extract_text","title":"<code>extract_text()</code>  <code>async</code>","text":"<p>Extract text from a single HTML file.</p> <p>Returns:</p> Type Description <code>Optional[Document]</code> <p>Document object with extracted text if successful, None otherwise</p> Source code in <code>data-processing/eve/steps/extraction/htmls.py</code> <pre><code>async def extract_text(self) -&gt; Optional[Document]:\n    \"\"\"Extract text from a single HTML file.\n\n    Returns:\n        Document object with extracted text if successful, None otherwise\n    \"\"\"\n    try:\n        content = await read_file(self.document.file_path, 'r')\n        if not content:\n            logger.error(f\"Failed to read file: {self.document.file_path}\")\n            return None\n\n        def parse_html():\n            return extract(content, include_comments = False, include_tables = True)\n\n        self.document.content = await asyncio.to_thread(parse_html)\n        return self.document\n    except Exception as e:\n        logger.error(f\"Error processing HTML file {self.document.file_path}: {e}\")\n        return None\n</code></pre>"},{"location":"data-processing/docs/api/stages/#xml-extractor","title":"XML Extractor","text":""},{"location":"data-processing/docs/api/stages/#eve.steps.extraction.xmls.XmlExtractor","title":"<code>XmlExtractor</code>","text":"Source code in <code>data-processing/eve/steps/extraction/xmls.py</code> <pre><code>class XmlExtractor:\n    def __init__(self, document: Document):\n        self.document = document\n\n    async def extract_text(self) -&gt; Optional[Document]:\n        \"\"\"Extract text from a single XML file.\n\n        Returns:\n            Document object with extracted text if successful, None otherwise\n        \"\"\"\n        try:\n            content = await read_file(self.document.file_path, 'r')\n            if not content:\n                logger.error(f\"Failed to read file: {self.document.file_path}\")\n                return None\n\n            def parse_and_extract():\n                root = ET.fromstring(content)\n\n                def extract_text_from_tree(element):\n                    texts = []\n                    if element.text:\n                        texts.append(element.text)\n                    for child in element:\n                        texts.extend(extract_text_from_tree(child))\n                    if element.tail:\n                        texts.append(element.tail)\n                    return texts\n\n                extracted_texts = extract_text_from_tree(root)\n                full_text = ''.join(extracted_texts)\n                cleaned_text = re.sub(r'\\n{3,}', '\\n\\n', full_text)\n                return cleaned_text.strip()\n\n            self.document.content = await asyncio.to_thread(parse_and_extract)\n            return self.document\n        except Exception as e:\n            logger.error(f\"Error processing XML file {self.document.file_path}: {e}\")\n            return None\n</code></pre>"},{"location":"data-processing/docs/api/stages/#eve.steps.extraction.xmls.XmlExtractor.extract_text","title":"<code>extract_text()</code>  <code>async</code>","text":"<p>Extract text from a single XML file.</p> <p>Returns:</p> Type Description <code>Optional[Document]</code> <p>Document object with extracted text if successful, None otherwise</p> Source code in <code>data-processing/eve/steps/extraction/xmls.py</code> <pre><code>async def extract_text(self) -&gt; Optional[Document]:\n    \"\"\"Extract text from a single XML file.\n\n    Returns:\n        Document object with extracted text if successful, None otherwise\n    \"\"\"\n    try:\n        content = await read_file(self.document.file_path, 'r')\n        if not content:\n            logger.error(f\"Failed to read file: {self.document.file_path}\")\n            return None\n\n        def parse_and_extract():\n            root = ET.fromstring(content)\n\n            def extract_text_from_tree(element):\n                texts = []\n                if element.text:\n                    texts.append(element.text)\n                for child in element:\n                    texts.extend(extract_text_from_tree(child))\n                if element.tail:\n                    texts.append(element.tail)\n                return texts\n\n            extracted_texts = extract_text_from_tree(root)\n            full_text = ''.join(extracted_texts)\n            cleaned_text = re.sub(r'\\n{3,}', '\\n\\n', full_text)\n            return cleaned_text.strip()\n\n        self.document.content = await asyncio.to_thread(parse_and_extract)\n        return self.document\n    except Exception as e:\n        logger.error(f\"Error processing XML file {self.document.file_path}: {e}\")\n        return None\n</code></pre>"},{"location":"data-processing/docs/api/stages/#markdown-extractor","title":"Markdown Extractor","text":""},{"location":"data-processing/docs/api/stages/#eve.steps.extraction.markdown.MarkdownExtractor","title":"<code>MarkdownExtractor</code>","text":"Source code in <code>data-processing/eve/steps/extraction/markdown.py</code> <pre><code>class MarkdownExtractor:\n    def __init__(self, document: Document):\n        self.document = document\n\n    async def extract_text(self) -&gt; Optional[Document]:\n        \"\"\"Extract text from a single markdown file.\n\n        Returns:\n            Document object with extracted text if successful, None otherwise\n        \"\"\"\n        try:\n            content = await read_file(self.document.file_path, 'r')\n            if not content:\n                logger.error(f\"Failed to read file: {self.document.file_path}\")\n                return None\n\n            self.document.content = content\n            return self.document\n        except Exception as e:\n            logger.error(f\"Error processing HTML file {self.document.file_path}: {e}\")\n            return None\n</code></pre>"},{"location":"data-processing/docs/api/stages/#eve.steps.extraction.markdown.MarkdownExtractor.extract_text","title":"<code>extract_text()</code>  <code>async</code>","text":"<p>Extract text from a single markdown file.</p> <p>Returns:</p> Type Description <code>Optional[Document]</code> <p>Document object with extracted text if successful, None otherwise</p> Source code in <code>data-processing/eve/steps/extraction/markdown.py</code> <pre><code>async def extract_text(self) -&gt; Optional[Document]:\n    \"\"\"Extract text from a single markdown file.\n\n    Returns:\n        Document object with extracted text if successful, None otherwise\n    \"\"\"\n    try:\n        content = await read_file(self.document.file_path, 'r')\n        if not content:\n            logger.error(f\"Failed to read file: {self.document.file_path}\")\n            return None\n\n        self.document.content = content\n        return self.document\n    except Exception as e:\n        logger.error(f\"Error processing HTML file {self.document.file_path}: {e}\")\n        return None\n</code></pre>"},{"location":"data-processing/docs/api/stages/#deduplication-stage","title":"Deduplication Stage","text":"<p>Removes duplicate and near-duplicate documents.</p>"},{"location":"data-processing/docs/api/stages/#eve.steps.dedup.dedup_step.DuplicationStep","title":"<code>DuplicationStep</code>","text":"<p>               Bases: <code>PipelineStep</code></p> Source code in <code>data-processing/eve/steps/dedup/dedup_step.py</code> <pre><code>class DuplicationStep(PipelineStep):\n    async def _exact_deduplication(self, documents: List[Document]) -&gt; List[Document]:\n        finder = ExactDuplication(documents)\n        duplicates = await finder.find_duplicates()\n        return duplicates\n\n    async def _lsh_deduplication(self, documents: List[Document]) -&gt; List[Document]:\n        shingle_size = self.config.get(\"shingle_size\", 3)\n        num_perm = self.config.get(\"num_perm\", 128)\n        threshold = self.config.get(\"threshold\", 0.8)\n        lsh = LSH(documents, shingle_size, num_perm, threshold)\n        duplicates = lsh.find_duplicates() \n        return duplicates\n\n    async def execute(self, documents: List[Document]) -&gt; List[Document]:\n        \"\"\"Execute deduplication on input files or documents.\n\n        Args:\n            input_data: List of file paths or Document objects to deduplicate\n\n        Returns:\n            List of Document objects with duplicates removed\n        \"\"\"\n        method = self.config.get(\"method\", \"exact\")  # default to exact\n\n        self.logger.info(f\"Executing duplication step with method: {method} file count: {len(documents)}\")\n\n        if method == \"exact\":\n            duplicates = await self._exact_deduplication(documents)\n        elif method == \"lsh\":\n            duplicates = await self._lsh_deduplication(documents)\n        else:\n            self.logger.error(f\"Invalid deduplication method: {method}\")\n            raise ValueError(f\"Invalid deduplication method: {method}\")\n\n        # Remove duplicates from documents\n        duplicate_docs = set()\n        duplicates_removed = 0\n        for group in duplicates:\n            # Keep the first doc in each group, mark the rest as duplicates\n            for doc in group[1:]:\n                duplicate_docs.add(doc)\n                duplicates_removed += 1\n\n        # Filter out duplicates, keeping the first occurrence\n        result_documents = []\n        for doc in documents:\n            if doc not in duplicate_docs:\n                result_documents.append(doc)\n\n        self.logger.info(\n            f\"Deduplication complete: {len(result_documents)} files remaining, {duplicates_removed} duplicates removed\"\n        )\n        return result_documents\n</code></pre>"},{"location":"data-processing/docs/api/stages/#eve.steps.dedup.dedup_step.DuplicationStep.execute","title":"<code>execute(documents)</code>  <code>async</code>","text":"<p>Execute deduplication on input files or documents.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <p>List of file paths or Document objects to deduplicate</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>List of Document objects with duplicates removed</p> Source code in <code>data-processing/eve/steps/dedup/dedup_step.py</code> <pre><code>async def execute(self, documents: List[Document]) -&gt; List[Document]:\n    \"\"\"Execute deduplication on input files or documents.\n\n    Args:\n        input_data: List of file paths or Document objects to deduplicate\n\n    Returns:\n        List of Document objects with duplicates removed\n    \"\"\"\n    method = self.config.get(\"method\", \"exact\")  # default to exact\n\n    self.logger.info(f\"Executing duplication step with method: {method} file count: {len(documents)}\")\n\n    if method == \"exact\":\n        duplicates = await self._exact_deduplication(documents)\n    elif method == \"lsh\":\n        duplicates = await self._lsh_deduplication(documents)\n    else:\n        self.logger.error(f\"Invalid deduplication method: {method}\")\n        raise ValueError(f\"Invalid deduplication method: {method}\")\n\n    # Remove duplicates from documents\n    duplicate_docs = set()\n    duplicates_removed = 0\n    for group in duplicates:\n        # Keep the first doc in each group, mark the rest as duplicates\n        for doc in group[1:]:\n            duplicate_docs.add(doc)\n            duplicates_removed += 1\n\n    # Filter out duplicates, keeping the first occurrence\n    result_documents = []\n    for doc in documents:\n        if doc not in duplicate_docs:\n            result_documents.append(doc)\n\n    self.logger.info(\n        f\"Deduplication complete: {len(result_documents)} files remaining, {duplicates_removed} duplicates removed\"\n    )\n    return result_documents\n</code></pre>"},{"location":"data-processing/docs/api/stages/#deduplication-methods","title":"Deduplication Methods","text":""},{"location":"data-processing/docs/api/stages/#exact-duplicates","title":"Exact Duplicates","text":""},{"location":"data-processing/docs/api/stages/#eve.steps.dedup.exact_duplicates.ExactDuplication","title":"<code>ExactDuplication</code>","text":"<p>this class does exact duplication by -</p> <ol> <li>calculate size as a first filter to save computation.</li> <li>calcuates checksum and finds the duplicates.</li> </ol> Source code in <code>data-processing/eve/steps/dedup/exact_duplicates.py</code> <pre><code>class ExactDuplication:\n    \"\"\"this class does exact duplication by -\n\n    1. calculate size as a first filter to save computation.\n    2. calcuates checksum and finds the duplicates.\n    \"\"\"\n\n    def __init__(self, documents: List[Document]):\n        self.documents = documents\n        self.duplicates = []\n\n        self._validate()\n\n    def _validate(self):\n        if len(self.documents) &lt; 2:\n            raise ValueError(\"need at least 2 files for duplication\")\n\n    @staticmethod\n    async def _calculate_sha256(file_path: Path) -&gt; str:\n        \"\"\"calculate SHA-256 checksum of a file.\"\"\"\n        sha256 = hashlib.sha256()\n        async for chunk in read_in_chunks(file_path, 'rb'):\n            sha256.update(chunk)\n        return sha256.hexdigest()\n\n    @staticmethod\n    async def _calculate_size(file_path: Path) -&gt; int:\n        \"\"\"calculate file size\"\"\"\n        stat = await asyncio.to_thread(lambda: file_path.stat())  # run blocking stat in thread\n        return stat.st_size\n\n    async def find_duplicates(self) -&gt; list[list[Document]]:\n        \"\"\"Find duplicate files based on size and SHA-256 checksum.\"\"\"\n\n        # stage 1: group files by size\n        size_tasks = [self._calculate_size(doc.file_path) for doc in self.documents]\n        sizes = await asyncio.gather(*size_tasks)\n\n        size_groups = defaultdict(list)\n        for doc, size in zip(self.documents, sizes):\n            size_groups[size].append(doc)\n\n        # stage 2: calculate checksums for potential duplicates\n        checksum_tasks = []\n        file_info = []\n\n        for size, docs in size_groups.items():\n            if len(docs) &gt;= 2:  # Only consider docs with matching sizes\n                for doc in docs:\n                    checksum_tasks.append(self._calculate_sha256(doc.file_path))\n                    file_info.append((doc, size))\n\n        if not checksum_tasks:\n            return []\n\n        checksums = await asyncio.gather(*checksum_tasks)\n\n        file_map = defaultdict(list)\n        for (doc, size), checksum in zip(file_info, checksums):\n            key = (size, checksum)\n            file_map[key].append(doc)\n\n        self.duplicates = {key: docs for key, docs in file_map.items() if len(docs) &gt; 1}\n        return list(self.duplicates.values())\n</code></pre>"},{"location":"data-processing/docs/api/stages/#eve.steps.dedup.exact_duplicates.ExactDuplication.find_duplicates","title":"<code>find_duplicates()</code>  <code>async</code>","text":"<p>Find duplicate files based on size and SHA-256 checksum.</p> Source code in <code>data-processing/eve/steps/dedup/exact_duplicates.py</code> <pre><code>async def find_duplicates(self) -&gt; list[list[Document]]:\n    \"\"\"Find duplicate files based on size and SHA-256 checksum.\"\"\"\n\n    # stage 1: group files by size\n    size_tasks = [self._calculate_size(doc.file_path) for doc in self.documents]\n    sizes = await asyncio.gather(*size_tasks)\n\n    size_groups = defaultdict(list)\n    for doc, size in zip(self.documents, sizes):\n        size_groups[size].append(doc)\n\n    # stage 2: calculate checksums for potential duplicates\n    checksum_tasks = []\n    file_info = []\n\n    for size, docs in size_groups.items():\n        if len(docs) &gt;= 2:  # Only consider docs with matching sizes\n            for doc in docs:\n                checksum_tasks.append(self._calculate_sha256(doc.file_path))\n                file_info.append((doc, size))\n\n    if not checksum_tasks:\n        return []\n\n    checksums = await asyncio.gather(*checksum_tasks)\n\n    file_map = defaultdict(list)\n    for (doc, size), checksum in zip(file_info, checksums):\n        key = (size, checksum)\n        file_map[key].append(doc)\n\n    self.duplicates = {key: docs for key, docs in file_map.items() if len(docs) &gt; 1}\n    return list(self.duplicates.values())\n</code></pre>"},{"location":"data-processing/docs/api/stages/#minhash-lsh","title":"MinHash LSH","text":"<p>Adjust NUM_PERM: Higher values increase accuracy but use more memory. Adjust THRESHOLD: Higher values find closer duplicates but may miss some. Adjust SHINGLE_SIZE: Larger shingles are more specific but increase computation.</p>"},{"location":"data-processing/docs/api/stages/#eve.steps.dedup.minhash.LSH","title":"<code>LSH</code>","text":"Source code in <code>data-processing/eve/steps/dedup/minhash.py</code> <pre><code>class LSH:\n    def __init__(\n        self,\n        documents: List[Document],\n        shingle_size: int = 3,\n        num_perm: int = 128,\n        threshold: float = 0.8,\n    ):\n        self.documents = documents\n        self.shingle_size = shingle_size\n        self.num_perm = num_perm\n        self.threshold = threshold\n        self.doc_hashes = {}   # map: document -&gt; minHash\n        self.duplicates = []\n\n        self._validate()\n\n    def _validate(self):\n        if len(self.documents) &lt; 2:\n            raise ValueError(\"need at least 2 files for duplication\")\n\n    def create_shingles(self, text: str) -&gt; set[str]:\n        \"\"\"Create shingles (word n-grams) from text.\"\"\"\n        words = text.lower().split()\n        return {\" \".join(gram) for gram in ngrams(words, self.shingle_size)}\n\n    def _do_lsh(self) -&gt; Any:\n        lsh = MinHashLSH(threshold=self.threshold, num_perm=self.num_perm)\n\n        for doc in tqdm(self.documents, total=len(self.documents)):\n            shingles = self.create_shingles(doc.content)\n\n            m = MinHash(num_perm=self.num_perm)\n            for shingle in shingles:\n                m.update(shingle.encode(\"utf8\"))\n\n            # Use file_path as the LSH key, but keep mapping to Document\n            lsh.insert(str(doc.file_path), m)\n            self.doc_hashes[doc] = m\n\n        return lsh\n\n    def find_duplicates(self) -&gt; list[list[Document]]:\n        \"\"\"Find near-duplicate documents using LSH.\"\"\"\n        file_hashes = self._do_lsh()\n        processed = set()\n\n        for doc in self.documents:\n            if doc in processed:\n                continue\n\n            m = self.doc_hashes[doc]\n            candidates = file_hashes.query(m)\n\n            # Convert LSH string keys back to Document objects\n            candidate_docs = [\n                d for d in self.documents if str(d.file_path) in candidates and d != doc\n            ]\n\n            if candidate_docs:\n                group = [doc, *candidate_docs]\n                group = sorted(group, key = lambda d: str(d.file_path))  # consistent ordering\n                if group not in self.duplicates:\n                    self.duplicates.append(group)\n                processed.update(group)\n\n        return self.duplicates\n</code></pre>"},{"location":"data-processing/docs/api/stages/#eve.steps.dedup.minhash.LSH.create_shingles","title":"<code>create_shingles(text)</code>","text":"<p>Create shingles (word n-grams) from text.</p> Source code in <code>data-processing/eve/steps/dedup/minhash.py</code> <pre><code>def create_shingles(self, text: str) -&gt; set[str]:\n    \"\"\"Create shingles (word n-grams) from text.\"\"\"\n    words = text.lower().split()\n    return {\" \".join(gram) for gram in ngrams(words, self.shingle_size)}\n</code></pre>"},{"location":"data-processing/docs/api/stages/#eve.steps.dedup.minhash.LSH.find_duplicates","title":"<code>find_duplicates()</code>","text":"<p>Find near-duplicate documents using LSH.</p> Source code in <code>data-processing/eve/steps/dedup/minhash.py</code> <pre><code>def find_duplicates(self) -&gt; list[list[Document]]:\n    \"\"\"Find near-duplicate documents using LSH.\"\"\"\n    file_hashes = self._do_lsh()\n    processed = set()\n\n    for doc in self.documents:\n        if doc in processed:\n            continue\n\n        m = self.doc_hashes[doc]\n        candidates = file_hashes.query(m)\n\n        # Convert LSH string keys back to Document objects\n        candidate_docs = [\n            d for d in self.documents if str(d.file_path) in candidates and d != doc\n        ]\n\n        if candidate_docs:\n            group = [doc, *candidate_docs]\n            group = sorted(group, key = lambda d: str(d.file_path))  # consistent ordering\n            if group not in self.duplicates:\n                self.duplicates.append(group)\n            processed.update(group)\n\n    return self.duplicates\n</code></pre>"},{"location":"data-processing/docs/api/stages/#cleaning-stage","title":"Cleaning Stage","text":"<p>Cleans and improves document quality.</p> <p>Comprehensive cleaning step that applies all data cleaning components.</p>"},{"location":"data-processing/docs/api/stages/#eve.steps.cleaning.cleaning_step.CleaningStep","title":"<code>CleaningStep</code>","text":"<p>               Bases: <code>PipelineStep</code></p> <p>Comprehensive cleaning step that applies multiple data cleaning components.</p> <p>This step processes extracted text through various cleaning components to:</p> <ul> <li>Fix OCR-induced errors</li> <li>Remove OCR duplicates</li> <li>Apply Nougat corrections</li> <li>Apply rule-based corrections</li> <li>Remove Nougat artifacts</li> <li>Correct LaTeX syntax errors (optional)</li> </ul> Source code in <code>data-processing/eve/steps/cleaning/cleaning_step.py</code> <pre><code>class CleaningStep(PipelineStep):\n    \"\"\"\n    Comprehensive cleaning step that applies multiple data cleaning components.\n\n    This step processes extracted text through various cleaning components to:\n\n    - Fix OCR-induced errors\n    - Remove OCR duplicates\n    - Apply Nougat corrections\n    - Apply rule-based corrections\n    - Remove Nougat artifacts\n    - Correct LaTeX syntax errors (optional)\n    \"\"\"\n\n    def __init__(self, config: dict):\n        \"\"\"Initialize the cleaning step with configuration.\n\n        Args:\n            config: Configuration dictionary with component settings.\n\n                Expected keys:\n\n                - ocr_threshold: float (default 0.99) - OCR duplicate threshold\n                - min_words: int (default 2) - Minimum words for processing\n                - enable_latex_correction: bool (default False) - Enable LaTeX correction\n                - openrouter_api_key: str (optional) - API key for LaTeX correction\n                - openrouter_model: str (default \"anthropic/claude-3-haiku\") - Model for corrections\n                - debug: bool (default False) - Enable debug output\n        \"\"\"\n        super().__init__(config, name=\"CleaningStep\")\n\n        ocr_threshold = config.get(\"ocr_threshold\", 0.99)\n        min_words = config.get(\"min_words\", 2)\n        enable_latex = config.get(\"enable_latex_correction\", False)\n        openrouter_key = config.get(\"openrouter_api_key\")\n        openrouter_model = config.get(\"openrouter_model\", \"anthropic/claude-3-haiku\")\n\n        self.processors = [\n            OCRProcessor(debug=self.debug),\n            DuplicateRemovalProcessor(threshold=ocr_threshold, min_words=min_words, debug=self.debug),\n            NougatProcessor(debug=self.debug),\n            RuleBasedProcessor(debug=self.debug),\n        ]\n\n        if enable_latex:\n            self.processors.append(\n                LaTeXProcessor(\n                    debug=self.debug,\n                    api_key=openrouter_key,\n                    model=openrouter_model\n                )\n            )\n\n    async def execute(self, documents: List[Document]) -&gt; List[Document]:\n        \"\"\"Execute the cleaning step on input data.\n\n        Args:\n            documents: List of Documents.\n\n        Returns:\n            List of cleaned Documents.\n        \"\"\"\n        self.logger.info(f\"Executing cleaning step on {len(documents)} documents\")\n\n        if not documents:\n            self.logger.warning(\"No input data provided to cleaning step\")\n            return []\n\n        result = []\n        processed_count = 0\n        failed_count = 0\n\n        for document in documents:\n            if document.is_empty():\n                self.logger.warning(f\"{document.filename} - Empty content, skipping cleaning\")\n                result.append(document)\n                failed_count += 1\n                continue\n\n            try:\n                processed_document = document\n                original_length = document.content_length\n\n                for processor in self.processors:\n                    try:\n                        processed_document = await processor.process(processed_document)\n\n                        if processed_document is None:\n                            self.logger.error(f\"{document.filename} - Processor {processor.__class__.__name__} returned None\")\n                            processed_document = document\n                            break\n\n                    except Exception as e:\n                        self.logger.error(f\"{document.filename} - Processor {processor.__class__.__name__} failed: {str(e)}\")\n                        continue\n\n                if original_length &gt; 0 and processed_document.content_length != original_length:\n                    reduction_percent = ((original_length - processed_document.content_length) / original_length) * 100\n\n                    if reduction_percent &gt; 0:\n                        self.logger.info(f\"{document.filename} - Cleaned: {reduction_percent:.2f}% text removed ({original_length} -&gt; {processed_document.content_length} chars)\")\n                    else:\n                        self.logger.info(f\"{document.filename} - Cleaned: No significant changes\")\n\n                result.append(processed_document)\n                processed_count += 1\n\n            except Exception as e:\n                self.logger.error(f\"{document.filename} - Cleaning failed: {str(e)}\")\n                result.append(document)\n                failed_count += 1\n\n        self.logger.info(f\"Cleaning step completed: {processed_count} processed, {failed_count} failed\")\n        return result\n\n    def _get_applicable_formats(self) -&gt; List[str]:\n        \"\"\"Get list of formats that these cleaning components apply to.\n\n        Returns:\n            List of file formats that can be processed by cleaning components.\n        \"\"\"\n        return [\n            \"md\",\n            \"txt\",\n            \"tex\",\n            \"html\",\n            \"xml\",\n        ]\n\n    def get_component_info(self) -&gt; dict:\n        \"\"\"Get information about enabled cleaning processors.\n\n        Returns:\n            Dictionary with processor information.\n        \"\"\"\n        component_info = {\n            \"total_processors\": len(self.processors),\n            \"processors\": [processor.__class__.__name__ for processor in self.processors],\n            \"applicable_formats\": self._get_applicable_formats(),\n            \"debug_enabled\": self.debug\n        }\n\n        latex_enabled = any(isinstance(proc, LaTeXProcessor) for proc in self.processors)\n        component_info[\"latex_correction_enabled\"] = latex_enabled\n\n        return component_info\n</code></pre>"},{"location":"data-processing/docs/api/stages/#eve.steps.cleaning.cleaning_step.CleaningStep.__init__","title":"<code>__init__(config)</code>","text":"<p>Initialize the cleaning step with configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Configuration dictionary with component settings.</p> <p>Expected keys:</p> <ul> <li>ocr_threshold: float (default 0.99) - OCR duplicate threshold</li> <li>min_words: int (default 2) - Minimum words for processing</li> <li>enable_latex_correction: bool (default False) - Enable LaTeX correction</li> <li>openrouter_api_key: str (optional) - API key for LaTeX correction</li> <li>openrouter_model: str (default \"anthropic/claude-3-haiku\") - Model for corrections</li> <li>debug: bool (default False) - Enable debug output</li> </ul> required Source code in <code>data-processing/eve/steps/cleaning/cleaning_step.py</code> <pre><code>def __init__(self, config: dict):\n    \"\"\"Initialize the cleaning step with configuration.\n\n    Args:\n        config: Configuration dictionary with component settings.\n\n            Expected keys:\n\n            - ocr_threshold: float (default 0.99) - OCR duplicate threshold\n            - min_words: int (default 2) - Minimum words for processing\n            - enable_latex_correction: bool (default False) - Enable LaTeX correction\n            - openrouter_api_key: str (optional) - API key for LaTeX correction\n            - openrouter_model: str (default \"anthropic/claude-3-haiku\") - Model for corrections\n            - debug: bool (default False) - Enable debug output\n    \"\"\"\n    super().__init__(config, name=\"CleaningStep\")\n\n    ocr_threshold = config.get(\"ocr_threshold\", 0.99)\n    min_words = config.get(\"min_words\", 2)\n    enable_latex = config.get(\"enable_latex_correction\", False)\n    openrouter_key = config.get(\"openrouter_api_key\")\n    openrouter_model = config.get(\"openrouter_model\", \"anthropic/claude-3-haiku\")\n\n    self.processors = [\n        OCRProcessor(debug=self.debug),\n        DuplicateRemovalProcessor(threshold=ocr_threshold, min_words=min_words, debug=self.debug),\n        NougatProcessor(debug=self.debug),\n        RuleBasedProcessor(debug=self.debug),\n    ]\n\n    if enable_latex:\n        self.processors.append(\n            LaTeXProcessor(\n                debug=self.debug,\n                api_key=openrouter_key,\n                model=openrouter_model\n            )\n        )\n</code></pre>"},{"location":"data-processing/docs/api/stages/#eve.steps.cleaning.cleaning_step.CleaningStep.execute","title":"<code>execute(documents)</code>  <code>async</code>","text":"<p>Execute the cleaning step on input data.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>List[Document]</code> <p>List of Documents.</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>List of cleaned Documents.</p> Source code in <code>data-processing/eve/steps/cleaning/cleaning_step.py</code> <pre><code>async def execute(self, documents: List[Document]) -&gt; List[Document]:\n    \"\"\"Execute the cleaning step on input data.\n\n    Args:\n        documents: List of Documents.\n\n    Returns:\n        List of cleaned Documents.\n    \"\"\"\n    self.logger.info(f\"Executing cleaning step on {len(documents)} documents\")\n\n    if not documents:\n        self.logger.warning(\"No input data provided to cleaning step\")\n        return []\n\n    result = []\n    processed_count = 0\n    failed_count = 0\n\n    for document in documents:\n        if document.is_empty():\n            self.logger.warning(f\"{document.filename} - Empty content, skipping cleaning\")\n            result.append(document)\n            failed_count += 1\n            continue\n\n        try:\n            processed_document = document\n            original_length = document.content_length\n\n            for processor in self.processors:\n                try:\n                    processed_document = await processor.process(processed_document)\n\n                    if processed_document is None:\n                        self.logger.error(f\"{document.filename} - Processor {processor.__class__.__name__} returned None\")\n                        processed_document = document\n                        break\n\n                except Exception as e:\n                    self.logger.error(f\"{document.filename} - Processor {processor.__class__.__name__} failed: {str(e)}\")\n                    continue\n\n            if original_length &gt; 0 and processed_document.content_length != original_length:\n                reduction_percent = ((original_length - processed_document.content_length) / original_length) * 100\n\n                if reduction_percent &gt; 0:\n                    self.logger.info(f\"{document.filename} - Cleaned: {reduction_percent:.2f}% text removed ({original_length} -&gt; {processed_document.content_length} chars)\")\n                else:\n                    self.logger.info(f\"{document.filename} - Cleaned: No significant changes\")\n\n            result.append(processed_document)\n            processed_count += 1\n\n        except Exception as e:\n            self.logger.error(f\"{document.filename} - Cleaning failed: {str(e)}\")\n            result.append(document)\n            failed_count += 1\n\n    self.logger.info(f\"Cleaning step completed: {processed_count} processed, {failed_count} failed\")\n    return result\n</code></pre>"},{"location":"data-processing/docs/api/stages/#eve.steps.cleaning.cleaning_step.CleaningStep.get_component_info","title":"<code>get_component_info()</code>","text":"<p>Get information about enabled cleaning processors.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with processor information.</p> Source code in <code>data-processing/eve/steps/cleaning/cleaning_step.py</code> <pre><code>def get_component_info(self) -&gt; dict:\n    \"\"\"Get information about enabled cleaning processors.\n\n    Returns:\n        Dictionary with processor information.\n    \"\"\"\n    component_info = {\n        \"total_processors\": len(self.processors),\n        \"processors\": [processor.__class__.__name__ for processor in self.processors],\n        \"applicable_formats\": self._get_applicable_formats(),\n        \"debug_enabled\": self.debug\n    }\n\n    latex_enabled = any(isinstance(proc, LaTeXProcessor) for proc in self.processors)\n    component_info[\"latex_correction_enabled\"] = latex_enabled\n\n    return component_info\n</code></pre>"},{"location":"data-processing/docs/api/stages/#cleaning-components","title":"Cleaning Components","text":""},{"location":"data-processing/docs/api/stages/#processors","title":"Processors","text":"<p>Consolidated text processing components for the cleaning pipeline.</p> <p>This module combines all the individual cleaning components into a unified structure for better organization and maintainability.</p>"},{"location":"data-processing/docs/api/stages/#eve.steps.cleaning.processors.DuplicateRemovalProcessor","title":"<code>DuplicateRemovalProcessor</code>","text":"<p>               Bases: <code>TextProcessor</code></p> <p>Processor for removing OCR-induced duplicate text segments.</p> Source code in <code>data-processing/eve/steps/cleaning/processors.py</code> <pre><code>class DuplicateRemovalProcessor(TextProcessor):\n    \"\"\"Processor for removing OCR-induced duplicate text segments.\"\"\"\n\n    def __init__(\n        self, threshold: float = 0.99, min_words: int = 2, debug: bool = False\n    ):\n        \"\"\"\n        Initialize the duplicate removal processor.\n\n        Args:\n            threshold: Similarity threshold for duplicates.\n            min_words: Minimum words required for a unit to be processed.\n            debug: Enable debug output.\n        \"\"\"\n        super().__init__(debug=debug)\n        self.threshold = threshold\n        self.min_words = min_words\n\n    def _is_similar(self, sent1: str, sent2: str) -&gt; bool:\n        \"\"\"Check if two sentences are similar based on word overlap.\"\"\"\n        words1 = sent1.lower().split()\n        words2 = sent2.lower().split()\n\n        if len(words1) &lt; self.min_words:\n            return False\n\n        set1, set2 = set(words1), set(words2)\n        overlap = len(set1 &amp; set2)\n        return (\n            overlap / len(set1) &gt;= self.threshold\n            or overlap / len(set2) &gt;= self.threshold\n        )\n\n    def _remove_near_adjacent_duplicates(\n        self, content: str, filename: str\n    ) -&gt; Tuple[str, List[str]]:\n        \"\"\"Remove near-adjacent duplicate sentences.\"\"\"\n        sentences = content.split(\"\\n\")\n        cleaned = []\n        removed = []\n        i = 0\n\n        while i &lt; len(sentences):\n            current = sentences[i]\n            if len(current.split()) &lt; self.min_words:\n                cleaned.append(current)\n                i += 1\n                continue\n\n            j = i + 1\n            while j &lt; len(sentences) and not sentences[j].strip():\n                j += 1\n\n            if j &lt; len(sentences) and self._is_similar(current, sentences[j]):\n                self.logger.info(\n                    f\"{filename} - Removing near-duplicate: {repr(sentences[j])}\"\n                )\n                removed.append(sentences[j])\n                i = j\n            else:\n                cleaned.append(current)\n                i += 1\n\n        return \"\\n\".join(cleaned), removed\n\n    async def process(self, document: Document) -&gt; Document:\n        \"\"\"Remove duplicate content from the document.\"\"\"\n        if self.debug:\n            self.logger.info(\n                f\"Before duplicate removal ({document.filename}): {document.content[:200]}...\"\n            )\n\n        if document.is_empty():\n            self.logger.warning(\n                f\"{document.filename} - Empty content in duplicate removal\"\n            )\n            return document\n\n        try:\n            cleaned_content, removed = self._remove_near_adjacent_duplicates(\n                document.content, document.filename\n            )\n\n            percent_removed = 0.0\n            if document.content:\n                percent_removed = (\n                    (len(document.content) - len(cleaned_content))\n                    / len(document.content)\n                    * 100\n                )\n\n            document.update_content(cleaned_content)\n            document.add_metadata(\"duplicates_removed\", len(removed))\n            document.add_metadata(\"duplicate_removal_percent\", percent_removed)\n\n            self.logger.info(\n                f\"{document.filename} - Duplicate removal: {len(removed)} segments, {percent_removed:.2f}% text removed\"\n            )\n\n            if self.debug:\n                self.logger.info(\n                    f\"After duplicate removal ({document.filename}): {document.content[:200]}...\"\n                )\n\n            return document\n\n        except Exception as e:\n            self.logger.error(\n                f\"{document.filename} - Duplicate removal failed: {str(e)}\"\n            )\n            return document\n</code></pre>"},{"location":"data-processing/docs/api/stages/#eve.steps.cleaning.processors.DuplicateRemovalProcessor.__init__","title":"<code>__init__(threshold=0.99, min_words=2, debug=False)</code>","text":"<p>Initialize the duplicate removal processor.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>Similarity threshold for duplicates.</p> <code>0.99</code> <code>min_words</code> <code>int</code> <p>Minimum words required for a unit to be processed.</p> <code>2</code> <code>debug</code> <code>bool</code> <p>Enable debug output.</p> <code>False</code> Source code in <code>data-processing/eve/steps/cleaning/processors.py</code> <pre><code>def __init__(\n    self, threshold: float = 0.99, min_words: int = 2, debug: bool = False\n):\n    \"\"\"\n    Initialize the duplicate removal processor.\n\n    Args:\n        threshold: Similarity threshold for duplicates.\n        min_words: Minimum words required for a unit to be processed.\n        debug: Enable debug output.\n    \"\"\"\n    super().__init__(debug=debug)\n    self.threshold = threshold\n    self.min_words = min_words\n</code></pre>"},{"location":"data-processing/docs/api/stages/#eve.steps.cleaning.processors.DuplicateRemovalProcessor.process","title":"<code>process(document)</code>  <code>async</code>","text":"<p>Remove duplicate content from the document.</p> Source code in <code>data-processing/eve/steps/cleaning/processors.py</code> <pre><code>async def process(self, document: Document) -&gt; Document:\n    \"\"\"Remove duplicate content from the document.\"\"\"\n    if self.debug:\n        self.logger.info(\n            f\"Before duplicate removal ({document.filename}): {document.content[:200]}...\"\n        )\n\n    if document.is_empty():\n        self.logger.warning(\n            f\"{document.filename} - Empty content in duplicate removal\"\n        )\n        return document\n\n    try:\n        cleaned_content, removed = self._remove_near_adjacent_duplicates(\n            document.content, document.filename\n        )\n\n        percent_removed = 0.0\n        if document.content:\n            percent_removed = (\n                (len(document.content) - len(cleaned_content))\n                / len(document.content)\n                * 100\n            )\n\n        document.update_content(cleaned_content)\n        document.add_metadata(\"duplicates_removed\", len(removed))\n        document.add_metadata(\"duplicate_removal_percent\", percent_removed)\n\n        self.logger.info(\n            f\"{document.filename} - Duplicate removal: {len(removed)} segments, {percent_removed:.2f}% text removed\"\n        )\n\n        if self.debug:\n            self.logger.info(\n                f\"After duplicate removal ({document.filename}): {document.content[:200]}...\"\n            )\n\n        return document\n\n    except Exception as e:\n        self.logger.error(\n            f\"{document.filename} - Duplicate removal failed: {str(e)}\"\n        )\n        return document\n</code></pre>"},{"location":"data-processing/docs/api/stages/#eve.steps.cleaning.processors.LaTeXProcessor","title":"<code>LaTeXProcessor</code>","text":"<p>               Bases: <code>TextProcessor</code></p> <p>Processor for detecting and correcting LaTeX syntax errors.</p> Source code in <code>data-processing/eve/steps/cleaning/processors.py</code> <pre><code>class LaTeXProcessor(TextProcessor):\n    \"\"\"Processor for detecting and correcting LaTeX syntax errors.\"\"\"\n\n    def __init__(\n        self,\n        debug: bool = False,\n        api_key: Optional[str] = None,\n        model: str = \"anthropic/claude-3-haiku\",\n    ):\n        \"\"\"Initialize the LaTeX processor.\n\n        Args:\n            debug: Enable debug output.\n            api_key: OpenRouter API key. If None, will use OPENROUTER_API_KEY environment variable.\n            model: OpenRouter model to use for corrections.\n        \"\"\"\n        super().__init__(debug=debug)\n        self.api_key = api_key or os.getenv(\"OPENROUTER_API_KEY\")\n        self.model = model\n        self.formula_patterns = get_latex_formula_patterns()\n\n        if not self.api_key and debug:\n            self.logger.warning(\n                \"No OPENROUTER_API_KEY found. LaTeX correction will only detect errors.\"\n            )\n\n    def _extract_latex_formulas(self, content: str) -&gt; List[Tuple[str, str]]:\n        \"\"\"Extract LaTeX formulas from content with their types.\"\"\"\n        formulas = []\n\n        for formula_type, pattern in self.formula_patterns.items():\n            for match in pattern.finditer(content):\n                if formula_type == \"environment\":\n                    env_name = match.group(1)\n                    env_content = match.group(2).strip()\n                    formulas.append(\n                        (\n                            formula_type,\n                            f\"\\\\begin{{{env_name}}}{env_content}\\\\end{{{env_name}}}\",\n                        )\n                    )\n                else:\n                    formulas.append((formula_type, match.group(1).strip()))\n\n        return formulas\n\n    async def _check_formula_syntax(\n        self, formula: str, formula_type: str\n    ) -&gt; Tuple[bool, str]:\n        \"\"\"Check if a LaTeX formula has valid syntax using pylatex and subprocess.\"\"\"\n        try:\n            def check_latex():\n                \"\"\"Run pdflatex compilation in a separate thread.\"\"\"\n                try:\n                    with tempfile.TemporaryDirectory() as tmp_dir:\n                        # Create a pylatex Document\n                        doc = LaTeXDocument(documentclass=\"article\")\n\n                        # Add required packages\n                        doc.packages.append(Package(\"amsmath\"))\n                        doc.packages.append(Package(\"amssymb\"))\n\n                        # Add formula-specific content wrapped in NoEscape\n                        if formula_type == \"inline\":\n                            doc.append(NoEscape(f\"${formula}$\"))\n                        elif formula_type == \"display\":\n                            doc.append(NoEscape(f\"$${formula}$$\"))\n                        elif formula_type == \"bracket\":\n                            doc.append(NoEscape(f\"\\\\({formula}\\\\)\"))\n                        elif formula_type == \"square_bracket\":\n                            doc.append(NoEscape(f\"\\\\[{formula}\\\\]\"))\n                        else:\n                            # Environment type - add extra packages\n                            doc.packages.append(Package(\"multirow\"))\n                            doc.packages.append(Package(\"bm\"))\n                            doc.append(NoEscape(formula))\n\n                        # Generate the .tex file\n                        tex_file = os.path.join(tmp_dir, \"test\")\n                        doc.generate_tex(tex_file)\n\n                        # Run pdflatex using subprocess\n                        result = subprocess.run(\n                            [\"pdflatex\", \"-interaction=nonstopmode\", f\"{tex_file}.tex\"],\n                            cwd=tmp_dir,\n                            stdout=subprocess.PIPE,\n                            stderr=subprocess.PIPE,\n                            timeout=30,\n                        )\n\n                        if result.returncode == 0:\n                            return True, \"Formula syntax is valid\"\n                        else:\n                            # Parse output for error messages\n                            output = result.stdout.decode(\"utf-8\", errors=\"replace\")\n                            error_lines = output.split(\"\\n\")\n                            error_msg = \"Unknown error\"\n                            for i, line in enumerate(error_lines):\n                                if \"! \" in line:\n                                    error_msg = line.strip()\n                                    if (\n                                        i + 1 &lt; len(error_lines)\n                                        and error_lines[i + 1].strip()\n                                    ):\n                                        error_msg += \" \" + error_lines[i + 1].strip()\n                                    break\n                            return False, error_msg\n                except subprocess.TimeoutExpired:\n                    return False, \"PDFLaTeX compilation timed out\"\n                except FileNotFoundError:\n                    return False, \"pdflatex command not found. Please ensure LaTeX is installed.\"\n                except Exception as e:\n                    return False, f\"PDFLaTeX compilation failed: {str(e)}\"\n\n            # Run the blocking operation in a thread pool\n            return await asyncio.get_event_loop().run_in_executor(None, check_latex)\n\n        except Exception as e:\n            return False, f\"Syntax check failed: {str(e)}\"\n\n    def _replace_formula_in_content(\n        self, content: str, original: str, corrected: str, formula_type: str\n    ) -&gt; str:\n        \"\"\"Replace original formula with corrected version in content.\"\"\"\n        try:\n            if formula_type == \"inline\":\n                pattern = re.escape(f\"${original}$\")\n                replacement = f\"${corrected}$\"\n            elif formula_type == \"display\":\n                pattern = re.escape(f\"$${original}$$\")\n                replacement = f\"$${corrected}$$\"\n            elif formula_type == \"bracket\":\n                pattern = re.escape(f\"\\\\({original}\\\\)\")\n                replacement = f\"\\\\({corrected}\\\\)\"\n            elif formula_type == \"square_bracket\":\n                pattern = re.escape(f\"\\\\[{original}\\\\]\")\n                replacement = f\"\\\\[{corrected}\\\\]\"\n            else:\n                pattern = re.escape(original)\n                replacement = corrected\n\n            return re.sub(pattern, replacement, content, count=1)\n        except Exception:\n            return content.replace(original, corrected, 1)\n\n    async def process(self, document: Document) -&gt; Document:\n        \"\"\"Process document to detect and correct LaTeX syntax errors.\"\"\"\n        if self.debug:\n            self.logger.info(\n                f\"Before LaTeX processing ({document.filename}): {document.content[:200]}...\"\n            )\n\n        if document.is_empty():\n            self.logger.warning(\n                f\"{document.filename} - Empty content in LaTeX processing\"\n            )\n            return document\n\n        try:\n            formulas = self._extract_latex_formulas(document.content)\n\n            if not formulas:\n                self.logger.info(f\"{document.filename} - No LaTeX formulas found\")\n                document.add_metadata(\"latex_processed\", True)\n                return document\n\n            errors_found = 0\n            corrections_made = 0\n            modified_content = document.content\n\n            for formula_type, formula in formulas:\n                is_valid, error_message = await self._check_formula_syntax(\n                    formula, formula_type\n                )\n\n                if not is_valid:\n                    errors_found += 1\n                    self.logger.warning(\n                        f\"{document.filename} - Invalid LaTeX formula: {formula[:10]}... Error: {error_message}\"\n                    )\n\n                    if self.api_key:\n                        prompt = get_latex_correction_prompt(\n                            formula_type, error_message, formula, document.content\n                        )\n                        corrected_formula = await make_openrouter_request(\n                            self.api_key, self.model, prompt\n                        )\n\n                        if corrected_formula and corrected_formula != formula:\n                            is_corrected_valid, _ = await self._check_formula_syntax(\n                                corrected_formula, formula_type\n                            )\n\n                            if is_corrected_valid:\n                                modified_content = self._replace_formula_in_content(\n                                    modified_content,\n                                    formula,\n                                    corrected_formula,\n                                    formula_type,\n                                )\n                                corrections_made += 1\n                                self.logger.info(\n                                    f\"{document.filename} - Corrected LaTeX formula: {formula[:30]}... -&gt; {corrected_formula[:30]}...\"\n                                )\n                            else:\n                                self.logger.warning(\n                                    f\"{document.filename} - LLM correction still invalid: {corrected_formula[:50]}...\"\n                                )\n\n            document.update_content(modified_content)\n            document.add_metadata(\"latex_errors_found\", errors_found)\n            document.add_metadata(\"latex_corrections_made\", corrections_made)\n            document.add_metadata(\"latex_processed\", True)\n\n            if errors_found &gt; 0:\n                self.logger.info(\n                    f\"{document.filename} - LaTeX processing complete: {errors_found} errors found, {corrections_made} corrected\"\n                )\n            else:\n                self.logger.info(f\"{document.filename} - All LaTeX formulas are valid\")\n\n            if self.debug:\n                self.logger.info(\n                    f\"After LaTeX processing ({document.filename}): {errors_found} errors, {corrections_made} fixed\"\n                )\n\n            return document\n\n        except Exception as e:\n            self.logger.error(\n                f\"{document.filename} - LaTeX processing failed: {str(e)}\"\n            )\n            return document\n</code></pre>"},{"location":"data-processing/docs/api/stages/#eve.steps.cleaning.processors.LaTeXProcessor.__init__","title":"<code>__init__(debug=False, api_key=None, model='anthropic/claude-3-haiku')</code>","text":"<p>Initialize the LaTeX processor.</p> <p>Parameters:</p> Name Type Description Default <code>debug</code> <code>bool</code> <p>Enable debug output.</p> <code>False</code> <code>api_key</code> <code>Optional[str]</code> <p>OpenRouter API key. If None, will use OPENROUTER_API_KEY environment variable.</p> <code>None</code> <code>model</code> <code>str</code> <p>OpenRouter model to use for corrections.</p> <code>'anthropic/claude-3-haiku'</code> Source code in <code>data-processing/eve/steps/cleaning/processors.py</code> <pre><code>def __init__(\n    self,\n    debug: bool = False,\n    api_key: Optional[str] = None,\n    model: str = \"anthropic/claude-3-haiku\",\n):\n    \"\"\"Initialize the LaTeX processor.\n\n    Args:\n        debug: Enable debug output.\n        api_key: OpenRouter API key. If None, will use OPENROUTER_API_KEY environment variable.\n        model: OpenRouter model to use for corrections.\n    \"\"\"\n    super().__init__(debug=debug)\n    self.api_key = api_key or os.getenv(\"OPENROUTER_API_KEY\")\n    self.model = model\n    self.formula_patterns = get_latex_formula_patterns()\n\n    if not self.api_key and debug:\n        self.logger.warning(\n            \"No OPENROUTER_API_KEY found. LaTeX correction will only detect errors.\"\n        )\n</code></pre>"},{"location":"data-processing/docs/api/stages/#eve.steps.cleaning.processors.LaTeXProcessor.process","title":"<code>process(document)</code>  <code>async</code>","text":"<p>Process document to detect and correct LaTeX syntax errors.</p> Source code in <code>data-processing/eve/steps/cleaning/processors.py</code> <pre><code>async def process(self, document: Document) -&gt; Document:\n    \"\"\"Process document to detect and correct LaTeX syntax errors.\"\"\"\n    if self.debug:\n        self.logger.info(\n            f\"Before LaTeX processing ({document.filename}): {document.content[:200]}...\"\n        )\n\n    if document.is_empty():\n        self.logger.warning(\n            f\"{document.filename} - Empty content in LaTeX processing\"\n        )\n        return document\n\n    try:\n        formulas = self._extract_latex_formulas(document.content)\n\n        if not formulas:\n            self.logger.info(f\"{document.filename} - No LaTeX formulas found\")\n            document.add_metadata(\"latex_processed\", True)\n            return document\n\n        errors_found = 0\n        corrections_made = 0\n        modified_content = document.content\n\n        for formula_type, formula in formulas:\n            is_valid, error_message = await self._check_formula_syntax(\n                formula, formula_type\n            )\n\n            if not is_valid:\n                errors_found += 1\n                self.logger.warning(\n                    f\"{document.filename} - Invalid LaTeX formula: {formula[:10]}... Error: {error_message}\"\n                )\n\n                if self.api_key:\n                    prompt = get_latex_correction_prompt(\n                        formula_type, error_message, formula, document.content\n                    )\n                    corrected_formula = await make_openrouter_request(\n                        self.api_key, self.model, prompt\n                    )\n\n                    if corrected_formula and corrected_formula != formula:\n                        is_corrected_valid, _ = await self._check_formula_syntax(\n                            corrected_formula, formula_type\n                        )\n\n                        if is_corrected_valid:\n                            modified_content = self._replace_formula_in_content(\n                                modified_content,\n                                formula,\n                                corrected_formula,\n                                formula_type,\n                            )\n                            corrections_made += 1\n                            self.logger.info(\n                                f\"{document.filename} - Corrected LaTeX formula: {formula[:30]}... -&gt; {corrected_formula[:30]}...\"\n                            )\n                        else:\n                            self.logger.warning(\n                                f\"{document.filename} - LLM correction still invalid: {corrected_formula[:50]}...\"\n                            )\n\n        document.update_content(modified_content)\n        document.add_metadata(\"latex_errors_found\", errors_found)\n        document.add_metadata(\"latex_corrections_made\", corrections_made)\n        document.add_metadata(\"latex_processed\", True)\n\n        if errors_found &gt; 0:\n            self.logger.info(\n                f\"{document.filename} - LaTeX processing complete: {errors_found} errors found, {corrections_made} corrected\"\n            )\n        else:\n            self.logger.info(f\"{document.filename} - All LaTeX formulas are valid\")\n\n        if self.debug:\n            self.logger.info(\n                f\"After LaTeX processing ({document.filename}): {errors_found} errors, {corrections_made} fixed\"\n            )\n\n        return document\n\n    except Exception as e:\n        self.logger.error(\n            f\"{document.filename} - LaTeX processing failed: {str(e)}\"\n        )\n        return document\n</code></pre>"},{"location":"data-processing/docs/api/stages/#eve.steps.cleaning.processors.NougatProcessor","title":"<code>NougatProcessor</code>","text":"<p>               Bases: <code>TextProcessor</code></p> <p>Processor for fixing Nougat-related issues and artifacts.</p> Source code in <code>data-processing/eve/steps/cleaning/processors.py</code> <pre><code>class NougatProcessor(TextProcessor):\n    \"\"\"Processor for fixing Nougat-related issues and artifacts.\"\"\"\n\n    async def process(self, document: Document) -&gt; Document:\n        \"\"\"Process Nougat-specific issues in the document.\"\"\"\n        if self.debug:\n            self.logger.info(\n                f\"Before Nougat processing ({document.filename}): {document.content[:200]}...\"\n            )\n\n        if document.is_empty():\n            self.logger.warning(\n                f\"{document.filename} - Empty content in Nougat processing\"\n            )\n            return document\n\n        try:\n            # Apply Nougat postprocessing\n            cleaned = postprocess_single(document.content, markdown_fix=True)\n\n            # Clean LaTeX table formatting\n            cleaned = clean_doubled_backslashes(cleaned)\n\n            # Remove Nougat artifacts\n            cleaned = remove_nougat_artifacts(cleaned)\n\n            # Convert escaped newlines\n            cleaned = cleaned.replace(\"\\\\n\", \"\\n\")\n\n            # Remove surrounding quotes\n            cleaned = cleaned.strip('\"')\n\n            document.update_content(cleaned)\n            document.add_metadata(\"nougat_processed\", True)\n\n            self.logger.info(f\"{document.filename} - Nougat processing completed\")\n\n            if self.debug:\n                self.logger.info(\n                    f\"After Nougat processing ({document.filename}): {document.content[:200]}...\"\n                )\n\n            return document\n\n        except Exception as e:\n            self.logger.error(\n                f\"{document.filename} - Nougat processing failed: {str(e)}\"\n            )\n            return document\n</code></pre>"},{"location":"data-processing/docs/api/stages/#eve.steps.cleaning.processors.NougatProcessor.process","title":"<code>process(document)</code>  <code>async</code>","text":"<p>Process Nougat-specific issues in the document.</p> Source code in <code>data-processing/eve/steps/cleaning/processors.py</code> <pre><code>async def process(self, document: Document) -&gt; Document:\n    \"\"\"Process Nougat-specific issues in the document.\"\"\"\n    if self.debug:\n        self.logger.info(\n            f\"Before Nougat processing ({document.filename}): {document.content[:200]}...\"\n        )\n\n    if document.is_empty():\n        self.logger.warning(\n            f\"{document.filename} - Empty content in Nougat processing\"\n        )\n        return document\n\n    try:\n        # Apply Nougat postprocessing\n        cleaned = postprocess_single(document.content, markdown_fix=True)\n\n        # Clean LaTeX table formatting\n        cleaned = clean_doubled_backslashes(cleaned)\n\n        # Remove Nougat artifacts\n        cleaned = remove_nougat_artifacts(cleaned)\n\n        # Convert escaped newlines\n        cleaned = cleaned.replace(\"\\\\n\", \"\\n\")\n\n        # Remove surrounding quotes\n        cleaned = cleaned.strip('\"')\n\n        document.update_content(cleaned)\n        document.add_metadata(\"nougat_processed\", True)\n\n        self.logger.info(f\"{document.filename} - Nougat processing completed\")\n\n        if self.debug:\n            self.logger.info(\n                f\"After Nougat processing ({document.filename}): {document.content[:200]}...\"\n            )\n\n        return document\n\n    except Exception as e:\n        self.logger.error(\n            f\"{document.filename} - Nougat processing failed: {str(e)}\"\n        )\n        return document\n</code></pre>"},{"location":"data-processing/docs/api/stages/#eve.steps.cleaning.processors.OCRProcessor","title":"<code>OCRProcessor</code>","text":"<p>               Bases: <code>TextProcessor</code></p> <p>Processor for fixing OCR-induced text issues.</p> Source code in <code>data-processing/eve/steps/cleaning/processors.py</code> <pre><code>class OCRProcessor(TextProcessor):\n    \"\"\"Processor for fixing OCR-induced text issues.\"\"\"\n\n    async def process(self, document: Document) -&gt; Document:\n        \"\"\"Fix OCR issues in the document content.\"\"\"\n        if self.debug:\n            self.logger.info(\n                f\"Before OCR processing ({document.filename}): {document.content[:200]}...\"\n            )\n\n        if document.is_empty():\n            self.logger.warning(\n                f\"{document.filename} - Empty content in OCR processing\"\n            )\n            return document\n\n        try:\n            # Fix digit-letter spacing issues\n            cleaned_content = fix_ocr_digit_letter_spacing(document.content)\n\n            document.update_content(cleaned_content)\n            document.add_metadata(\"ocr_processed\", True)\n\n            self.logger.info(f\"{document.filename} - OCR processing completed\")\n\n            if self.debug:\n                self.logger.info(\n                    f\"After OCR processing ({document.filename}): {document.content[:200]}...\"\n                )\n\n            return document\n\n        except Exception as e:\n            self.logger.error(f\"{document.filename} - OCR processing failed: {str(e)}\")\n            return document\n</code></pre>"},{"location":"data-processing/docs/api/stages/#eve.steps.cleaning.processors.OCRProcessor.process","title":"<code>process(document)</code>  <code>async</code>","text":"<p>Fix OCR issues in the document content.</p> Source code in <code>data-processing/eve/steps/cleaning/processors.py</code> <pre><code>async def process(self, document: Document) -&gt; Document:\n    \"\"\"Fix OCR issues in the document content.\"\"\"\n    if self.debug:\n        self.logger.info(\n            f\"Before OCR processing ({document.filename}): {document.content[:200]}...\"\n        )\n\n    if document.is_empty():\n        self.logger.warning(\n            f\"{document.filename} - Empty content in OCR processing\"\n        )\n        return document\n\n    try:\n        # Fix digit-letter spacing issues\n        cleaned_content = fix_ocr_digit_letter_spacing(document.content)\n\n        document.update_content(cleaned_content)\n        document.add_metadata(\"ocr_processed\", True)\n\n        self.logger.info(f\"{document.filename} - OCR processing completed\")\n\n        if self.debug:\n            self.logger.info(\n                f\"After OCR processing ({document.filename}): {document.content[:200]}...\"\n            )\n\n        return document\n\n    except Exception as e:\n        self.logger.error(f\"{document.filename} - OCR processing failed: {str(e)}\")\n        return document\n</code></pre>"},{"location":"data-processing/docs/api/stages/#eve.steps.cleaning.processors.RuleBasedProcessor","title":"<code>RuleBasedProcessor</code>","text":"<p>               Bases: <code>TextProcessor</code></p> <p>Processor for applying rule-based text corrections.</p> Source code in <code>data-processing/eve/steps/cleaning/processors.py</code> <pre><code>class RuleBasedProcessor(TextProcessor):\n    \"\"\"Processor for applying rule-based text corrections.\"\"\"\n\n    async def process(self, document: Document) -&gt; Document:\n        \"\"\"Apply rule-based corrections to the document.\"\"\"\n        if self.debug:\n            self.logger.info(\n                f\"Before rule-based processing ({document.filename}): {document.content[:200]}...\"\n            )\n\n        if document.is_empty():\n            self.logger.warning(\n                f\"{document.filename} - Empty content in rule-based processing\"\n            )\n            return document\n\n        try:\n            # Remove single symbol lines\n            cleaned = remove_single_symbol_lines(document.content)\n\n            # Normalize excessive newlines\n            cleaned = normalize_excessive_newlines(cleaned)\n\n            # Trim whitespace\n            cleaned = cleaned.strip()\n\n            document.update_content(cleaned)\n            document.add_metadata(\"rule_based_processed\", True)\n\n            self.logger.info(f\"{document.filename} - Rule-based processing completed\")\n\n            if self.debug:\n                self.logger.info(\n                    f\"After rule-based processing ({document.filename}): {document.content[:200]}...\"\n                )\n\n            return document\n\n        except Exception as e:\n            self.logger.error(\n                f\"{document.filename} - Rule-based processing failed: {str(e)}\"\n            )\n            return document\n</code></pre>"},{"location":"data-processing/docs/api/stages/#eve.steps.cleaning.processors.RuleBasedProcessor.process","title":"<code>process(document)</code>  <code>async</code>","text":"<p>Apply rule-based corrections to the document.</p> Source code in <code>data-processing/eve/steps/cleaning/processors.py</code> <pre><code>async def process(self, document: Document) -&gt; Document:\n    \"\"\"Apply rule-based corrections to the document.\"\"\"\n    if self.debug:\n        self.logger.info(\n            f\"Before rule-based processing ({document.filename}): {document.content[:200]}...\"\n        )\n\n    if document.is_empty():\n        self.logger.warning(\n            f\"{document.filename} - Empty content in rule-based processing\"\n        )\n        return document\n\n    try:\n        # Remove single symbol lines\n        cleaned = remove_single_symbol_lines(document.content)\n\n        # Normalize excessive newlines\n        cleaned = normalize_excessive_newlines(cleaned)\n\n        # Trim whitespace\n        cleaned = cleaned.strip()\n\n        document.update_content(cleaned)\n        document.add_metadata(\"rule_based_processed\", True)\n\n        self.logger.info(f\"{document.filename} - Rule-based processing completed\")\n\n        if self.debug:\n            self.logger.info(\n                f\"After rule-based processing ({document.filename}): {document.content[:200]}...\"\n            )\n\n        return document\n\n    except Exception as e:\n        self.logger.error(\n            f\"{document.filename} - Rule-based processing failed: {str(e)}\"\n        )\n        return document\n</code></pre>"},{"location":"data-processing/docs/api/stages/#eve.steps.cleaning.processors.TextProcessor","title":"<code>TextProcessor</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for text processing components.</p> Source code in <code>data-processing/eve/steps/cleaning/processors.py</code> <pre><code>class TextProcessor(ABC):\n    \"\"\"Abstract base class for text processing components.\"\"\"\n\n    def __init__(self, debug: bool = False):\n        \"\"\"Initialize the text processor.\n\n        Args:\n            debug: Enable debug output.\n        \"\"\"\n        self.debug = debug\n        self.logger = get_logger(self.__class__.__name__)\n\n    @abstractmethod\n    async def process(self, document: Document) -&gt; Document:\n        \"\"\"Process a document and return the cleaned result.\n\n        Args:\n            document: The document to process.\n\n        Returns:\n            Processed document.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"data-processing/docs/api/stages/#eve.steps.cleaning.processors.TextProcessor.__init__","title":"<code>__init__(debug=False)</code>","text":"<p>Initialize the text processor.</p> <p>Parameters:</p> Name Type Description Default <code>debug</code> <code>bool</code> <p>Enable debug output.</p> <code>False</code> Source code in <code>data-processing/eve/steps/cleaning/processors.py</code> <pre><code>def __init__(self, debug: bool = False):\n    \"\"\"Initialize the text processor.\n\n    Args:\n        debug: Enable debug output.\n    \"\"\"\n    self.debug = debug\n    self.logger = get_logger(self.__class__.__name__)\n</code></pre>"},{"location":"data-processing/docs/api/stages/#eve.steps.cleaning.processors.TextProcessor.process","title":"<code>process(document)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Process a document and return the cleaned result.</p> <p>Parameters:</p> Name Type Description Default <code>document</code> <code>Document</code> <p>The document to process.</p> required <p>Returns:</p> Type Description <code>Document</code> <p>Processed document.</p> Source code in <code>data-processing/eve/steps/cleaning/processors.py</code> <pre><code>@abstractmethod\nasync def process(self, document: Document) -&gt; Document:\n    \"\"\"Process a document and return the cleaned result.\n\n    Args:\n        document: The document to process.\n\n    Returns:\n        Processed document.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"data-processing/docs/api/stages/#nougat-helpers","title":"Nougat Helpers","text":"<p>Copyright (c) Meta Platforms, Inc. and affiliates.</p> <p>This source code is licensed under the MIT license found in the LICENSE file in the root directory of this source tree.</p> <p>Script from here - https://github.com/facebookresearch/nougat/blob/main/nougat/postprocessing.py</p>"},{"location":"data-processing/docs/api/stages/#eve.steps.cleaning.nougat_helpers.get_slices","title":"<code>get_slices(lines, clean_lines)</code>","text":"<p>Get slices of potentially hallucinated reference sections.</p> Source code in <code>data-processing/eve/steps/cleaning/nougat_helpers.py</code> <pre><code>def get_slices(lines: List[str], clean_lines: List[str]) -&gt; List[slice]:\n    \"\"\"Get slices of potentially hallucinated reference sections.\"\"\"\n    slices = []\n    for i, line in enumerate(lines):\n        if line.strip().lower().startswith('## references'):\n            j = i + 1\n            while j &lt; len(lines) and not lines[j].strip().startswith('##'):\n                j += 1\n            slices.append(slice(i, j))\n    return slices\n</code></pre>"},{"location":"data-processing/docs/api/stages/#eve.steps.cleaning.nougat_helpers.markdown_compatible","title":"<code>markdown_compatible(s)</code>","text":"<p>Make text compatible with Markdown formatting.</p> <p>This function makes various text formatting adjustments to make it compatible with Markdown.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>The input text to be made Markdown-compatible.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The Markdown-compatible text.</p> Source code in <code>data-processing/eve/steps/cleaning/nougat_helpers.py</code> <pre><code>def markdown_compatible(s: str) -&gt; str:\n    \"\"\"\n    Make text compatible with Markdown formatting.\n\n    This function makes various text formatting adjustments to make it compatible with Markdown.\n\n    Args:\n        s (str): The input text to be made Markdown-compatible.\n\n    Returns:\n        str: The Markdown-compatible text.\n    \"\"\"\n    s = re.sub(\n        r\"^\\(([\\d.]+[a-zA-Z]?)\\) \\\\\\[(.+?)\\\\\\]$\", r\"\\[\\2 \\\\tag{\\1}\\]\", s, flags=re.M\n    )\n    s = re.sub(\n        r\"^\\\\\\[(.+?)\\\\\\] \\(([\\d.]+[a-zA-Z]?)\\)$\", r\"\\[\\1 \\\\tag{\\2}\\]\", s, flags=re.M\n    )\n    s = re.sub(\n        r\"^\\\\\\[(.+?)\\\\\\] \\(([\\d.]+[a-zA-Z]?)\\) (\\\\\\[.+?\\\\\\])$\",\n        r\"\\[\\1 \\\\tag{\\2}\\] \\3\",\n        s,\n        flags=re.M,\n    )\n    s = s.replace(r\"\\. \", \". \")\n    s = s.replace(r\"\\.}\", \".}\")\n    s = s.replace(r\"\\. }\", \". }\")\n    s = s.replace(r\"\\.\\]\", \".]\")\n    s = s.replace(r\"\\. ]\", \". ]\")\n    s = re.sub(r\"\\\\begin\\{table\\}\\s*\\\\begin\\{tabular\\}(.*?)\\\\end\\{tabular\\}\\s*\\\\end\\{table\\}\", r\"\\n\\\\begin{table}\\n\\\\begin{tabular}\\1\\\\end{tabular}\\n\\\\end{table}\\n\", s, flags=re.DOTALL)\n\n    s = re.sub(r\"([^\\s])\\$([^\\$]*)\\$\", r\"\\1 $\\2$\", s)\n    s = re.sub(r\"\\$([^\\$]*)\\$([^\\s])\", r\"$\\1$ \\2\", s)\n\n    return s\n</code></pre>"},{"location":"data-processing/docs/api/stages/#eve.steps.cleaning.nougat_helpers.postprocess","title":"<code>postprocess(generation, markdown_fix=True)</code>","text":"<p>Postprocess generated text or a list of generated texts.</p> <p>This function can be used to perform postprocessing on generated text, such as fixing Markdown formatting.</p> <p>Parameters:</p> Name Type Description Default <code>generation</code> <code>Union[str, List[str]]</code> <p>The generated text or a list of generated texts.</p> required <code>markdown_fix</code> <code>bool</code> <p>Whether to perform Markdown formatting fixes. Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[str, List[str]]</code> <p>Union[str, List[str]]: The postprocessed text or list of postprocessed texts.</p> Source code in <code>data-processing/eve/steps/cleaning/nougat_helpers.py</code> <pre><code>def postprocess(\n    generation: Union[str, List[str]], markdown_fix: bool = True\n) -&gt; Union[str, List[str]]:\n    \"\"\"\n    Postprocess generated text or a list of generated texts.\n\n    This function can be used to perform postprocessing on generated text, such as fixing Markdown formatting.\n\n    Args:\n        generation (Union[str, List[str]]): The generated text or a list of generated texts.\n        markdown_fix (bool, optional): Whether to perform Markdown formatting fixes. Default is True.\n\n    Returns:\n        Union[str, List[str]]: The postprocessed text or list of postprocessed texts.\n    \"\"\"\n    if type(generation) == list:\n        if os.environ.get(\"NOUGAT_MULTIPROCESSING\"):\n            with Pool(int(os.environ.get(\"NOUGAT_MULTIPROCESSING\"))) as p:\n                return p.map(\n                    partial(postprocess_single, markdown_fix=markdown_fix), generation\n                )\n        else:\n            return [\n                postprocess_single(s, markdown_fix=markdown_fix) for s in generation\n            ]\n    else:\n        return postprocess_single(generation, markdown_fix=markdown_fix)\n</code></pre>"},{"location":"data-processing/docs/api/stages/#eve.steps.cleaning.nougat_helpers.postprocess_single","title":"<code>postprocess_single(generation, markdown_fix=True)</code>","text":"<p>Postprocess a single generated text.</p> <p>Parameters:</p> Name Type Description Default <code>generation</code> <code>str</code> <p>The generated text to be postprocessed.</p> required <code>markdown_fix</code> <code>bool</code> <p>Whether to perform Markdown formatting fixes. Default is True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The postprocessed text.</p> Source code in <code>data-processing/eve/steps/cleaning/nougat_helpers.py</code> <pre><code>def postprocess_single(generation: str, markdown_fix: bool = True) -&gt; str:\n    \"\"\"\n    Postprocess a single generated text.\n\n    Args:\n        generation (str): The generated text to be postprocessed.\n        markdown_fix (bool, optional): Whether to perform Markdown formatting fixes. Default is True.\n\n    Returns:\n        str: The postprocessed text.\n    \"\"\"\n    generation = re.sub(\n        r\"(?:\\n|^)#+ \\d*\\W? ?(.{100,})\", r\"\\n\\1\", generation\n    )\n    generation = generation.strip()\n    generation = generation.replace(\"\\n* [leftmargin=*]\\n\", \"\\n\")\n    generation = re.sub(\n        r\"^#+ (?:\\.?(?:\\d|[ixv])+)*\\s*(?:$|\\n\\s*)\", \"\", generation, flags=re.M\n    )\n    lines = generation.split(\"\\n\")\n    if (\n        lines[-1].startswith(\"#\")\n        and lines[-1].lstrip(\"#\").startswith(\" \")\n        and len(lines) &gt; 1\n    ):\n        print(\"INFO: likely hallucinated title at the end of the page: \" + lines[-1])\n        generation = \"\\n\".join(lines[:-1])\n    generation = truncate_repetitions(generation)\n    generation = remove_hallucinated_references(generation)\n    generation = re.sub(\n        r\"^\\* \\[\\d+\\](\\s?[A-W]\\.+\\s?){10,}.*$\", \"\", generation, flags=re.M\n    )\n    generation = re.sub(r\"^(\\* \\[\\d+\\])\\[\\](.*)$\", r\"\\1\\2\", generation, flags=re.M)\n    generation = re.sub(r\"(^\\w\\n\\n|\\n\\n\\w$)\", \"\", generation)\n    generation = re.sub(\n        r\"([\\s.,()])_([a-zA-Z0-9])__([a-zA-Z0-9]){1,3}_([\\s.,:()])\",\n        r\"\\1\\(\\2_{\\3}\\)\\4\",\n        generation,\n    )\n    generation = re.sub(\n        r\"([\\s.,\\d])_([a-zA-Z0-9])_([\\s.,\\d;])\", r\"\\1\\(\\2\\)\\3\", generation\n    )\n    generation = re.sub(\n        r\"(\\nFootnote .*?:) (?:footnotetext|thanks):\\W*(.*(?:\\n\\n|$))\",\n        r\"\\1 \\2\",\n        generation,\n    )\n    generation = re.sub(r\"\\[FOOTNOTE:.+?\\](.*?)\\[ENDFOOTNOTE\\]\", \"\", generation)\n    for match in reversed(\n        list(\n            re.finditer(\n                r\"(?:^)(-|\\*)?(?!-|\\*) ?((?:\\d|[ixv])+ )?.+? (-|\\*) (((?:\\d|[ixv])+)\\.(\\d|[ixv]) )?.*(?:$)\",\n                generation,\n                flags=re.I | re.M,\n            )\n        )\n    ):\n        start, stop = match.span()\n        delim = match.group(3) + \" \"\n        splits = match.group(0).split(delim)\n        replacement = \"\"\n        if match.group(1) is not None:\n            splits = splits[1:]\n            delim1 = match.group(1) + \" \"\n        else:\n            delim1 = \"\"\n            continue\n        pre, post = generation[:start], generation[stop:]\n        for i, item in enumerate(splits):\n            level = 0\n            potential_numeral, _, rest = item.strip().partition(\" \")\n            if not rest:\n                continue\n            if re.match(\n                r\"^[\\dixv]+((?:\\.[\\dixv])?)+$\", potential_numeral, flags=re.I | re.M\n            ):\n                level = potential_numeral.count(\".\")\n\n            replacement += (\n                (\"\\n\" if i &gt; 0 else \"\")\n                + (\"\\t\" * level)\n                + (delim if i &gt; 0 or start == 0 else delim1)\n                + item.strip()\n            )\n        if post == \"\":\n            post = \"\\n\"\n        generation = pre + replacement + post\n\n    if generation.endswith((\".\", \"}\")):\n        generation += \"\\n\\n\"\n    if re.match(r\"[A-Z0-9,;:]$\", generation):\n        generation += \" \"\n    elif generation.startswith((\"#\", \"**\", \"\\\\begin\")):\n        generation = \"\\n\\n\" + generation\n    elif generation.split(\"\\n\")[-1].startswith((\"#\", \"Figure\", \"Table\")):\n        generation = generation + \"\\n\\n\"\n    else:\n        try:\n            last_word = generation.split(\" \")[-1]\n            if last_word in words.words():\n                generation += \" \"\n        except LookupError:\n            generation += \" \"\n            import nltk\n\n            nltk.download(\"words\")\n    for l in generation.split(\"\\n\"):\n        if (\n            l.count(\"\\\\begin{tabular}\") &gt; 15\n            or l.count(\"\\\\multicolumn\") &gt; 60\n            or l.count(\"&amp;\") &gt; 400\n        ):\n            generation = generation.replace(l, \"\")\n    generation = generation.replace(\n        \"\\\\begin{table} \\\\begin{tabular}\", \"\\\\begin{table}\\n\\\\begin{tabular}\"\n    )\n    generation = generation.replace(\n        \"\\\\end{tabular} \\\\end{table}\", \"\\\\end{tabular}\\n\\\\end{table}\"\n    )\n    generation = generation.replace(\"\\\\end{table} Tab\", \"\\\\end{table}\\nTab\")\n    generation = re.sub(r\"(^.+)\\\\begin{tab\", r\"\\1\\n\\\\begin{tab\", generation, flags=re.M)\n\n    generation = generation.replace(\n        r\"\\begin{tabular}{l l}  &amp; \\\\ \\end{tabular}\", \"\"\n    ).replace(\"\\\\begin{tabular}{}\\n\\n\\\\end{tabular}\", \"\")\n    generation = generation.replace(\"\\\\begin{array}[]{\", \"\\\\begin{array}{\")\n    generation = re.sub(\n        r\"\\\\begin{tabular}{([clr ]){2,}}\\s*[&amp; ]*\\s*(\\\\\\\\)? \\\\end{tabular}\",\n        \"\",\n        generation,\n    )\n    generation = re.sub(r\"(\\*\\*S\\. A\\. B\\.\\*\\*\\n+){2,}\", \"\", generation)\n    generation = re.sub(r\"^#+( [\\[\\d\\w])?$\", \"\", generation, flags=re.M)\n    generation = re.sub(r\"^\\.\\s*$\", \"\", generation, flags=re.M)\n    generation = re.sub(r\"\\n{3,}\", \"\\n\\n\", generation)\n    if markdown_fix:\n        return markdown_compatible(generation)\n    else:\n        return generation\n</code></pre>"},{"location":"data-processing/docs/api/stages/#eve.steps.cleaning.nougat_helpers.remove_hallucinated_references","title":"<code>remove_hallucinated_references(text)</code>","text":"<p>Remove hallucinated or missing references from the text.</p> <p>This function identifies and removes references that are marked as missing or hallucinated from the input text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text containing references.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The text with hallucinated references removed.</p> Source code in <code>data-processing/eve/steps/cleaning/nougat_helpers.py</code> <pre><code>def remove_hallucinated_references(text: str) -&gt; str:\n    \"\"\"\n    Remove hallucinated or missing references from the text.\n\n    This function identifies and removes references that are marked as missing or hallucinated\n    from the input text.\n\n    Args:\n        text (str): The input text containing references.\n\n    Returns:\n        str: The text with hallucinated references removed.\n    \"\"\"\n    lines = text.split(\"\\n\")\n    if len(lines) == 0:\n        return \"\"\n    clean_lines = remove_numbers(lines)\n    slices = get_slices(lines, clean_lines)\n    to_delete = []\n    for sli in slices:\n        to_delete.append(remove_slice_from_lines(lines, clean_lines, sli))\n    for to_delete in reversed(to_delete):\n        text = text.replace(to_delete, \"\\n\\n[MISSING_PAGE_POST]\\n\\n\")\n    text = re.sub(\n        r\"## References\\n+\\[MISSING_PAGE_POST(:\\d+)?\\]\",\n        \"\\n\\n[MISSING_PAGE_POST\\\\1]\",\n        text,\n    )\n    return text\n</code></pre>"},{"location":"data-processing/docs/api/stages/#eve.steps.cleaning.nougat_helpers.remove_numbers","title":"<code>remove_numbers(lines)</code>","text":"<p>Remove number patterns from lines.</p> Source code in <code>data-processing/eve/steps/cleaning/nougat_helpers.py</code> <pre><code>def remove_numbers(lines: List[str]) -&gt; List[str]:\n    \"\"\"Remove number patterns from lines.\"\"\"\n    clean_lines = []\n    for line in lines:\n        clean_line = re.sub(r'\\[\\d+\\]', '', line)\n        clean_line = re.sub(r'\\d+\\.', '', clean_line)\n        clean_lines.append(clean_line.strip())\n    return clean_lines\n</code></pre>"},{"location":"data-processing/docs/api/stages/#eve.steps.cleaning.nougat_helpers.remove_slice_from_lines","title":"<code>remove_slice_from_lines(lines, clean_lines, sli)</code>","text":"<p>Remove slice from lines and return the removed text.</p> Source code in <code>data-processing/eve/steps/cleaning/nougat_helpers.py</code> <pre><code>def remove_slice_from_lines(lines: List[str], clean_lines: List[str], sli: slice) -&gt; str:\n    \"\"\"Remove slice from lines and return the removed text.\"\"\"\n    removed_text = '\\n'.join(lines[sli])\n    return removed_text\n</code></pre>"},{"location":"data-processing/docs/api/stages/#eve.steps.cleaning.nougat_helpers.truncate_repetitions","title":"<code>truncate_repetitions(generation, score_cutoff=0.5, min_len=30)</code>","text":"<p>Truncate repetitions in the given generation.</p> <p>This function identifies and truncates repetitive content in the text.</p> Source code in <code>data-processing/eve/steps/cleaning/nougat_helpers.py</code> <pre><code>def truncate_repetitions(generation: str, score_cutoff: float = 0.5, min_len: int = 30):\n    \"\"\"\n    Truncate repetitions in the given generation.\n\n    This function identifies and truncates repetitive content in the text.\n    \"\"\"\n    try:\n        sentences = generation.split(\".\")\n        if len(sentences) &lt; 3:\n            return generation\n\n        to_delete = set()\n        for i in range(len(sentences)):\n            for j in range(i + 1, len(sentences)):\n                sent_i = sentences[i].strip()\n                sent_j = sentences[j].strip()\n\n                if len(sent_i) &lt; min_len or len(sent_j) &lt; min_len:\n                    continue\n\n                if ratio(sent_i, sent_j) &gt; score_cutoff:\n                    to_delete.add(j)\n\n        new_sentences = [sent for i, sent in enumerate(sentences) if i not in to_delete]\n        return \".\".join(new_sentences)\n    except Exception:\n        return generation\n</code></pre>"},{"location":"data-processing/docs/api/stages/#pii-removal-stage","title":"PII Removal Stage","text":"<p>Removes personally identifiable information from documents.</p>"},{"location":"data-processing/docs/api/stages/#eve.steps.pii.pii_step.PiiStep","title":"<code>PiiStep</code>","text":"<p>               Bases: <code>PipelineStep</code></p> Source code in <code>data-processing/eve/steps/pii/pii_step.py</code> <pre><code>class PiiStep(PipelineStep):\n\n    async def _remove_pii(\n        self,\n        document: Document,\n        entities: Optional[List[str]] = None,\n        threshold: float = 0.35,\n        return_analysis: bool = False,\n        url: str = None,\n    ) -&gt; Document:\n        \"\"\"Make a call to the litserve API and remove PII (async with aiohttp).\"\"\"\n\n        if not url:\n            self.logger.error(\"No URL provided for PII service\")\n            return document\n\n        if entities is None:\n            entities = [\"PERSON\", \"EMAIL_ADDRESS\"]\n\n        payload = {\n            \"text\": document.content,\n            \"entities\": entities,\n            \"score_threshold\": threshold,\n            \"return_analysis\": return_analysis,\n        }\n\n        try:\n            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=120)) as session:\n                async with session.post(\n                    url,\n                    json = payload,\n                    headers={\"Content-Type\": \"application/json\"},\n                ) as response:\n                    response.raise_for_status()\n                    result = await response.json()\n                    self.logger.debug(f\"PII API result: {result}\")\n                    document.content = result.get(\"anonymized_text\", document.content)\n                    return document\n\n        except aiohttp.ClientError as e:\n            self.logger.error(f\"PII API request failed: {e}\")\n            return document\n\n    async def remove_pii(\n        self,\n        document: Document,\n        entities: Optional[List[str]] = None,\n        threshold: float = 0.35,\n        return_analysis: bool = False,\n        url: str = None,\n    ) -&gt; Document:\n        return await self._remove_pii(document, entities, threshold, return_analysis, url)\n\n    async def execute(self, documents: List[Document]) -&gt; List[Document]:\n        base_url = self.config.get(\"url\")\n        if not base_url:\n            self.logger.error(\"No URL provided for PII service\")\n            return []\n\n        url = f\"{base_url}/predict\"\n\n        # Run all requests concurrently\n        tasks = [self.remove_pii(document, url=url) for document in documents]\n        results = await asyncio.gather(*tasks, return_exceptions = True)\n\n        final = []\n        for doc in results:\n            if doc and getattr(doc, \"content_length\", 0) &gt; 1:\n                final.append(doc)\n                self.logger.info(f\"Successfully anonymized {doc.filename}\")\n\n        return final\n</code></pre>"},{"location":"data-processing/docs/api/stages/#metadata-extraction-stage","title":"Metadata Extraction Stage","text":"<p>Extracts structured metadata from documents.</p> <p>Metadata extraction step for the EVE pipeline.</p>"},{"location":"data-processing/docs/api/stages/#eve.steps.metadata.metadata_step.MetadataStep","title":"<code>MetadataStep</code>","text":"<p>               Bases: <code>PipelineStep</code></p> <p>Metadata extraction step that extracts metadata from PDF and HTML documents.</p> Source code in <code>data-processing/eve/steps/metadata/metadata_step.py</code> <pre><code>class MetadataStep(PipelineStep):\n    \"\"\"\n    Metadata extraction step that extracts metadata from PDF and HTML documents.\n    \"\"\"\n\n    def __init__(self, config: dict):\n        \"\"\"\n        Initialize the metadata extraction step.\n\n        Args:\n            config: Configuration dictionary with options:\n\n                - enabled_formats: List of file formats to process (pdf, html, txt, md)\n                - fallback_to_filename: Use filename as title fallback\n                - debug: Enable debug logging\n                - export_metadata: Whether to export metadata to JSON file\n                - metadata_destination: Directory to save metadata file\n                - metadata_filename: Name of the metadata JSON file\n                  Note: Text formats (txt, md) automatically enable this feature\n        \"\"\"\n        super().__init__(config)\n\n        self.enabled_formats = set(config.get(\"enabled_formats\", [\"pdf\", \"html\", \"txt\", \"md\"]))\n        self.fallback_to_filename = config.get(\"fallback_to_filename\", True)\n        self.export_metadata = config.get(\"export_metadata\", True)\n        self.metadata_destination = Path(config.get(\"metadata_destination\", \"./output\"))\n        self.metadata_filename = config.get(\"metadata_filename\", \"metadata.jsonl\")\n\n        self.extractors = {\n            \"pdf\": PdfMetadataExtractor(debug=self.debug),\n            \"html\": HtmlMetadataExtractor(debug=self.debug)\n        }\n\n    async def _extract_metadata_for_document(self, document: Document) -&gt; Document:\n        \"\"\"\n        Extract metadata for a single document using appropriate extractor.\n        Args:\n            document: Document to extract metadata from\n\n        Returns:\n            Document with metadata added to the metadata field:\n            - extracted_title: Document title\n            - extracted_authors: List of authors\n            - extracted_year: Publication year\n            - extracted_metadata: Full metadata dictionary\n            - extraction_error: Error message (if extraction failed)\n        \"\"\"\n        if document.file_format not in self.enabled_formats:\n            self.logger.debug(f\"Skipping metadata extraction for unsupported format: {document.file_format}\")\n            return document\n\n        metadata = None\n\n        try:\n            if document.file_format in [\"txt\", \"md\"]:\n                if not document.content.strip() and document.file_path.exists():\n                    try:\n                        with open(document.file_path, 'r', encoding='utf-8') as f:\n                            document.update_content(f.read())\n                        self.logger.info(f\"Loaded content for text file {document.filename}: {len(document.content)} characters\")\n                    except Exception as e:\n                        self.logger.error(f\"Failed to load content for {document.filename}: {str(e)}\")\n                        return document\n\n            elif document.file_format in self.extractors:\n                extractor = self.extractors[document.file_format]\n                metadata = await extractor.extract_metadata(document)\n            else:\n                self.logger.warning(f\"No extractor available for format: {document.file_format}\")\n\n            if metadata:\n                document.add_metadata(\"extracted_metadata\", metadata)\n\n                self.logger.info(f\"Successfully extracted metadata from {document.filename}\")\n                if self.debug:\n                    self.logger.debug(f\"Extracted metadata keys: {list(metadata.keys())}\")\n            else:\n                self.logger.warning(f\"No metadata extracted from {document.filename}\")\n\n                if self.fallback_to_filename:\n                    title = document.file_path.stem.replace(\"_\", \" \").replace(\"-\", \" \")\n                    document.add_metadata(\"title\", title)\n                    self.logger.info(f\"Using filename as title fallback: {title}\")\n\n        except Exception as e:\n            self.logger.error(f\"Failed to extract metadata from {document.filename}: {str(e)}\")\n\n            if self.fallback_to_filename:\n                title = document.file_path.stem.replace(\"_\", \" \").replace(\"-\", \" \")\n                document.add_metadata(\"title\", title)\n            document.add_metadata(\"extraction_error\", str(e))\n\n        return document\n\n    async def execute(self, documents: List[Document]) -&gt; List[Document]:\n        \"\"\"\n        Execute metadata extraction on input documents.\n\n        Args:\n            documents: List of Document objects to extract metadata from\n\n        Returns:\n            List of Document objects with metadata added\n        \"\"\"\n        if not documents:\n            self.logger.warning(\"No input documents provided to metadata step\")\n            return []\n\n        supported_documents = [\n            doc for doc in documents \n            if doc.file_format in self.enabled_formats\n        ]\n\n        unsupported_documents = [\n            doc for doc in documents \n            if doc.file_format not in self.enabled_formats\n        ]\n\n        if unsupported_documents:\n            self.logger.info(f\"Skipping {len(unsupported_documents)} documents with unsupported formats\")\n\n        if not supported_documents:\n            self.logger.warning(\"No documents with supported formats found\")\n            return documents\n\n        self.logger.info(f\"Extracting metadata from {len(supported_documents)} documents\")\n\n        tasks = [\n            self._extract_metadata_for_document(document) \n            for document in supported_documents\n        ]\n\n        processed_supported = await asyncio.gather(*tasks, return_exceptions=True)\n\n        result_supported = []\n        for i, result in enumerate(processed_supported):\n            if isinstance(result, Exception):\n                self.logger.error(f\"Exception processing {supported_documents[i].filename}: {result}\")\n                result_supported.append(supported_documents[i])\n            else:\n                result_supported.append(result)\n\n        final_result = result_supported + unsupported_documents\n\n        successful_count = sum(1 for doc in result_supported if doc.get_metadata(\"extracted_metadata\"))\n        self.logger.info(f\"Successfully extracted metadata from {successful_count}/{len(supported_documents)} supported documents\")\n\n        if self.export_metadata:\n            await self._export_metadata_to_json(final_result)\n\n        return final_result\n\n    async def _export_metadata_to_json(self, documents: List[Document]) -&gt; None:\n        \"\"\"\n        Export extracted metadata to a JSON file.\n\n        Args:\n            documents: List of processed documents with metadata\n        \"\"\"\n        if not self.metadata_destination.exists():\n            self.logger.info(f\"Creating metadata destination directory: {self.metadata_destination}\")\n            self.metadata_destination.mkdir(parents=True, exist_ok=True)\n\n        metadata_file = self.metadata_destination / self.metadata_filename\n\n        for document in documents:\n            doc_metadata = {\n                \"filename\": document.filename,\n                \"file_path\": str(document.file_path),\n                \"file_format\": document.file_format,\n                \"content_length\": document.content_length,\n                \"has_extracted_metadata\": bool(document.get_metadata(\"extracted_metadata\"))\n            }\n            if document.metadata:\n                if document.file_format == 'pdf': # TO-DO, hacky fix, find a more cleaner solution\n                    file_id = document.filename.removesuffix(\".pdf\")\n                    document.metadata = {\n                        \"extracted_metadata\": {\n                            file_id: document.metadata[\"extracted_metadata\"].get(file_id)\n                        }\n                    }\n                for key, value in document.metadata.items():\n                    doc_metadata[key] = value\n\n            metadata_file = self.metadata_destination / self.metadata_filename\n\n            try:\n                with open(metadata_file, 'a', encoding='utf-8') as f:\n                    json.dump(doc_metadata, f, ensure_ascii=False, default=str)\n                    f.write('\\n')\n\n                self.logger.info(f\"Exported metadata to: {metadata_file}\")\n                self.logger.info(f\"Metadata exported for {len(documents)} documents ({sum(1 for doc in documents if doc.get_metadata('extracted_metadata'))} with extracted metadata)\")\n\n            except Exception as e:\n                self.logger.error(f\"Failed to export metadata to {metadata_file}: {str(e)}\")\n</code></pre>"},{"location":"data-processing/docs/api/stages/#eve.steps.metadata.metadata_step.MetadataStep.__init__","title":"<code>__init__(config)</code>","text":"<p>Initialize the metadata extraction step.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Configuration dictionary with options:</p> <ul> <li>enabled_formats: List of file formats to process (pdf, html, txt, md)</li> <li>fallback_to_filename: Use filename as title fallback</li> <li>debug: Enable debug logging</li> <li>export_metadata: Whether to export metadata to JSON file</li> <li>metadata_destination: Directory to save metadata file</li> <li>metadata_filename: Name of the metadata JSON file   Note: Text formats (txt, md) automatically enable this feature</li> </ul> required Source code in <code>data-processing/eve/steps/metadata/metadata_step.py</code> <pre><code>def __init__(self, config: dict):\n    \"\"\"\n    Initialize the metadata extraction step.\n\n    Args:\n        config: Configuration dictionary with options:\n\n            - enabled_formats: List of file formats to process (pdf, html, txt, md)\n            - fallback_to_filename: Use filename as title fallback\n            - debug: Enable debug logging\n            - export_metadata: Whether to export metadata to JSON file\n            - metadata_destination: Directory to save metadata file\n            - metadata_filename: Name of the metadata JSON file\n              Note: Text formats (txt, md) automatically enable this feature\n    \"\"\"\n    super().__init__(config)\n\n    self.enabled_formats = set(config.get(\"enabled_formats\", [\"pdf\", \"html\", \"txt\", \"md\"]))\n    self.fallback_to_filename = config.get(\"fallback_to_filename\", True)\n    self.export_metadata = config.get(\"export_metadata\", True)\n    self.metadata_destination = Path(config.get(\"metadata_destination\", \"./output\"))\n    self.metadata_filename = config.get(\"metadata_filename\", \"metadata.jsonl\")\n\n    self.extractors = {\n        \"pdf\": PdfMetadataExtractor(debug=self.debug),\n        \"html\": HtmlMetadataExtractor(debug=self.debug)\n    }\n</code></pre>"},{"location":"data-processing/docs/api/stages/#eve.steps.metadata.metadata_step.MetadataStep.execute","title":"<code>execute(documents)</code>  <code>async</code>","text":"<p>Execute metadata extraction on input documents.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>List[Document]</code> <p>List of Document objects to extract metadata from</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>List of Document objects with metadata added</p> Source code in <code>data-processing/eve/steps/metadata/metadata_step.py</code> <pre><code>async def execute(self, documents: List[Document]) -&gt; List[Document]:\n    \"\"\"\n    Execute metadata extraction on input documents.\n\n    Args:\n        documents: List of Document objects to extract metadata from\n\n    Returns:\n        List of Document objects with metadata added\n    \"\"\"\n    if not documents:\n        self.logger.warning(\"No input documents provided to metadata step\")\n        return []\n\n    supported_documents = [\n        doc for doc in documents \n        if doc.file_format in self.enabled_formats\n    ]\n\n    unsupported_documents = [\n        doc for doc in documents \n        if doc.file_format not in self.enabled_formats\n    ]\n\n    if unsupported_documents:\n        self.logger.info(f\"Skipping {len(unsupported_documents)} documents with unsupported formats\")\n\n    if not supported_documents:\n        self.logger.warning(\"No documents with supported formats found\")\n        return documents\n\n    self.logger.info(f\"Extracting metadata from {len(supported_documents)} documents\")\n\n    tasks = [\n        self._extract_metadata_for_document(document) \n        for document in supported_documents\n    ]\n\n    processed_supported = await asyncio.gather(*tasks, return_exceptions=True)\n\n    result_supported = []\n    for i, result in enumerate(processed_supported):\n        if isinstance(result, Exception):\n            self.logger.error(f\"Exception processing {supported_documents[i].filename}: {result}\")\n            result_supported.append(supported_documents[i])\n        else:\n            result_supported.append(result)\n\n    final_result = result_supported + unsupported_documents\n\n    successful_count = sum(1 for doc in result_supported if doc.get_metadata(\"extracted_metadata\"))\n    self.logger.info(f\"Successfully extracted metadata from {successful_count}/{len(supported_documents)} supported documents\")\n\n    if self.export_metadata:\n        await self._export_metadata_to_json(final_result)\n\n    return final_result\n</code></pre>"},{"location":"data-processing/docs/api/stages/#metadata-extractors","title":"Metadata Extractors","text":""},{"location":"data-processing/docs/api/stages/#html-metadata-extractor","title":"HTML Metadata Extractor","text":""},{"location":"data-processing/docs/api/stages/#eve.steps.metadata.extractors.html_extractor.HtmlMetadataExtractor","title":"<code>HtmlMetadataExtractor</code>","text":"<p>Metadata extractor for HTML files and web pages.</p> Source code in <code>data-processing/eve/steps/metadata/extractors/html_extractor.py</code> <pre><code>class HtmlMetadataExtractor():\n    \"\"\"\n    Metadata extractor for HTML files and web pages.\n    \"\"\"\n\n    def __init__(self, debug: bool = False):\n        \"\"\"\n        Initialize the HTML metadata extractor.\n\n        The HTML extractor relies on regex patterns defined in eve.common.regex_patterns\n        for parsing HTML content efficiently without requiring a full HTML parser.\n\n        Args:\n            debug: Enable debug logging for detailed extraction information\n        \"\"\"\n        self.debug = debug\n        self.logger = get_logger(self.__class__.__name__)\n\n    def _clean_title(self, title: str) -&gt; Optional[str]:\n        \"\"\"\n        Clean and normalize a title string.\n\n        Args:\n            title: Raw title string from extracted metadata\n\n        Returns:\n            Cleaned title string, or None if title is invalid\n        \"\"\"\n        if not title or not isinstance(title, str):\n            return None\n\n        # Remove leading/trailing whitespace\n        cleaned = title.strip()\n\n        # Convert newlines and carriage returns to spaces\n        cleaned = cleaned.replace('\\n', ' ').replace('\\r', ' ')\n\n        # Collapse multiple spaces into single spaces\n        while '  ' in cleaned:\n            cleaned = cleaned.replace('  ', ' ')\n\n        # Filter out invalid titles\n        if len(cleaned) &lt; 3 or cleaned.isdigit():\n            return None\n\n        return cleaned\n\n    def _extract_content_with_tags(self, document: Document) -&gt; Optional[str]:\n        with open(document.file_path, 'r', encoding = 'utf-8') as file:\n            html_content = file.read()\n\n        document.content = html_content\n        return document\n\n    def _extract_title_from_html(self, html_content: str) -&gt; Optional[str]:\n        \"\"\"\n        Extract title from HTML &lt;title&gt; tag using regex patterns.\n\n        Args:\n            html_content: Raw HTML content as string\n\n        Returns:\n            Cleaned title string from &lt;title&gt; tag, or None if not found/invalid\n        \"\"\"\n        # Use regex pattern to extract title content\n        title = extract_html_title(html_content)\n\n        if title:\n            # Apply standard title cleaning (whitespace, length validation, etc.)\n            cleaned_title = self._clean_title(title)\n\n            if cleaned_title:\n                self.logger.debug(f\"Extracted title from HTML: {cleaned_title}\")\n                return cleaned_title\n\n        return None\n\n    async def extract_metadata(self, document: Document) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"\n        Extract metadata from an HTML document using multi-source approach.\n\n        Args:\n            document: HTML document to extract metadata from\n\n        Returns:\n            Dictionary containing extracted metadata with fields:\n            - title: Page title (from various sources, with title_source indicator)\n            - title_source: Source of title ('html_tag', 'meta_tag', 'filename')\n            - url: Source URL if available\n            - domain: Domain name from URL\n            - scheme: URL scheme (http/https)\n            - content_length: Length of HTML content\n            - has_content: Boolean indicating content exists\n            - extraction_methods: List containing 'html_parsing'\n\n            Returns None if document format is invalid\n        \"\"\"\n\n        metadata = {}\n\n        document = self._extract_content_with_tags(document) # do this because extraction from previous step removes tag\n\n        extracted_title = self._extract_title_from_html(document.content)\n        metadata['title'] = extracted_title\n        metadata['content_length'] = len(document.content)\n\n        return metadata\n</code></pre>"},{"location":"data-processing/docs/api/stages/#eve.steps.metadata.extractors.html_extractor.HtmlMetadataExtractor.__init__","title":"<code>__init__(debug=False)</code>","text":"<p>Initialize the HTML metadata extractor.</p> <p>The HTML extractor relies on regex patterns defined in eve.common.regex_patterns for parsing HTML content efficiently without requiring a full HTML parser.</p> <p>Parameters:</p> Name Type Description Default <code>debug</code> <code>bool</code> <p>Enable debug logging for detailed extraction information</p> <code>False</code> Source code in <code>data-processing/eve/steps/metadata/extractors/html_extractor.py</code> <pre><code>def __init__(self, debug: bool = False):\n    \"\"\"\n    Initialize the HTML metadata extractor.\n\n    The HTML extractor relies on regex patterns defined in eve.common.regex_patterns\n    for parsing HTML content efficiently without requiring a full HTML parser.\n\n    Args:\n        debug: Enable debug logging for detailed extraction information\n    \"\"\"\n    self.debug = debug\n    self.logger = get_logger(self.__class__.__name__)\n</code></pre>"},{"location":"data-processing/docs/api/stages/#eve.steps.metadata.extractors.html_extractor.HtmlMetadataExtractor.extract_metadata","title":"<code>extract_metadata(document)</code>  <code>async</code>","text":"<p>Extract metadata from an HTML document using multi-source approach.</p> <p>Parameters:</p> Name Type Description Default <code>document</code> <code>Document</code> <p>HTML document to extract metadata from</p> required <p>Returns:</p> Type Description <code>Optional[Dict[str, Any]]</code> <p>Dictionary containing extracted metadata with fields:</p> <code>Optional[Dict[str, Any]]</code> <ul> <li>title: Page title (from various sources, with title_source indicator)</li> </ul> <code>Optional[Dict[str, Any]]</code> <ul> <li>title_source: Source of title ('html_tag', 'meta_tag', 'filename')</li> </ul> <code>Optional[Dict[str, Any]]</code> <ul> <li>url: Source URL if available</li> </ul> <code>Optional[Dict[str, Any]]</code> <ul> <li>domain: Domain name from URL</li> </ul> <code>Optional[Dict[str, Any]]</code> <ul> <li>scheme: URL scheme (http/https)</li> </ul> <code>Optional[Dict[str, Any]]</code> <ul> <li>content_length: Length of HTML content</li> </ul> <code>Optional[Dict[str, Any]]</code> <ul> <li>has_content: Boolean indicating content exists</li> </ul> <code>Optional[Dict[str, Any]]</code> <ul> <li>extraction_methods: List containing 'html_parsing'</li> </ul> <code>Optional[Dict[str, Any]]</code> <p>Returns None if document format is invalid</p> Source code in <code>data-processing/eve/steps/metadata/extractors/html_extractor.py</code> <pre><code>async def extract_metadata(self, document: Document) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"\n    Extract metadata from an HTML document using multi-source approach.\n\n    Args:\n        document: HTML document to extract metadata from\n\n    Returns:\n        Dictionary containing extracted metadata with fields:\n        - title: Page title (from various sources, with title_source indicator)\n        - title_source: Source of title ('html_tag', 'meta_tag', 'filename')\n        - url: Source URL if available\n        - domain: Domain name from URL\n        - scheme: URL scheme (http/https)\n        - content_length: Length of HTML content\n        - has_content: Boolean indicating content exists\n        - extraction_methods: List containing 'html_parsing'\n\n        Returns None if document format is invalid\n    \"\"\"\n\n    metadata = {}\n\n    document = self._extract_content_with_tags(document) # do this because extraction from previous step removes tag\n\n    extracted_title = self._extract_title_from_html(document.content)\n    metadata['title'] = extracted_title\n    metadata['content_length'] = len(document.content)\n\n    return metadata\n</code></pre>"},{"location":"data-processing/docs/api/stages/#pdf-metadata-extractor","title":"PDF Metadata Extractor","text":"<p>PDF metadata extractor is a two stage process.</p> <ol> <li>Extract content using monkeyocr.</li> <li>Use crossref to extract metadata from the content.</li> </ol>"},{"location":"data-processing/docs/api/stages/#eve.steps.metadata.extractors.pdf_extractor.PdfMetadataExtractor","title":"<code>PdfMetadataExtractor</code>","text":"Source code in <code>data-processing/eve/steps/metadata/extractors/pdf_extractor.py</code> <pre><code>class PdfMetadataExtractor():\n\n    def __init__(self, debug: bool = False):\n        \"\"\"\n        Initialize the PDF metadata extractor.\n\n        Args:\n            debug: Enable debug logging for detailed extraction information\n        \"\"\"\n        self.debug = debug\n        self.logger = get_logger(self.__class__.__name__)\n\n    @staticmethod\n    def _safe_str(value):\n        if isinstance(value, list):\n            return value[0] if value else None\n        if isinstance(value, dict):\n            return json.dumps(value)\n        return str(value) if value not in (None, \"\", []) else None\n\n    @staticmethod\n    def _extract_identifier(main_dir, sub_dir):\n        md_path = f\"{main_dir}/{sub_dir}/{sub_dir}.md\"\n        try:\n            with open(md_path, 'r', encoding=\"utf-8\", errors=\"ignore\") as f:\n                content = f.read()\n        except Exception:\n            return \"NA\", None\n\n        for pattern in doi_regexp:\n            match = re.findall(pattern, content, re.I)\n            if match:\n                return \"doi\", match[0]\n\n        for pattern in arxiv_regexp:\n            match = re.findall(pattern, content, re.I)\n            if match:\n                return \"arxiv\", match[0]\n\n        for pattern in isbn_regexp:\n            match = re.findall(pattern, content, re.I)\n            if match:\n                return \"isbn\", match[0]\n\n        return \"NA\", None\n\n    @staticmethod\n    def _extract_title(main_dir, sub_dir):\n        json_path = os.path.join(main_dir, sub_dir, f\"{sub_dir}_content_list.json\")\n        try:\n            with open(json_path, \"r\", encoding=\"utf-8\") as f:\n                data = json.load(f)\n            candidates = [row[\"text\"] for row in data if row.get(\"text_level\", 0) == 1]\n            return max(candidates, key=len) if candidates else \"NA\"\n        except Exception:\n            return \"NA\"\n\n    @staticmethod\n    async def _fetch_json(client, url):\n        try:\n            resp = await client.get(url, timeout=50) # \n            resp.raise_for_status()\n            return resp.json()\n        except Exception:\n            return None\n\n    async def _fetch_crossref_by_doi(self, client, doi):\n        data = await self._fetch_json(client, f\"https://api.crossref.org/works/{doi}\")\n        if not data:\n            return None\n        item = data.get(\"message\", {})\n\n        return {\n            \"title\": self._safe_str(item.get(\"title\")),\n            \"authors\": \", \".join(\n                f\"{a.get('given', '')} {a.get('family', '')}\".strip()\n                for a in item.get(\"author\", [])\n                if isinstance(a, dict)\n            ) or None,\n            \"year\": self._safe_str(item.get(\"issued\", {}).get(\"date-parts\", [[None]])[0]),\n            \"publisher\": self._safe_str(item.get(\"publisher\")),\n            \"journal\": self._safe_str(item.get(\"container-title\")),\n            \"pub_url\": self._safe_str(item.get(\"URL\")),\n            \"doi\": self._safe_str(item.get(\"DOI\")),\n            \"citation_count\": item.get(\"is-referenced-by-count\")\n        }\n\n\n    async def _fetch_crossref_by_title(self, client, title):\n        q = re.sub(r\"[\\s]+\", \"+\", title)\n        data = await self._fetch_json(client, f\"https://api.crossref.org/works?query.bibliographic={q}&amp;rows=1\")\n        if not data:\n            return None\n        items = data.get(\"message\", {}).get(\"items\", [])\n        if not items:\n            return None\n        item = items[0]\n\n        return {\n            \"title\": self._safe_str(item.get(\"title\")),\n            \"authors\": \", \".join(\n                f\"{a.get('given', '')} {a.get('family', '')}\".strip()\n                for a in item.get(\"author\", [])\n                if isinstance(a, dict)\n            ) or None,\n            \"year\": self._safe_str(item.get(\"issued\", {}).get(\"date-parts\", [[None]])[0]),\n            \"publisher\": self._safe_str(item.get(\"publisher\")),\n            \"journal\": self._safe_str(item.get(\"container-title\")),\n            \"pub_url\": self._safe_str(item.get(\"URL\")),\n            \"doi\": self._safe_str(item.get(\"DOI\")),\n        }\n\n\n    async def fetch_doi_from_arxiv(self, client, arxiv_id):\n        data = await self._fetch_json(client, f\"https://api.crossref.org/works?query={arxiv_id}\")\n        if not data:\n            return None\n        items = data.get(\"message\", {}).get(\"items\", [])\n        return self._safe_str(items[0].get(\"DOI\")) if items else None\n\n    async def _extract_metadata(self, sub_dir, main_dir, client, sem):\n        async with sem:\n            id_type, identifier = self._extract_identifier(main_dir, sub_dir)\n            meta = None\n\n            if id_type == \"doi\":\n                meta = await self._fetch_crossref_by_doi(client, identifier)\n            elif id_type == \"arxiv\":\n                doi = await self._fetch_doi_from_arxiv(client, identifier)\n                if doi:\n                    meta = await self._fetch_crossref_by_doi(client, doi)\n\n            title = self._extract_title(main_dir, sub_dir)\n            if not meta and title != \"NA\":\n                meta = await self._fetch_crossref_by_title(client, title)\n\n            return {\n                \"sub_dir\": sub_dir,\n                \"id_type\": id_type,\n                \"identifier\": identifier,\n                \"title_extracted\": title,\n                \"meta\": meta,\n            }\n\n    async def extract_metadata(self, document: Document) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"\n        Extract metadata from a PDF document using crossref.\n\n        Args:\n            document: PDF document to extract metadata from\n\n        Returns:\n            Dictionary containing extracted metadata with fields:\n            - title: Document title\n            - authors: List of author names \n            - year: Publication year\n            - journal: Publication venue\n            - doi: Digital Object Identifier\n            - identifier: Document identifier (DOI, arXiv, etc.)\n\n            Returns None if document format is invalid\n        \"\"\"\n\n        metadata = {}\n        done = set()\n\n        subdirs = [d for d in os.listdir(main_dir)\n                if os.path.isdir(os.path.join(main_dir, d)) and d not in done]\n\n        if not subdirs:\n            print(\"All subdirectories already processed.\")\n            return\n\n        sem = asyncio.Semaphore(MAX_CONCURRENT)\n        async with httpx.AsyncClient() as client:\n            tasks = [self._extract_metadata(d, main_dir, client, sem) for d in subdirs]\n\n            for coro in tqdm(asyncio.as_completed(tasks), total=len(tasks)):\n                try:\n                    r = await coro\n                    metadata[r.get(\"sub_dir\")] = r.get(\"meta\")\n                except Exception as e:\n                    print(f\"Failed on {coro}: {e}\")\n\n        return metadata\n</code></pre>"},{"location":"data-processing/docs/api/stages/#eve.steps.metadata.extractors.pdf_extractor.PdfMetadataExtractor.__init__","title":"<code>__init__(debug=False)</code>","text":"<p>Initialize the PDF metadata extractor.</p> <p>Parameters:</p> Name Type Description Default <code>debug</code> <code>bool</code> <p>Enable debug logging for detailed extraction information</p> <code>False</code> Source code in <code>data-processing/eve/steps/metadata/extractors/pdf_extractor.py</code> <pre><code>def __init__(self, debug: bool = False):\n    \"\"\"\n    Initialize the PDF metadata extractor.\n\n    Args:\n        debug: Enable debug logging for detailed extraction information\n    \"\"\"\n    self.debug = debug\n    self.logger = get_logger(self.__class__.__name__)\n</code></pre>"},{"location":"data-processing/docs/api/stages/#eve.steps.metadata.extractors.pdf_extractor.PdfMetadataExtractor.extract_metadata","title":"<code>extract_metadata(document)</code>  <code>async</code>","text":"<p>Extract metadata from a PDF document using crossref.</p> <p>Parameters:</p> Name Type Description Default <code>document</code> <code>Document</code> <p>PDF document to extract metadata from</p> required <p>Returns:</p> Type Description <code>Optional[Dict[str, Any]]</code> <p>Dictionary containing extracted metadata with fields:</p> <code>Optional[Dict[str, Any]]</code> <ul> <li>title: Document title</li> </ul> <code>Optional[Dict[str, Any]]</code> <ul> <li>authors: List of author names </li> </ul> <code>Optional[Dict[str, Any]]</code> <ul> <li>year: Publication year</li> </ul> <code>Optional[Dict[str, Any]]</code> <ul> <li>journal: Publication venue</li> </ul> <code>Optional[Dict[str, Any]]</code> <ul> <li>doi: Digital Object Identifier</li> </ul> <code>Optional[Dict[str, Any]]</code> <ul> <li>identifier: Document identifier (DOI, arXiv, etc.)</li> </ul> <code>Optional[Dict[str, Any]]</code> <p>Returns None if document format is invalid</p> Source code in <code>data-processing/eve/steps/metadata/extractors/pdf_extractor.py</code> <pre><code>async def extract_metadata(self, document: Document) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"\n    Extract metadata from a PDF document using crossref.\n\n    Args:\n        document: PDF document to extract metadata from\n\n    Returns:\n        Dictionary containing extracted metadata with fields:\n        - title: Document title\n        - authors: List of author names \n        - year: Publication year\n        - journal: Publication venue\n        - doi: Digital Object Identifier\n        - identifier: Document identifier (DOI, arXiv, etc.)\n\n        Returns None if document format is invalid\n    \"\"\"\n\n    metadata = {}\n    done = set()\n\n    subdirs = [d for d in os.listdir(main_dir)\n            if os.path.isdir(os.path.join(main_dir, d)) and d not in done]\n\n    if not subdirs:\n        print(\"All subdirectories already processed.\")\n        return\n\n    sem = asyncio.Semaphore(MAX_CONCURRENT)\n    async with httpx.AsyncClient() as client:\n        tasks = [self._extract_metadata(d, main_dir, client, sem) for d in subdirs]\n\n        for coro in tqdm(asyncio.as_completed(tasks), total=len(tasks)):\n            try:\n                r = await coro\n                metadata[r.get(\"sub_dir\")] = r.get(\"meta\")\n            except Exception as e:\n                print(f\"Failed on {coro}: {e}\")\n\n    return metadata\n</code></pre>"},{"location":"data-processing/docs/api/stages/#export-stage","title":"Export Stage","text":"<p>Saves processed documents to output formats.</p>"},{"location":"data-processing/docs/api/stages/#eve.steps.export.export_step.ExportStep","title":"<code>ExportStep</code>","text":"<p>               Bases: <code>PipelineStep</code></p> Source code in <code>data-processing/eve/steps/export/export_step.py</code> <pre><code>class ExportStep(PipelineStep):\n\n    def __init__(self, config: dict, name: str = \"ExportStep\"):\n        \"\"\"Initialize the export step.\n\n        Args:\n            config: Configuration containing:\n\n                - output_dir: Output directory path\n                - format: Output format (jsonl, md, etc.)\n                - resume: Whether to enable resume functionality (default: False)\n            name: Name for logging purposes\n        \"\"\"\n        super().__init__(config, name)\n\n        # Initialize checkpoint manager if resume is enabled\n        self.resume = config.get(\"resume\", False)\n        output_dir = Path(config.get(\"output_dir\", \"./output\"))\n\n        if self.resume:\n            self.checkpoint = CheckpointManager(output_dir, resume=True)\n            stats = self.checkpoint.get_stats()\n            self.logger.info(f\"Resume mode enabled: {stats['processed_count']} documents already processed\")\n        else:\n            self.checkpoint = None\n\n    async def export_jsonl(self, documents: List[Document]) -&gt; List[Document]:\n        output_dir = Path(self.config.get(\"output_dir\", \"./output\"))\n        result = []\n\n        if not output_dir.exists():\n            self.logger.info(f\"{output_dir} does not exist. creating...\")\n            output_dir.mkdir(parents=True, exist_ok=True)\n\n        for document in documents:\n            output_file = (\n                output_dir\n                / f\"{Path(document.filename).stem}.{self.config.get('format', 'md')}\"\n            )\n            async with aiofiles.open(output_file, \"a+\", encoding=\"utf-8\") as f:\n                await f.write(json.dumps(document.__dict__()))\n                await f.write(\"\\n\")\n\n            # Mark as processed in checkpoint\n            if self.checkpoint:\n                self.checkpoint.mark_processed(document)\n\n            result.append(document)\n        return result\n\n    async def export_md(self, documents: List[Document]) -&gt; List[Document]:\n        output_dir = Path(self.config.get(\"output_dir\", \"./output\"))\n        result = []\n        if not output_dir.exists():\n            self.logger.info(f\"{output_dir} does not exist. creating...\")\n            output_dir.mkdir(parents=True, exist_ok=True)\n        for document in documents:\n            output_file = (\n                output_dir\n                / f\"{Path(document.filename).stem}.{self.config.get('format', 'md')}\"\n            )\n            async with aiofiles.open(output_file, \"a+\", encoding=\"utf-8\") as f:\n                await f.write(json.dumps(document.content))\n                await f.write(\"\\n\")\n            self.logger.info(f\"Saved file: {output_file}\")\n\n            # Mark as processed in checkpoint\n            if self.checkpoint:\n                self.checkpoint.mark_processed(document)\n\n            result.append(document)\n        return result\n\n    async def dummy_export(self, documents: List[Document]) -&gt; List[Document]:\n        return documents\n\n    async def execute(self, documents: List[Document]) -&gt; List[Document]:\n        # Note: Documents are already filtered in create_batches() when resume is enabled\n        # No need to filter again here to avoid memory overhead\n\n        format = self.config.get(\"format\", \"md\")\n        if format == \"jsonl\":\n            result = await self.export_jsonl(documents)\n        elif format == \"dummy\":\n            result = await self.dummy_export(documents)\n        else:\n            result = await self.export_md(documents)\n\n        return result\n</code></pre>"},{"location":"data-processing/docs/api/stages/#eve.steps.export.export_step.ExportStep.__init__","title":"<code>__init__(config, name='ExportStep')</code>","text":"<p>Initialize the export step.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Configuration containing:</p> <ul> <li>output_dir: Output directory path</li> <li>format: Output format (jsonl, md, etc.)</li> <li>resume: Whether to enable resume functionality (default: False)</li> </ul> required <code>name</code> <code>str</code> <p>Name for logging purposes</p> <code>'ExportStep'</code> Source code in <code>data-processing/eve/steps/export/export_step.py</code> <pre><code>def __init__(self, config: dict, name: str = \"ExportStep\"):\n    \"\"\"Initialize the export step.\n\n    Args:\n        config: Configuration containing:\n\n            - output_dir: Output directory path\n            - format: Output format (jsonl, md, etc.)\n            - resume: Whether to enable resume functionality (default: False)\n        name: Name for logging purposes\n    \"\"\"\n    super().__init__(config, name)\n\n    # Initialize checkpoint manager if resume is enabled\n    self.resume = config.get(\"resume\", False)\n    output_dir = Path(config.get(\"output_dir\", \"./output\"))\n\n    if self.resume:\n        self.checkpoint = CheckpointManager(output_dir, resume=True)\n        stats = self.checkpoint.get_stats()\n        self.logger.info(f\"Resume mode enabled: {stats['processed_count']} documents already processed\")\n    else:\n        self.checkpoint = None\n</code></pre>"},{"location":"data-processing/docs/api/utilities/","title":"Utilities","text":"<p>This section covers utility functions and helper modules used throughout the pipeline.</p>"},{"location":"data-processing/docs/api/utilities/#common-utilities","title":"Common Utilities","text":"<p>General-purpose utility functions.</p>"},{"location":"data-processing/docs/api/utilities/#eve.utils.read_in_chunks","title":"<code>read_in_chunks(file_path, mode, chunk_size=4096)</code>  <code>async</code>","text":"<p>read a binary file in chunks.</p> Source code in <code>data-processing/eve/utils.py</code> <pre><code>async def read_in_chunks(file_path: Path, mode: str, chunk_size: int = 4096) -&gt; AsyncGenerator[bytes, None]:\n    \"\"\"\n    read a binary file in chunks.\n    \"\"\"\n    async with aiofiles.open(file_path, mode) as f:\n        while chunk := await f.read(chunk_size):\n            yield chunk\n</code></pre>"},{"location":"data-processing/docs/api/utilities/#http-utils","title":"HTTP Utils","text":"<p>HTTP client utilities for server-based processing.</p> <p>Common HTTP utilities for making API calls across the pipeline.</p>"},{"location":"data-processing/docs/api/utilities/#eve.common.http_utils.make_openrouter_request","title":"<code>make_openrouter_request(api_key, model, prompt, max_tokens=1000, temperature=0.1)</code>  <code>async</code>","text":"<p>Make a request to OpenRouter API for LLM completion.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>OpenRouter API key</p> required <code>model</code> <code>str</code> <p>Model name to use</p> required <code>prompt</code> <code>str</code> <p>The prompt to send</p> required <code>max_tokens</code> <code>int</code> <p>Maximum tokens in response</p> <code>1000</code> <code>temperature</code> <code>float</code> <p>Temperature for response generation</p> <code>0.1</code> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Response content or None if request failed</p> Source code in <code>data-processing/eve/common/http_utils.py</code> <pre><code>async def make_openrouter_request(\n    api_key: str,\n    model: str,\n    prompt: str,\n    max_tokens: int = 1000,\n    temperature: float = 0.1\n) -&gt; Optional[str]:\n    \"\"\"\n    Make a request to OpenRouter API for LLM completion.\n\n    Args:\n        api_key: OpenRouter API key\n        model: Model name to use\n        prompt: The prompt to send\n        max_tokens: Maximum tokens in response\n        temperature: Temperature for response generation\n\n    Returns:\n        Response content or None if request failed\n    \"\"\"\n    url = \"https://openrouter.ai/api/v1/chat/completions\"\n    headers = {\n        \"Authorization\": f\"Bearer {api_key}\",\n        \"Content-Type\": \"application/json\"\n    }\n    data = {\n        \"model\": model,\n        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n        \"max_tokens\": max_tokens,\n        \"temperature\": temperature\n    }\n\n    response = await post_request(url, headers, data)\n    if response and \"choices\" in response:\n        content = response[\"choices\"][0][\"message\"][\"content\"].strip()\n        import re\n        content = re.sub(r'^```latex\\n?', '', content)\n        content = re.sub(r'\\n?```$', '', content)\n        return content.strip()\n\n    return None\n</code></pre>"},{"location":"data-processing/docs/api/utilities/#eve.common.http_utils.post_request","title":"<code>post_request(url, headers, data, timeout=30)</code>  <code>async</code>","text":"<p>Make an async POST request and return JSON response.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL to make the request to</p> required <code>headers</code> <code>Dict[str, str]</code> <p>Request headers</p> required <code>data</code> <code>Dict[str, Any]</code> <p>Request data to send as JSON</p> required <code>timeout</code> <code>int</code> <p>Request timeout in seconds</p> <code>30</code> <p>Returns:</p> Type Description <code>Optional[Dict[str, Any]]</code> <p>Response JSON data or None if request failed</p> Source code in <code>data-processing/eve/common/http_utils.py</code> <pre><code>async def post_request(\n    url: str,\n    headers: Dict[str, str],\n    data: Dict[str, Any],\n    timeout: int = 30\n) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"\n    Make an async POST request and return JSON response.\n\n    Args:\n        url: The URL to make the request to\n        headers: Request headers\n        data: Request data to send as JSON\n        timeout: Request timeout in seconds\n\n    Returns:\n        Response JSON data or None if request failed\n    \"\"\"\n    try:\n        async with aiohttp.ClientSession() as session:\n            async with session.post(\n                url, \n                headers=headers, \n                json=data,\n                timeout=aiohttp.ClientTimeout(total=timeout)\n            ) as response:\n                if response.status == 200:\n                    return await response.json()\n                else:\n                    logger.error(f\"HTTP request failed with status {response.status}\")\n                    return None\n    except Exception as e:\n        logger.error(f\"HTTP request failed: {str(e)}\")\n        return None\n</code></pre>"},{"location":"data-processing/docs/api/utilities/#regex-patterns","title":"Regex Patterns","text":"<p>Common regular expression patterns used throughout the pipeline.</p> <p>Common regex patterns used across the pipeline.</p>"},{"location":"data-processing/docs/api/utilities/#eve.common.regex_patterns.clean_doubled_backslashes","title":"<code>clean_doubled_backslashes(text)</code>","text":"<p>Clean up doubled backslashes in LaTeX content.</p> Source code in <code>data-processing/eve/common/regex_patterns.py</code> <pre><code>def clean_doubled_backslashes(text: str) -&gt; str:\n    \"\"\"Clean up doubled backslashes in LaTeX content.\"\"\"\n    return DOUBLED_BACKSLASH_PATTERN.sub(lambda m: '\\\\' * (len(m.group()) // 2), text)\n</code></pre>"},{"location":"data-processing/docs/api/utilities/#eve.common.regex_patterns.extract_html_meta_tags","title":"<code>extract_html_meta_tags(html_content)</code>","text":"<p>Extract metadata from HTML meta tags.</p> <p>Parameters:</p> Name Type Description Default <code>html_content</code> <code>str</code> <p>HTML content as string</p> required <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>Dictionary containing extracted meta tag information</p> Source code in <code>data-processing/eve/common/regex_patterns.py</code> <pre><code>def extract_html_meta_tags(html_content: str) -&gt; dict[str, str]:\n    \"\"\"\n    Extract metadata from HTML meta tags.\n\n    Args:\n        html_content: HTML content as string\n\n    Returns:\n        Dictionary containing extracted meta tag information\n    \"\"\"\n    meta_data = {}\n\n    if not html_content:\n        return meta_data\n\n    meta_patterns = {\n        'description': r'&lt;meta[^&gt;]*name=[\"\\']description[\"\\'][^&gt;]*content=[\"\\']([^\"\\']*)[\"\\']',\n        'keywords': r'&lt;meta[^&gt;]*name=[\"\\']keywords[\"\\'][^&gt;]*content=[\"\\']([^\"\\']*)[\"\\']',\n        'author': r'&lt;meta[^&gt;]*name=[\"\\']author[\"\\'][^&gt;]*content=[\"\\']([^\"\\']*)[\"\\']',\n        'og_title': r'&lt;meta[^&gt;]*property=[\"\\']og:title[\"\\'][^&gt;]*content=[\"\\']([^\"\\']*)[\"\\']',\n        'og_description': r'&lt;meta[^&gt;]*property=[\"\\']og:description[\"\\'][^&gt;]*content=[\"\\']([^\"\\']*)[\"\\']',\n        'twitter_title': r'&lt;meta[^&gt;]*name=[\"\\']twitter:title[\"\\'][^&gt;]*content=[\"\\']([^\"\\']*)[\"\\']',\n    }\n\n    for key, pattern in meta_patterns.items():\n        match = re.search(pattern, html_content, re.IGNORECASE)\n        if match:\n            value = match.group(1).strip()\n            if value:\n                meta_data[key] = value\n\n    return meta_data\n</code></pre>"},{"location":"data-processing/docs/api/utilities/#eve.common.regex_patterns.extract_html_title","title":"<code>extract_html_title(html_content)</code>","text":"<p>Extract title from HTML content.</p> <p>Parameters:</p> Name Type Description Default <code>html_content</code> <code>str</code> <p>HTML content as string</p> required <p>Returns:</p> Type Description <code>str</code> <p>Extracted and cleaned title, or None if not found</p> Source code in <code>data-processing/eve/common/regex_patterns.py</code> <pre><code>def extract_html_title(html_content: str) -&gt; str:\n    \"\"\"\n    Extract title from HTML content.\n\n    Args:\n        html_content: HTML content as string\n\n    Returns:\n        Extracted and cleaned title, or None if not found\n    \"\"\"\n    if not html_content:\n        return None\n\n    title_match = HTML_TITLE_PATTERN.search(html_content)\n\n    if title_match:\n        title = title_match.group(1)\n\n        title = HTML_TAG_PATTERN.sub('', title)\n        title = HTML_ENTITY_PATTERN.sub(' ', title)\n        title = HTML_NUMERIC_ENTITY_PATTERN.sub(' ', title)\n\n        return title.strip()\n\n    return None\n</code></pre>"},{"location":"data-processing/docs/api/utilities/#eve.common.regex_patterns.extract_json_ld_count","title":"<code>extract_json_ld_count(html_content)</code>","text":"<p>Count JSON-LD structured data blocks in HTML.</p> <p>Parameters:</p> Name Type Description Default <code>html_content</code> <code>str</code> <p>HTML content as string</p> required <p>Returns:</p> Type Description <code>int</code> <p>Number of JSON-LD script blocks found</p> Source code in <code>data-processing/eve/common/regex_patterns.py</code> <pre><code>def extract_json_ld_count(html_content: str) -&gt; int:\n    \"\"\"\n    Count JSON-LD structured data blocks in HTML.\n\n    Args:\n        html_content: HTML content as string\n\n    Returns:\n        Number of JSON-LD script blocks found\n    \"\"\"\n    if not html_content:\n        return 0\n\n    json_ld_matches = JSON_LD_SCRIPT_PATTERN.findall(html_content)\n    return len(json_ld_matches)\n</code></pre>"},{"location":"data-processing/docs/api/utilities/#eve.common.regex_patterns.fix_ocr_digit_letter_spacing","title":"<code>fix_ocr_digit_letter_spacing(text)</code>","text":"<p>Fix OCR issues where digits are concatenated with letters.</p> Source code in <code>data-processing/eve/common/regex_patterns.py</code> <pre><code>def fix_ocr_digit_letter_spacing(text: str) -&gt; str:\n    \"\"\"Fix OCR issues where digits are concatenated with letters.\"\"\"\n    return DIGIT_LETTER_PATTERN.sub(r'\\1 \\2', text)\n</code></pre>"},{"location":"data-processing/docs/api/utilities/#eve.common.regex_patterns.get_latex_formula_patterns","title":"<code>get_latex_formula_patterns()</code>","text":"<p>Get all LaTeX formula patterns in a dictionary.</p> <p>Returns:</p> Type Description <code>dict[str, Pattern[str]]</code> <p>Dictionary mapping pattern names to compiled regex patterns</p> Source code in <code>data-processing/eve/common/regex_patterns.py</code> <pre><code>def get_latex_formula_patterns() -&gt; dict[str, Pattern[str]]:\n    \"\"\"\n    Get all LaTeX formula patterns in a dictionary.\n\n    Returns:\n        Dictionary mapping pattern names to compiled regex patterns\n    \"\"\"\n    return {\n        'inline': INLINE_MATH_PATTERN,\n        'display': DISPLAY_MATH_PATTERN,\n        'bracket': BRACKET_MATH_PATTERN,\n        'square_bracket': SQUARE_BRACKET_MATH_PATTERN,\n        'environment': LATEX_ENV_PATTERN\n    }\n</code></pre>"},{"location":"data-processing/docs/api/utilities/#eve.common.regex_patterns.normalize_excessive_newlines","title":"<code>normalize_excessive_newlines(text)</code>","text":"<p>Replace 3+ consecutive newlines with exactly 2.</p> Source code in <code>data-processing/eve/common/regex_patterns.py</code> <pre><code>def normalize_excessive_newlines(text: str) -&gt; str:\n    \"\"\"Replace 3+ consecutive newlines with exactly 2.\"\"\"\n    return EXCESSIVE_NEWLINES_PATTERN.sub('\\n\\n', text)\n</code></pre>"},{"location":"data-processing/docs/api/utilities/#eve.common.regex_patterns.remove_nougat_artifacts","title":"<code>remove_nougat_artifacts(text)</code>","text":"<p>Remove Nougat-specific warning and error artifacts.</p> Source code in <code>data-processing/eve/common/regex_patterns.py</code> <pre><code>def remove_nougat_artifacts(text: str) -&gt; str:\n    \"\"\"Remove Nougat-specific warning and error artifacts.\"\"\"\n    text = WARNING_PATTERN.sub('', text)\n    text = ERROR_PATTERN.sub('', text)\n    text = text.replace('[MISSING_PAGE_POST]', '')\n    return text\n</code></pre>"},{"location":"data-processing/docs/api/utilities/#eve.common.regex_patterns.remove_single_symbol_lines","title":"<code>remove_single_symbol_lines(text)</code>","text":"<p>Remove lines that contain only a single symbol or punctuation.</p> Source code in <code>data-processing/eve/common/regex_patterns.py</code> <pre><code>def remove_single_symbol_lines(text: str) -&gt; str:\n    \"\"\"Remove lines that contain only a single symbol or punctuation.\"\"\"\n    lines = text.split('\\n')\n    cleaned_lines = []\n\n    for line in lines:\n        stripped = line.strip()\n        if re.search(r'\\w', stripped) or len(stripped) != 1:\n            cleaned_lines.append(line)\n\n    return '\\n'.join(cleaned_lines)\n</code></pre>"},{"location":"data-processing/docs/api/utilities/#prompts","title":"Prompts","text":"<p>Prompt templates used in LLM-based processing.</p> <p>Common prompts used across the pipeline.</p>"},{"location":"data-processing/docs/api/utilities/#eve.common.prompts.get_latex_correction_prompt","title":"<code>get_latex_correction_prompt(formula_type, error_message, formula, context)</code>","text":"<p>Generate a LaTeX correction prompt.</p> <p>Parameters:</p> Name Type Description Default <code>formula_type</code> <code>str</code> <p>Type of LaTeX formula (inline, display, etc.)</p> required <code>error_message</code> <code>str</code> <p>The error message from LaTeX compilation</p> required <code>formula</code> <code>str</code> <p>The problematic formula</p> required <code>context</code> <code>str</code> <p>Surrounding context for better understanding</p> required <p>Returns:</p> Type Description <code>str</code> <p>Formatted prompt string</p> Source code in <code>data-processing/eve/common/prompts.py</code> <pre><code>def get_latex_correction_prompt(\n    formula_type: str,\n    error_message: str,\n    formula: str,\n    context: str\n) -&gt; str:\n    \"\"\"\n    Generate a LaTeX correction prompt.\n\n    Args:\n        formula_type: Type of LaTeX formula (inline, display, etc.)\n        error_message: The error message from LaTeX compilation\n        formula: The problematic formula\n        context: Surrounding context for better understanding\n\n    Returns:\n        Formatted prompt string\n    \"\"\"\n    context_snippet = context[:1000] + \"...\" if len(context) &gt; 1000 else context\n\n    return LATEX_CORRECTION_PROMPT.format(\n        formula_type=formula_type,\n        error_message=error_message,\n        formula=formula,\n        context_snippet=context_snippet\n    )\n</code></pre>"},{"location":"data-processing/docs/api/utilities/#logging","title":"Logging","text":"<p>Logging configuration and utilities.</p>"},{"location":"data-processing/docs/examples/basic-usage/","title":"Basic Usage Examples","text":"<p>This section provides practical examples of using the EVE Pipeline for common document processing tasks.</p>"},{"location":"data-processing/docs/examples/basic-usage/#simple-document-processing","title":"Simple Document Processing","text":"<p>Process all documents from an input directory and export to markdown:</p> <pre><code># config.yaml\npipeline:\n  batch_size: 10\n  inputs:\n    path: \"input_documents\"\n  stages:\n    - name: extraction\n    - name: export\n      config:\n        format: \"md\"\n        destination: \"output\"\n</code></pre> <pre><code># Run the pipeline\neve run\n</code></pre>"},{"location":"data-processing/docs/examples/basic-usage/#pdf-processing-pipeline","title":"PDF Processing Pipeline","text":"<p>Process PDF documents with cleaning and deduplication:</p> <pre><code>pipeline:\n  batch_size: 5\n  inputs:\n    path: \"research_papers\"\n  stages:\n    - name: extraction\n      config:\n        format: \"pdf\"\n    - name: duplication\n    - name: cleaning\n    - name: export\n      config:\n        format: \"md\"\n        destination: \"processed_papers\"\n</code></pre>"},{"location":"data-processing/docs/examples/basic-usage/#web-content-processing","title":"Web Content Processing","text":"<p>Process HTML documents with PII removal:</p> <pre><code>pipeline:\n  batch_size: 20\n  inputs:\n    path: \"web_pages\"\n  stages:\n    - name: extraction\n      config:\n        format: \"html\"\n    - name: pii\n      config:\n        url: \"http://127.0.0.1:8000\"\n    - name: export\n      config:\n        format: \"txt\"\n        destination: \"clean_content\"\n</code></pre>"},{"location":"data-processing/docs/examples/basic-usage/#advanced-pipeline-with-all-features","title":"Advanced Pipeline with All Features","text":"<p>Complete pipeline for scientific document processing:</p> <pre><code>pipeline:\n  batch_size: 10\n  inputs:\n    path: \"scientific_documents\"\n  stages:\n    - name: extraction\n      config:\n        url: \"http://127.0.0.1:8001\"\n    - name: duplication\n      config:\n        method: \"lsh\"\n        shingle_size: 3\n        num_perm: 128\n        threshold: 0.85\n    - name: cleaning\n      config:\n        ocr_threshold: 0.99\n        enable_latex_correction: true\n        debug: true\n    - name: pii\n      config:\n        url: \"http://127.0.0.1:8000\"\n    - name: metadata\n      config:\n    - name: export\n      config:\n        export_metadata: true\n        metadata_destination: \"./output\"\n</code></pre>"},{"location":"data-processing/docs/examples/basic-usage/#mixed-format-processing","title":"Mixed Format Processing","text":"<p>Process different document types in the same pipeline:</p> <pre><code>pipeline:\n  batch_size: 10\n  inputs:\n    path: \"mixed_documents\"\n  stages:\n    - name: extraction\n      # Auto-detect format based on file extension\n    - name: duplication\n      config:\n        method: \"lsh\"\n        threshold: 0.8\n    - name: cleaning\n    - name: export\n      config:\n        format: \"md\"\n        destination: \"unified_output\"\n</code></pre>"},{"location":"data-processing/docs/examples/basic-usage/#process-and-upload-to-qdrant","title":"Process and Upload to Qdrant","text":"<p>This example demonstrates a complete pipeline that extracts, chunks, filters, embeds, and uploads documents to Qdrant in one workflow.</p> <p>Prerequisites: - VLLM server running for embeddings: <code>python server/vllm.py</code> - Qdrant instance running: <code>docker run -p 6333:6333 qdrant/qdrant</code></p>"},{"location":"data-processing/docs/examples/basic-usage/#understanding-jsonl-input-format","title":"Understanding JSONL Input Format","text":"<p>The pipeline accepts JSONL (JSON Lines) files where each line is a JSON document. The extractor recognizes the following fields:</p> <p>Required Fields: - <code>content</code> (string): The document text content</p> <p>Optional Fields: - <code>metadata</code> (object): Custom metadata that will be preserved throughout the pipeline - <code>embedding</code> (array): Pre-computed embedding vector (if using <code>use_existing_embeddings: true</code>) - <code>pipeline_metadata</code> (object): Internal metadata from previous pipeline runs</p> <p>Example JSONL file:</p> <pre><code>{\"content\": \"This is the first document.\", \"metadata\": {\"title\": \"Doc 1\", \"author\": \"John Doe\", \"year\": 2024}}\n{\"content\": \"This is the second document.\", \"metadata\": {\"title\": \"Doc 2\", \"source\": \"research_paper.pdf\"}}\n{\"content\": \"Third document with embedding.\", \"metadata\": {\"title\": \"Doc 3\"}, \"embedding\": [0.123, 0.456, ...]}\n</code></pre> <p>Important Notes: - Each line must be valid JSON - The <code>content</code> field is required; documents without it will be skipped - Metadata fields are completely flexible - you can include any custom fields - When chunks are created, they inherit all metadata from the original document - Chunking adds a <code>headers</code> field to metadata containing markdown header hierarchy</p> <pre><code># examples/process_and_upload.yaml\npipeline:\n  batch_size: 10\n  inputs:\n    path: \"data/doc_w_metadata.jsonl\"\n\n  stages:\n    # Step 1: Extract content from documents\n    - name: extraction\n      config: { format: \"jsonl\" }\n\n    # Step 2: Chunk documents into semantic pieces\n    - name: chunker\n      config: {\n        \"chunk_overlap\": 0,\n        \"max_chunk_size\": 512,\n        \"word_overlap\": 0,\n        \"add_headers\": false,\n        \"merge_small_chunks\": true,\n        \"headers_to_split_on\": [ 1, 2, 3, 4, 5, 6 ]\n      }\n\n    # Step 3: Remove short chunks (&lt; 40 words)\n    - name: length_filter\n      config:\n        length: 40\n        comparison: \"greater\"\n        action: \"keep\"\n\n    # Step 4: Remove long chunks (&gt;= 1024 words)\n    - name: length_filter\n      config:\n        length: 1024\n        comparison: \"less\"\n        action: \"keep\"\n\n    # Step 5: Remove references and acknowledgements\n    - name: reference_filter\n      config:\n        action: \"discard\"\n\n    # Step 6: PII filter with threshold\n    - name: pii_filter\n      config:\n        threshold: 0.03\n        action: \"discard\"\n        apply_filter: true\n\n    # Step 7: Remove chunks with excessive newlines\n    - name: newline_filter\n      config:\n        chunks: 60\n        comparison: \"less\"\n        action: \"keep\"\n\n    # Step 8: Generate embeddings and upload to Qdrant\n    - name: qdrant_upload\n      config:\n        mode: \"qdrant\"\n        use_existing_embeddings: false\n        upload_pipeline_metadata: true\n\n        embedder:\n          model_name: \"Qwen/Qwen3-Embedding-4B\"\n          url: 'http://0.0.0.0:8000'\n          timeout: 300\n          api_key: \"EMPTY\"\n\n        vector_store:\n          batch_size: 1000\n          collection_name: \"your-collection-name\"\n          vector_size: 2560\n          url: \"http://localhost:6333\"\n          api_key: \"your-api-key\"\n</code></pre> <p>To run:</p> <pre><code>cp examples/process_and_upload.yaml config.yaml\n# Edit config.yaml to set your Qdrant collection name, URL, and API key\neve run\n</code></pre> <p>What this pipeline does: 1. Extracts content from JSONL documents (preserves all metadata from input) 2. Splits documents into chunks of up to 512 words    - Each chunk inherits all metadata from the original document    - Chunking adds a <code>headers</code> field to metadata with markdown header hierarchy 3. Filters chunks by length (40-1024 words) 4. Removes references and acknowledgements sections 5. Filters out chunks with PII above 3% threshold 6. Removes chunks with excessive newlines 7. Generates embeddings using VLLM server 8. Uploads filtered documents with embeddings to Qdrant    - Includes original metadata from JSONL input    - Includes chunk headers    - Includes filter statistics (if <code>upload_pipeline_metadata: true</code>)</p> <p>Metadata Flow Example:</p> <pre><code>Input JSONL:\n{\"content\": \"# Introduction\\n\\nThis is my paper...\", \"metadata\": {\"title\": \"My Paper\", \"author\": \"Jane Doe\"}}\n\nAfter Chunking:\nDocument 1: {\"content\": \"This is my paper...\", \"metadata\": {\"title\": \"My Paper\", \"author\": \"Jane Doe\", \"headers\": [\"#Introduction\"]}}\n\nAfter Upload to Qdrant:\nAll chunks retain: title=\"My Paper\", author=\"Jane Doe\", headers=[\"#Introduction\"], plus any filter metadata\n</code></pre>"},{"location":"data-processing/docs/examples/basic-usage/#selective-stage-processing","title":"Selective Stage Processing","text":"<p>Skip certain stages based on your needs:</p>"},{"location":"data-processing/docs/examples/basic-usage/#extraction-only","title":"Extraction Only","text":"<pre><code>pipeline:\n  batch_size: 20\n  inputs:\n    path: \"raw_documents\"\n  stages:\n    - name: extraction\n    - name: export\n      config:\n        format: \"md\"\n        destination: \"extracted_content\"\n</code></pre>"},{"location":"data-processing/docs/examples/basic-usage/#deduplication-only","title":"Deduplication Only","text":"<pre><code>pipeline:\n  inputs:\n    path: \"markdown_documents\"\n  stages:\n    - name: duplication\n      config:\n        method: \"lsh\"\n        threshold: 0.9\n    - name: export\n      config:\n        format: \"md\"\n        destination: \"unique_documents\"\n</code></pre>"},{"location":"data-processing/docs/getting-started/configuration/","title":"Configuration Guide","text":"<p>This guide covers the main configuration options for the EVE Pipeline. You can find the detailed configurations under each <code>Pipeline Stage</code>.</p>"},{"location":"data-processing/docs/getting-started/configuration/#configuration-file-structure","title":"Configuration File Structure","text":"<p>The pipeline is configured using a YAML file (typically <code>config.yaml</code>) with the following structure:</p> <pre><code>pipeline:\n  batch_size: integer\n  inputs:\n    path: string\n    # ... other input options\n  stages:\n    - name: string\n      config: object\n    # ... more stages\n</code></pre>"},{"location":"data-processing/docs/getting-started/configuration/#global-configuration","title":"Global Configuration","text":""},{"location":"data-processing/docs/getting-started/configuration/#batch_size","title":"batch_size","text":"<ul> <li>Type: Integer</li> <li>Default: <code>10</code></li> <li>Description: Number of documents to process in each batch</li> <li>Note: Not applicable to deduplication stage</li> </ul> <pre><code>pipeline:\n  batch_size: 20\n</code></pre>"},{"location":"data-processing/docs/getting-started/configuration/#inputs","title":"inputs","text":""},{"location":"data-processing/docs/getting-started/configuration/#path","title":"path","text":"<ul> <li>Type: String</li> <li>Required: Yes</li> <li>Description: Path to input directory or file containing documents</li> <li>Supported Formats: Directories with PDF/HTML/XML/Markdown files, or JSONL files</li> </ul> <pre><code>pipeline:\n  inputs:\n    path: \"input_documents\"  # Directory with various document formats\n    # OR\n    path: \"data/documents.jsonl\"  # JSONL file with structured data\n</code></pre>"},{"location":"data-processing/docs/getting-started/configuration/#using-jsonl-input-files","title":"Using JSONL Input Files","text":"<p>JSONL (JSON Lines) format is a powerful way to provide pre-structured documents with metadata. Each line in the file must be a valid JSON object.</p> <p>Required Fields: - <code>content</code> (string): The document text content</p> <p>Optional Fields: - <code>metadata</code> (object): Custom metadata preserved throughout the pipeline - <code>embedding</code> (array): Pre-computed embedding vector - <code>pipeline_metadata</code> (object): Internal metadata from previous pipeline runs</p> <p>Example JSONL file (<code>documents.jsonl</code>):</p> <pre><code>{\"content\": \"First document text here.\", \"metadata\": {\"title\": \"Document 1\", \"author\": \"John Doe\", \"year\": 2024}}\n{\"content\": \"Second document text.\", \"metadata\": {\"title\": \"Document 2\", \"source\": \"research.pdf\", \"tags\": [\"AI\", \"ML\"]}}\n{\"content\": \"Document with embedding.\", \"metadata\": {\"title\": \"Doc 3\"}, \"embedding\": [0.123, 0.456, 0.789, ...]}\n</code></pre> <p>Key Benefits: - Metadata Preservation: All metadata fields are preserved throughout the pipeline - Metadata Inheritance: When chunking, each chunk inherits the original document's metadata - Pre-computed Embeddings: Can include embeddings to skip re-computation - Flexible Schema: Add any custom metadata fields you need</p> <p>Usage Example:</p> <pre><code>pipeline:\n  inputs:\n    path: \"data/papers.jsonl\"\n  stages:\n    - name: extraction\n      config: { format: \"jsonl\" }\n    - name: chunker\n      config: { max_chunk_size: 512 }\n    # Chunks will have metadata: {\"title\": \"...\", \"author\": \"...\", \"year\": ..., \"headers\": [...]}\n</code></pre>"},{"location":"data-processing/docs/getting-started/configuration/#pipeline-stages","title":"Pipeline Stages","text":""},{"location":"data-processing/docs/getting-started/configuration/#extraction-stage","title":"Extraction Stage","text":"<p>Extracts content from various document formats.</p> <pre><code>- name: extraction\n  config:\n    format: \"\"  # or \"pdf\", \"html\", \"xml\", \"markdown\", \"jsonl\"\n    url: \"http://127.0.0.1:8001\"  # for server-based extraction\n</code></pre>"},{"location":"data-processing/docs/getting-started/configuration/#options","title":"Options","text":"<ul> <li> <p>format: Document format specification</p> <ul> <li><code>\"\"</code> (default): Automatically detect format</li> <li><code>\"pdf\"</code>: PDF documents</li> <li><code>\"html\"</code>: HTML documents</li> <li><code>\"xml\"</code>: XML documents</li> <li><code>\"markdown\"</code>: Markdown documents</li> <li><code>\"jsonl\"</code>: JSON Lines format (one JSON object per line)</li> </ul> </li> <li> <p>url: Server URL for nougat extraction (required for PDF format)</p> </li> </ul> <p>Note on JSONL Format: When using JSONL input, each line must be a valid JSON object with a required <code>content</code> field. See the JSONL Input Files section above for detailed format specifications and examples.</p>"},{"location":"data-processing/docs/getting-started/configuration/#deduplication-stage","title":"Deduplication Stage","text":"<p>Removes duplicate and near-duplicate documents.</p> <pre><code>- name: duplication\n  config:\n    method: \"exact\"  # or \"lsh\"\n    # LSH options (when method: \"lsh\")\n    shingle_size: 3\n    num_perm: 128\n    threshold: 0.8\n</code></pre>"},{"location":"data-processing/docs/getting-started/configuration/#options_1","title":"Options","text":"<ul> <li> <p>method: Deduplication method</p> <ul> <li><code>\"exact\"</code> (default): Exact hash-based deduplication</li> <li><code>\"lsh\"</code>: Locality Sensitive Hashing for near-duplicates</li> </ul> </li> </ul>"},{"location":"data-processing/docs/getting-started/configuration/#lsh-options","title":"LSH Options","text":"<ul> <li>shingle_size: Size of text shingles (default: <code>3</code>)</li> <li>num_perm: Number of permutations (default: <code>128</code>)</li> <li>threshold: Similarity threshold (default: <code>0.8</code>)</li> </ul>"},{"location":"data-processing/docs/getting-started/configuration/#cleaning-stage","title":"Cleaning Stage","text":"<p>Removes noise and improves document quality.</p> <pre><code>- name: cleaning\n  config:\n    ocr_threshold: 0.9\n    min_words: 2\n    enable_latex_correction: True\n</code></pre>"},{"location":"data-processing/docs/getting-started/configuration/#options_2","title":"Options","text":"<ul> <li>ocr_threshold: OCR duplicate threshold (default: <code>0.99</code>)</li> <li>min_words: Minimum words for processing (default: <code>2</code>)</li> <li>enable_latex_correction: Use LLM to fix latex formulas and tables (default: <code>false</code>)</li> </ul>"},{"location":"data-processing/docs/getting-started/configuration/#pii-removal-stage","title":"PII Removal Stage","text":"<p>Redacts personally identifiable information.</p> <pre><code>- name: pii\n  config:\n    url: \"http://127.0.0.1:8000\"\n</code></pre>"},{"location":"data-processing/docs/getting-started/configuration/#options_3","title":"Options","text":"<ul> <li>url: Presidio server URL </li> </ul>"},{"location":"data-processing/docs/getting-started/configuration/#export-stage","title":"Export Stage","text":"<p>Saves processed documents to output.</p> <pre><code>- name: export\n  config:\n    format: \"md\"  # or \"txt\", \"json\"\n    destination: \"output\"\n</code></pre>"},{"location":"data-processing/docs/getting-started/configuration/#options_4","title":"Options","text":"<ul> <li> <p>format: Output format</p> <ul> <li><code>\"md\"</code> (default): Markdown</li> <li><code>\"txt\"</code>: Plain text</li> <li><code>\"json\"</code>: JSON with metadata</li> </ul> </li> <li> <p>destination: Output directory path</p> </li> </ul>"},{"location":"data-processing/docs/getting-started/installation/","title":"Installation","text":"<p>This guide will help you install and set up the EVE Pipeline on your system.</p>"},{"location":"data-processing/docs/getting-started/installation/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following installed:</p> <ul> <li>Python 3.10 or higher</li> <li>uv (recommended) or pip for package management</li> </ul>"},{"location":"data-processing/docs/getting-started/installation/#install-uv-recommended","title":"Install uv (Recommended)","text":"<pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre>"},{"location":"data-processing/docs/getting-started/installation/#installation-methods","title":"Installation Methods","text":""},{"location":"data-processing/docs/getting-started/installation/#method-1-using-uv-recommended","title":"Method 1: Using uv (Recommended)","text":"<ol> <li> <p>Clone the repository</p> <p><code>bash git clone https://github.com/eve-esa/eve-pipeline.git cd eve-pipeline</code></p> </li> <li> <p>Install dependencies</p> <p><code>bash uv sync</code></p> </li> </ol>"},{"location":"data-processing/docs/getting-started/installation/#method-2-using-pip","title":"Method 2: Using pip","text":"<ol> <li> <p>Clone the repository</p> <p><code>bash git clone https://github.com/eve-esa/eve-pipeline.git cd eve-pipeline</code></p> </li> <li> <p>Create a virtual environment</p> <p><code>bash python -m venv venv source venv/bin/activate  # On Windows: venv\\Scripts\\activate</code></p> </li> <li> <p>Install dependencies</p> <p><code>bash pip install -r requirements.txt pip install -e .</code></p> </li> </ol>"},{"location":"data-processing/docs/getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":""},{"location":"data-processing/docs/getting-started/installation/#server-setup","title":"Server Setup","text":"<p>Some pipeline stages require external servers:</p>"},{"location":"data-processing/docs/getting-started/installation/#pii-server","title":"PII Server","text":"<p>For PII (Personally Identifiable Information) removal:</p> <pre><code>cd server\npython3 pii_server.py\n</code></pre>"},{"location":"data-processing/docs/getting-started/installation/#ocr-server","title":"OCR Server","text":"<pre><code>cd server\npython3 nougat_server.py\n</code></pre>"},{"location":"data-processing/docs/getting-started/installation/#next-steps","title":"Next Steps","text":"<p>After installation, proceed to the Quick Start guide to learn how to configure and run your first pipeline.</p>"},{"location":"data-processing/docs/getting-started/quick-start/","title":"Quick Start","text":"<p>This tutorial will walk you through running your first data processing pipeline with EVE.</p>"},{"location":"data-processing/docs/getting-started/quick-start/#step-1-prepare-your-data","title":"Step 1: Prepare Your Data","text":"<p>Create an input directory with your documents:</p> <pre><code>mkdir -p input_data\n# Copy your PDF, HTML, XML, or Markdown files here\ncp /path/to/your/documents/* input_data/\n</code></pre>"},{"location":"data-processing/docs/getting-started/quick-start/#step-2-basic-configuration","title":"Step 2: Basic Configuration","text":"<p>Create a <code>config.yaml</code> file:</p> <pre><code>pipeline:\n  batch_size: 10\n  inputs:\n    path: \"input_data\"\n  stages:\n    - name: extraction\n      # Automatically detects file format\n    - name: duplication\n    - name: export\n      config:\n        format: \"md\"\n        destination: \"output\"\n</code></pre>"},{"location":"data-processing/docs/getting-started/quick-start/#step-3-run-the-pipeline","title":"Step 3: Run the Pipeline","text":"<p>Execute the pipeline:</p> <pre><code>eve run\n</code></pre>"},{"location":"data-processing/docs/getting-started/quick-start/#step-4-check-results","title":"Step 4: Check Results","text":"<p>Your processed documents will be in the <code>output</code> directory:</p> <pre><code>ls output/\n</code></pre>"},{"location":"data-processing/docs/getting-started/quick-start/#example-pipeline-configurations","title":"Example Pipeline Configurations","text":""},{"location":"data-processing/docs/getting-started/quick-start/#pdf-processing-only","title":"PDF Processing Only","text":"<pre><code>pipeline:\n  batch_size: 5\n  inputs:\n    path: \"pdfs\"\n  stages:\n    - name: extraction\n      config: { format: \"pdf\" }\n    - name: cleaning\n    - name: export\n      config: { format: \"md\", destination: \"processed_pdfs\" }\n</code></pre>"},{"location":"data-processing/docs/getting-started/quick-start/#html-processing-with-pii-removal","title":"HTML Processing with PII Removal","text":"<pre><code>pipeline:\n  batch_size: 10\n  inputs:\n    path: \"html_docs\"\n  stages:\n    - name: extraction\n      config: { format: \"html\", url: \"http://127.0.0.1:8001\" }\n    - name: pii\n      config: { url: \"http://127.0.0.1:8000\" }\n    - name: export\n      config: { format: \"md\"}\n</code></pre>"},{"location":"data-processing/docs/getting-started/quick-start/#advanced-pipeline-with-all-stages","title":"Advanced Pipeline with All Stages","text":"<pre><code>pipeline:\n  batch_size: 10\n  inputs:\n    path: \"mixed_docs\"\n  stages:\n    - name: extraction\n      config: { url: \"http://127.0.0.1:8001\" }\n    - name: duplication\n      config: {\n        method: \"lsh\",\n        shingle_size: 3,\n        num_perm: 128,\n        threshold: 0.8\n      }\n    - name: cleaning\n    - name: pii\n      config: { url: \"http://127.0.0.1:8000\" }\n    - name: metadata\n</code></pre>"},{"location":"data-processing/docs/getting-started/quick-start/#monitoring-progress","title":"Monitoring Progress","text":"<p>The pipeline provides progress updates:</p> <pre><code>$ eve run\n\n[2024-01-15 10:30:00] INFO: Starting pipeline with 100 documents\n[2024-01-15 10:30:01] INFO: Stage 1/5: Extraction\n[2024-01-15 10:30:15] INFO: Processing batch 1/10 (10 documents)\n[2024-01-15 10:30:30] INFO: Processing batch 2/10 (20 documents)\n...\n[2024-01-15 10:35:00] INFO: Pipeline completed successfully\n[2024-01-15 10:35:00] INFO: Processed 95 documents, 5 duplicates removed\n</code></pre>"},{"location":"data-processing/docs/getting-started/quick-start/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about configuration options</li> </ul>"},{"location":"data-processing/docs/pipeline-stages/chunking/","title":"Chunking","text":"<p>The chunking stage splits large documents into smaller, semantically meaningful chunks that are suitable for downstream processing like embedding generation and vector database upload.</p>"},{"location":"data-processing/docs/pipeline-stages/chunking/#overview","title":"Overview","text":"<p>Chunking is essential for:</p> <ul> <li>Vector database upload: Breaking documents into appropriately-sized pieces for embedding</li> <li>Semantic retrieval: Creating chunks that represent coherent topics or concepts</li> <li>Context window management: Ensuring chunks fit within model token limits</li> <li>Performance optimization: Parallelizing processing across multiple chunks</li> </ul> <p>The Eve pipeline uses a sophisticated two-step chunking strategy that preserves document structure and special content:</p> <ol> <li>Header-based splitting: First splits documents by Markdown headers to maintain semantic structure</li> <li>Sentence-based splitting: If sections exceed the size limit, further splits them by sentences</li> <li>Smart merging: Optionally merges small chunks back together when they share compatible heading levels</li> <li>Content preservation: Keeps LaTeX formulas, equations, and tables intact as atomic units</li> </ol>"},{"location":"data-processing/docs/pipeline-stages/chunking/#features","title":"Features","text":"<ul> <li>Semantic chunking: Respects document structure by splitting on Markdown headers</li> <li>LaTeX preservation: Keeps mathematical formulas and equations together</li> <li>Table preservation: Maintains tables as complete units without splitting</li> <li>Configurable overlap: Add word-based overlap between chunks for better retrieval</li> <li>Parallel processing: Uses multiprocessing for fast chunking of large document sets</li> <li>Header inclusion: Optionally adds section headers to chunks for context</li> </ul>"},{"location":"data-processing/docs/pipeline-stages/chunking/#configuration","title":"Configuration","text":"<p>Step name: <code>chunker</code></p>"},{"location":"data-processing/docs/pipeline-stages/chunking/#configuration-parameters","title":"Configuration Parameters","text":"Parameter Type Required Default Description <code>max_chunk_size</code> int No <code>512</code> Maximum size of any chunk in words <code>chunk_overlap</code> int No <code>0</code> Number of characters to overlap between chunks during secondary splitting <code>word_overlap</code> int No <code>0</code> Number of words to overlap between chunks (takes precedence over chunk_overlap) <code>add_headers</code> bool No <code>False</code> Whether to prepend section headers to chunk content <code>merge_small_chunks</code> bool No <code>True</code> Whether to merge small chunks that share compatible heading levels <code>headers_to_split_on</code> list[int] No <code>[1, 2, 3, 4, 5, 6]</code> Markdown header levels to split on (1=<code>#</code>, 2=<code>##</code>, etc.) <code>max_workers</code> int No <code>None</code> Number of parallel workers (None = CPU count)"},{"location":"data-processing/docs/pipeline-stages/chunking/#basic-configuration","title":"Basic Configuration","text":"<pre><code>- name: chunker\n  config:\n    max_chunk_size: 512\n    add_headers: true\n    merge_small_chunks: true\n</code></pre>"},{"location":"data-processing/docs/pipeline-stages/chunking/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code>- name: chunker\n  config:\n    max_chunk_size: 1024\n    chunk_overlap: 0\n    word_overlap: 50\n    add_headers: true\n    merge_small_chunks: true\n    headers_to_split_on: [1, 2, 3]  # Only split on H1, H2, H3\n    max_workers: 8  # Use 8 parallel workers\n</code></pre>"},{"location":"data-processing/docs/pipeline-stages/chunking/#how-it-works","title":"How It Works","text":""},{"location":"data-processing/docs/pipeline-stages/chunking/#two-step-chunking-strategy","title":"Two-Step Chunking Strategy","text":""},{"location":"data-processing/docs/pipeline-stages/chunking/#step-1-header-based-splitting","title":"Step 1: Header-Based Splitting","text":"<p>The chunker first splits the document based on Markdown headers:</p> <pre><code># Introduction\nThis is the introduction text...\n\n## Background\nThis is the background section...\n\n## Methods\nThis section describes methods...\n</code></pre> <p>This creates initial chunks at natural document boundaries.</p>"},{"location":"data-processing/docs/pipeline-stages/chunking/#step-2-size-based-splitting","title":"Step 2: Size-Based Splitting","text":"<p>If any chunk exceeds <code>max_chunk_size</code>, it's further split using sentence boundaries while preserving:</p> <ul> <li>LaTeX environments (<code>\\begin{...}...\\end{...}</code>)</li> <li>Inline and display math (<code>$...$</code>, <code>$$...$$</code>)</li> <li>Markdown tables</li> <li>Figure and table references</li> </ul>"},{"location":"data-processing/docs/pipeline-stages/chunking/#step-3-smart-merging","title":"Step 3: Smart Merging","text":"<p>If <code>merge_small_chunks: true</code>, the chunker merges adjacent chunks when:</p> <ol> <li>Combined length doesn't exceed <code>max_chunk_size</code></li> <li>Chunks have compatible heading levels:</li> <li>Same level headers (e.g., two H2 sections)</li> <li>Previous chunk has higher level header (e.g., H1 followed by H2)</li> </ol> <p>This prevents overly small chunks while maintaining semantic coherence.</p>"},{"location":"data-processing/docs/pipeline-stages/chunking/#header-inclusion","title":"Header Inclusion","text":"<p>When <code>add_headers: true</code>, section headers are prepended to each chunk:</p> <p>Without headers:</p> <pre><code>This section describes the methodology used in the study...\n</code></pre> <p>With headers:</p> <pre><code># Introduction\n## Methods\nThis section describes the methodology used in the study...\n</code></pre> <p>This provides context for each chunk, especially useful for retrieval systems.</p>"},{"location":"data-processing/docs/pipeline-stages/chunking/#content-preservation","title":"Content Preservation","text":"<p>The chunker intelligently handles special content:</p> <p>LaTeX Formulas:</p> <pre><code>The equation \\begin{equation}\nE = mc^2\n\\end{equation} is preserved intact.\n</code></pre> <p>Tables:</p> <pre><code>| Column 1 | Column 2 |\n|----------|----------|\n| Data 1   | Data 2   |\n</code></pre> <p>These are never split mid-formula or mid-table, even if they exceed <code>max_chunk_size</code>.</p>"},{"location":"data-processing/docs/pipeline-stages/chunking/#use-cases","title":"Use Cases","text":""},{"location":"data-processing/docs/pipeline-stages/chunking/#small-chunks-for-dense-retrieval","title":"Small Chunks for Dense Retrieval","text":"<pre><code>- name: chunker\n  config:\n    max_chunk_size: 256\n    word_overlap: 20\n    add_headers: true\n    merge_small_chunks: false\n</code></pre> <p>Creates small, focused chunks with overlap for better semantic retrieval.</p>"},{"location":"data-processing/docs/pipeline-stages/chunking/#large-chunks-for-context","title":"Large Chunks for Context","text":"<pre><code>- name: chunker\n  config:\n    max_chunk_size: 2048\n    add_headers: false\n    merge_small_chunks: true\n    headers_to_split_on: [1, 2]  # Only split on major sections\n</code></pre> <p>Creates larger chunks that preserve more context, suitable for summarization or large context windows.</p>"},{"location":"data-processing/docs/pipeline-stages/chunking/#academic-papers","title":"Academic Papers","text":"<pre><code>- name: chunker\n  config:\n    max_chunk_size: 512\n    add_headers: true\n    merge_small_chunks: true\n    headers_to_split_on: [1, 2, 3, 4, 5, 6]\n</code></pre> <p>Respects the hierarchical structure of academic papers while maintaining readable chunk sizes.</p>"},{"location":"data-processing/docs/pipeline-stages/chunking/#output-format","title":"Output Format","text":"<p>Each chunk becomes a separate <code>Document</code> with:</p> <ul> <li>content: The chunk text (with headers if <code>add_headers: true</code>)</li> <li>file_path: Original document file path</li> <li>file_format: Original document format</li> <li>metadata.headers: List of Markdown headers that apply to this chunk</li> </ul>"},{"location":"data-processing/docs/pipeline-stages/chunking/#example-output","title":"Example Output","text":"<pre><code>Document(\n    content=\"# Introduction\\n## Background\\nThis paper discusses...\",\n    file_path=\"papers/paper1.pdf\",\n    file_format=\"pdf\",\n    metadata={\n        \"headers\": [\"#Introduction\", \"##Background\"],\n        # ... other metadata from original document\n    }\n)\n</code></pre>"},{"location":"data-processing/docs/pipeline-stages/chunking/#performance","title":"Performance","text":"<p>The chunker uses parallel processing to handle large document sets efficiently:</p> <ul> <li>Documents are processed in separate processes using <code>ProcessPoolExecutor</code></li> <li>Each process runs an independent chunker instance</li> <li>Results are collected and flattened into a single list</li> <li>Set <code>max_workers</code> to control parallelism (defaults to CPU count)</li> </ul> <p>Performance tip: For I/O-bound operations, use the default <code>max_workers=None</code>. For CPU-intensive chunking of very large documents, experiment with different worker counts.</p>"},{"location":"data-processing/docs/pipeline-stages/chunking/#integration-with-other-steps","title":"Integration with Other Steps","text":""},{"location":"data-processing/docs/pipeline-stages/chunking/#typical-pipeline-order","title":"Typical Pipeline Order","text":"<pre><code>pipeline:\n  inputs:\n    path: \"documents\"\n  stages:\n    - name: extraction\n\n    - name: deduplication\n      config:\n        method: \"lsh\"\n\n    - name: cleaning\n\n    - name: chunker\n      config:\n        max_chunk_size: 512\n        add_headers: true\n\n    - name: embedding  # Or qdrant upload\n\n    - name: export\n</code></pre>"},{"location":"data-processing/docs/pipeline-stages/chunking/#before-vector-database-upload","title":"Before Vector Database Upload","text":"<p>Chunking is typically done before uploading to vector databases:</p> <pre><code>- name: chunker\n  config:\n    max_chunk_size: 512\n    add_headers: true\n\n- name: qdrant\n  config:\n    database:\n      collection_name: \"documents\"\n    # ... other qdrant config\n</code></pre> <p>This ensures each chunk gets its own embedding vector in the database.</p>"},{"location":"data-processing/docs/pipeline-stages/chunking/#best-practices","title":"Best Practices","text":"<ol> <li>Choose appropriate chunk size:</li> <li>Smaller chunks (256-512 words) for dense retrieval</li> <li> <p>Larger chunks (1024-2048 words) for summarization or large context models</p> </li> <li> <p>Add headers for context: Enable <code>add_headers: true</code> when chunks will be retrieved without surrounding context</p> </li> <li> <p>Merge small chunks: Keep <code>merge_small_chunks: true</code> to avoid tiny chunks that lack sufficient context</p> </li> <li> <p>Adjust header levels: For documents with deep nesting, limit <code>headers_to_split_on</code> to major sections only</p> </li> </ol>"},{"location":"data-processing/docs/pipeline-stages/chunking/#troubleshooting","title":"Troubleshooting","text":""},{"location":"data-processing/docs/pipeline-stages/chunking/#chunks-are-too-large","title":"Chunks are too large","text":"<ul> <li>Decrease <code>max_chunk_size</code></li> <li>Add more header levels to <code>headers_to_split_on</code></li> <li>Set <code>merge_small_chunks: false</code></li> </ul>"},{"location":"data-processing/docs/pipeline-stages/chunking/#chunks-are-too-small","title":"Chunks are too small","text":"<ul> <li>Increase <code>max_chunk_size</code></li> <li>Set <code>merge_small_chunks: true</code></li> <li>Reduce header levels in <code>headers_to_split_on</code></li> </ul>"},{"location":"data-processing/docs/pipeline-stages/chunking/#latex-formulas-are-broken","title":"LaTeX formulas are broken","text":"<p>The chunker should preserve LaTeX automatically. If formulas are breaking:</p> <ul> <li>Check that LaTeX uses proper <code>\\begin{...}</code> and <code>\\end{...}</code> syntax</li> <li>Verify formulas aren't malformed in the original document</li> <li>Review the cleaning step output before chunking</li> </ul>"},{"location":"data-processing/docs/pipeline-stages/chunking/#slow-performance","title":"Slow performance","text":"<ul> <li>Adjust <code>max_workers</code> (try different values)</li> <li>Ensure you're chunking after deduplication and cleaning</li> <li>Consider increasing <code>max_chunk_size</code> to reduce total chunk count</li> </ul>"},{"location":"data-processing/docs/pipeline-stages/chunking/#next-steps","title":"Next Steps","text":"<ul> <li>Set up Qdrant upload to store chunks in a vector database</li> <li>Learn about Export options for saving chunked documents</li> </ul>"},{"location":"data-processing/docs/pipeline-stages/chunking/#code-reference","title":"Code Reference","text":"<p>Document chunking step using semantic two-step chunking strategy.</p>"},{"location":"data-processing/docs/pipeline-stages/chunking/#eve.steps.chunking.chunker_step.ChunkerStep","title":"<code>ChunkerStep</code>","text":"<p>               Bases: <code>PipelineStep</code></p> <p>Chunk documents into smaller, semantically meaningful pieces.</p> <p>Uses a two-step chunking strategy:</p> <ol> <li>Split by Markdown headers to maintain document structure</li> <li>Further split large sections by sentences while preserving LaTeX and tables</li> <li>Optionally merge small chunks that share compatible heading levels</li> </ol> <p>The chunker processes documents in parallel using multiprocessing for performance.</p> <p>Config parameters:</p> <pre><code>- max_chunk_size (int): Maximum size of any chunk in words (default: 512)\n- chunk_overlap (int): Number of characters to overlap between chunks (default: 0)\n- word_overlap (int): Number of words to overlap between chunks (default: 0)\n- add_headers (bool): Whether to prepend section headers to chunks (default: False)\n- merge_small_chunks (bool): Whether to merge small chunks with compatible headers (default: True)\n- headers_to_split_on (list[int]): Markdown header levels to split on (default: [1, 2, 3, 4, 5, 6])\n- max_workers (int): Number of parallel workers, None uses CPU count (default: None)\n</code></pre> <p>Examples:</p>"},{"location":"data-processing/docs/pipeline-stages/chunking/#eve.steps.chunking.chunker_step.ChunkerStep--basic-chunking-with-default-settings","title":"Basic chunking with default settings","text":"<p>config: {max_chunk_size: 512}</p>"},{"location":"data-processing/docs/pipeline-stages/chunking/#eve.steps.chunking.chunker_step.ChunkerStep--chunking-with-headers-and-overlap-for-retrieval","title":"Chunking with headers and overlap for retrieval","text":"<p>config: {     max_chunk_size: 512,     add_headers: true,     word_overlap: 20,     merge_small_chunks: true }</p>"},{"location":"data-processing/docs/pipeline-stages/chunking/#eve.steps.chunking.chunker_step.ChunkerStep--large-chunks-for-context-preservation","title":"Large chunks for context preservation","text":"<p>config: {     max_chunk_size: 2048,     headers_to_split_on: [1, 2],     merge_small_chunks: true }</p> Source code in <code>data-processing/eve/steps/chunking/chunker_step.py</code> <pre><code>class ChunkerStep(PipelineStep):\n    \"\"\"Chunk documents into smaller, semantically meaningful pieces.\n\n    Uses a two-step chunking strategy:\n\n    1. Split by Markdown headers to maintain document structure\n    2. Further split large sections by sentences while preserving LaTeX and tables\n    3. Optionally merge small chunks that share compatible heading levels\n\n    The chunker processes documents in parallel using multiprocessing for performance.\n\n    Config parameters:\n\n        - max_chunk_size (int): Maximum size of any chunk in words (default: 512)\n        - chunk_overlap (int): Number of characters to overlap between chunks (default: 0)\n        - word_overlap (int): Number of words to overlap between chunks (default: 0)\n        - add_headers (bool): Whether to prepend section headers to chunks (default: False)\n        - merge_small_chunks (bool): Whether to merge small chunks with compatible headers (default: True)\n        - headers_to_split_on (list[int]): Markdown header levels to split on (default: [1, 2, 3, 4, 5, 6])\n        - max_workers (int): Number of parallel workers, None uses CPU count (default: None)\n\n    Examples:\n        # Basic chunking with default settings\n        config: {max_chunk_size: 512}\n\n        # Chunking with headers and overlap for retrieval\n        config: {\n            max_chunk_size: 512,\n            add_headers: true,\n            word_overlap: 20,\n            merge_small_chunks: true\n        }\n\n        # Large chunks for context preservation\n        config: {\n            max_chunk_size: 2048,\n            headers_to_split_on: [1, 2],\n            merge_small_chunks: true\n        }\n    \"\"\"\n\n    def __init__(self, config: dict):\n        \"\"\"Initialize the chunker step.\n\n        Args:\n            config: Configuration dictionary containing chunking parameters\n        \"\"\"\n        super().__init__(config, name=\"ChunkerStep\")\n\n        self.chunk_overlap = config.get(\"chunk_overlap\", 0)\n        self.max_chunk_size = config.get(\"max_chunk_size\", 512)\n        self.word_overlap = config.get(\"word_overlap\", 0)\n        self.add_headers = config.get(\"add_headers\", False)\n        self.merge_small_chunks = config.get(\"merge_small_chunks\", True)\n        self.headers_to_split_on = config.get(\"headers_to_split_on\", [1, 2, 3, 4, 5, 6])\n        self.max_workers = config.get(\"max_workers\", None)  # None = CPU count\n\n        self.chunker = MarkdownTwoStepChunker(\n            self.max_chunk_size,\n            self.chunk_overlap,\n            self.add_headers,\n            self.word_overlap,\n            self.headers_to_split_on,\n            self.merge_small_chunks,\n        )\n\n    async def execute(self, documents: List[Document]) -&gt; List[Document]:\n        \"\"\"Execute chunking on documents in parallel.\n\n        Processes each document independently using multiprocessing, then flattens\n        all chunks into a single list.\n\n        Args:\n            documents: List of documents to chunk\n\n        Returns:\n            Flattened list of all chunks from all documents\n        \"\"\"\n        self.logger.info(f\"Chunking {len(documents)} documents\")\n        self.logger.info(f\"Using max_chunk_size={self.max_chunk_size}, chunk_overlap={self.chunk_overlap}\")\n        self.logger.info(f\"Parallel processing with max_workers={self.max_workers or 'CPU count'}\")\n\n        loop = asyncio.get_event_loop()\n\n        # Serialize documents to plain dicts for pickling\n        serialized_docs = [_serialize_document(doc) for doc in documents]\n\n        # Create a partial function with the chunker configuration\n        chunk_func = partial(\n            _chunk_document,\n            max_chunk_size=self.max_chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            add_headers=self.add_headers,\n            word_overlap=self.word_overlap,\n            headers_to_split_on=self.headers_to_split_on,\n            merge_small_chunks=self.merge_small_chunks,\n        )\n\n        # Process documents in parallel\n        with ProcessPoolExecutor(max_workers=self.max_workers) as executor:\n            tasks = [\n                loop.run_in_executor(executor, chunk_func, doc)\n                for doc in serialized_docs\n            ]\n            results = await asyncio.gather(*tasks)\n\n        # Flatten and deserialize results\n        all_chunks = []\n        for doc_chunks in results:\n            all_chunks.extend([_deserialize_document(chunk) for chunk in doc_chunks])\n\n        self.logger.info(f\"Chunking complete: {len(documents)} documents -&gt; {len(all_chunks)} chunks\")\n\n        return all_chunks\n</code></pre>"},{"location":"data-processing/docs/pipeline-stages/chunking/#eve.steps.chunking.chunker_step.ChunkerStep.__init__","title":"<code>__init__(config)</code>","text":"<p>Initialize the chunker step.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Configuration dictionary containing chunking parameters</p> required Source code in <code>data-processing/eve/steps/chunking/chunker_step.py</code> <pre><code>def __init__(self, config: dict):\n    \"\"\"Initialize the chunker step.\n\n    Args:\n        config: Configuration dictionary containing chunking parameters\n    \"\"\"\n    super().__init__(config, name=\"ChunkerStep\")\n\n    self.chunk_overlap = config.get(\"chunk_overlap\", 0)\n    self.max_chunk_size = config.get(\"max_chunk_size\", 512)\n    self.word_overlap = config.get(\"word_overlap\", 0)\n    self.add_headers = config.get(\"add_headers\", False)\n    self.merge_small_chunks = config.get(\"merge_small_chunks\", True)\n    self.headers_to_split_on = config.get(\"headers_to_split_on\", [1, 2, 3, 4, 5, 6])\n    self.max_workers = config.get(\"max_workers\", None)  # None = CPU count\n\n    self.chunker = MarkdownTwoStepChunker(\n        self.max_chunk_size,\n        self.chunk_overlap,\n        self.add_headers,\n        self.word_overlap,\n        self.headers_to_split_on,\n        self.merge_small_chunks,\n    )\n</code></pre>"},{"location":"data-processing/docs/pipeline-stages/chunking/#eve.steps.chunking.chunker_step.ChunkerStep.execute","title":"<code>execute(documents)</code>  <code>async</code>","text":"<p>Execute chunking on documents in parallel.</p> <p>Processes each document independently using multiprocessing, then flattens all chunks into a single list.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>List[Document]</code> <p>List of documents to chunk</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>Flattened list of all chunks from all documents</p> Source code in <code>data-processing/eve/steps/chunking/chunker_step.py</code> <pre><code>async def execute(self, documents: List[Document]) -&gt; List[Document]:\n    \"\"\"Execute chunking on documents in parallel.\n\n    Processes each document independently using multiprocessing, then flattens\n    all chunks into a single list.\n\n    Args:\n        documents: List of documents to chunk\n\n    Returns:\n        Flattened list of all chunks from all documents\n    \"\"\"\n    self.logger.info(f\"Chunking {len(documents)} documents\")\n    self.logger.info(f\"Using max_chunk_size={self.max_chunk_size}, chunk_overlap={self.chunk_overlap}\")\n    self.logger.info(f\"Parallel processing with max_workers={self.max_workers or 'CPU count'}\")\n\n    loop = asyncio.get_event_loop()\n\n    # Serialize documents to plain dicts for pickling\n    serialized_docs = [_serialize_document(doc) for doc in documents]\n\n    # Create a partial function with the chunker configuration\n    chunk_func = partial(\n        _chunk_document,\n        max_chunk_size=self.max_chunk_size,\n        chunk_overlap=self.chunk_overlap,\n        add_headers=self.add_headers,\n        word_overlap=self.word_overlap,\n        headers_to_split_on=self.headers_to_split_on,\n        merge_small_chunks=self.merge_small_chunks,\n    )\n\n    # Process documents in parallel\n    with ProcessPoolExecutor(max_workers=self.max_workers) as executor:\n        tasks = [\n            loop.run_in_executor(executor, chunk_func, doc)\n            for doc in serialized_docs\n        ]\n        results = await asyncio.gather(*tasks)\n\n    # Flatten and deserialize results\n    all_chunks = []\n    for doc_chunks in results:\n        all_chunks.extend([_deserialize_document(chunk) for chunk in doc_chunks])\n\n    self.logger.info(f\"Chunking complete: {len(documents)} documents -&gt; {len(all_chunks)} chunks\")\n\n    return all_chunks\n</code></pre>"},{"location":"data-processing/docs/pipeline-stages/chunking/#eve.steps.chunking.chunker_step.convert_langchain_doc","title":"<code>convert_langchain_doc(doc, chunk)</code>","text":"<p>Convert a LangChain Document chunk to an Eve Document.</p> <p>Extracts headers from chunk metadata and combines with original document metadata.</p> <p>Parameters:</p> Name Type Description Default <code>doc</code> <code>Document</code> <p>Original Eve Document</p> required <code>chunk</code> <code>Document</code> <p>LangChain Document chunk with header metadata</p> required <p>Returns:</p> Type Description <code>Document</code> <p>Eve Document with chunk content and combined metadata</p> Source code in <code>data-processing/eve/steps/chunking/chunker_step.py</code> <pre><code>def convert_langchain_doc(doc: Document, chunk: LangchainDocument) -&gt; Document:\n    \"\"\"Convert a LangChain Document chunk to an Eve Document.\n\n    Extracts headers from chunk metadata and combines with original document metadata.\n\n    Args:\n        doc: Original Eve Document\n        chunk: LangChain Document chunk with header metadata\n\n    Returns:\n        Eve Document with chunk content and combined metadata\n    \"\"\"\n    headers = [\"#\" * key + value for key, value in chunk.metadata.items()]\n    return Document(\n        content=chunk.page_content,\n        file_path=doc.file_path,\n        file_format=doc.file_format,\n        metadata={\"headers\": headers, **doc.metadata},\n    )\n</code></pre>"},{"location":"data-processing/docs/pipeline-stages/cleaning/","title":"Cleaning Stage","text":"<p>The cleaning stage improves document quality by removing OCR errors, noise artifacts, correcting formatting issues, and enhancing readability.</p>"},{"location":"data-processing/docs/pipeline-stages/cleaning/#features","title":"Features","text":"<ul> <li>Noise Removal: Fixes the errors introduced during the OCR extraction.</li> <li>Nougat Correction: This is a series of post processing cleaning step by Nougat to make the document markdown compactible.</li> <li>Rule based Correction: Custom regex based patterns to remove the most commonly occuring errors.</li> <li>LaTeX Correction: Fixes mathematical equations and notation</li> </ul>"},{"location":"data-processing/docs/pipeline-stages/cleaning/#configuration","title":"Configuration","text":""},{"location":"data-processing/docs/pipeline-stages/cleaning/#basic-configuration","title":"Basic Configuration","text":"<pre><code>- name: cleaning\n  config:\n    ocr_threshold: 0.99\n</code></pre>"},{"location":"data-processing/docs/pipeline-stages/cleaning/#llm-enhanced-cleaning-optional","title":"LLM Enhanced Cleaning (Optional)","text":"<p>For the latex correction, the latex components are extracted and passed to an LLM for improvement, the syntax is verified using pdflatex and then merged back into the document.</p> <p>To use this module, You need to set the .env key for <code>OPENROUTER_API_KEY</code>.</p> <pre><code>- name: cleaning\n  config:\n    enable_latex_correction: true\n</code></pre>"},{"location":"data-processing/docs/pipeline-stages/cleaning/#configuration-parameters","title":"Configuration Parameters","text":""},{"location":"data-processing/docs/pipeline-stages/cleaning/#ocr_threshold","title":"ocr_threshold","text":"<ul> <li>Type: Float</li> <li>Default: <code>0.99</code></li> <li>Description: This parameter controls what level of similarity is required for two sentences to be considered duplicate.</li> </ul>"},{"location":"data-processing/docs/pipeline-stages/cleaning/#min_words","title":"min_words","text":"<ul> <li>Type: Int</li> <li>Default: 2</li> <li>Description: This parameter defines the minimum number of words a sentence should have for the duplication process. Higher the value, the more accurate the duplicate ocr segments are removed.</li> </ul>"},{"location":"data-processing/docs/pipeline-stages/cleaning/#enable_latex_correction","title":"enable_latex_correction","text":"<ul> <li>Type: Boolean</li> <li>Default: <code>false</code></li> <li>Description: Use LLM to fix latex formulas and tables</li> </ul>"},{"location":"data-processing/docs/pipeline-stages/cleaning/#openrouter_model","title":"openrouter_model","text":"<ul> <li>Type: String</li> <li>Default: <code>anthropic/claude-3-haiku</code></li> <li>Description: The model to be used for latex correction</li> </ul>"},{"location":"data-processing/docs/pipeline-stages/cleaning/#debug","title":"debug","text":"<ul> <li>Type: Boolean</li> <li>Default: <code>false</code></li> <li>Description: To enable debug output</li> </ul>"},{"location":"data-processing/docs/pipeline-stages/cleaning/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about PII removal</li> <li>Configure metadata extraction</li> <li>Set up document export</li> </ul>"},{"location":"data-processing/docs/pipeline-stages/deduplication/","title":"Deduplication Stage","text":"<p>The deduplication stage removes duplicate and near-duplicate documents from your dataset, improving data quality and reducing processing overhead.</p>"},{"location":"data-processing/docs/pipeline-stages/deduplication/#deduplication-methods","title":"Deduplication Methods","text":""},{"location":"data-processing/docs/pipeline-stages/deduplication/#exact-deduplication","title":"Exact Deduplication","text":"<p>Uses SHA-256 checksums to identify identical documents:</p> <pre><code>- name: duplication\n  config:\n    method: \"exact\"\n</code></pre>"},{"location":"data-processing/docs/pipeline-stages/deduplication/#lsh-locality-sensitive-hashing","title":"LSH (Locality Sensitive Hashing)","text":"<p>Finds near-duplicates using MinHash:</p> <pre><code>- name: duplication\n  config:\n    method: \"lsh\"\n    shingle_size: 3\n    num_perm: 128\n    threshold: 0.8\n</code></pre>"},{"location":"data-processing/docs/pipeline-stages/deduplication/#configuration-parameters","title":"Configuration Parameters","text":""},{"location":"data-processing/docs/pipeline-stages/deduplication/#lsh-parameters","title":"LSH Parameters","text":""},{"location":"data-processing/docs/pipeline-stages/deduplication/#shingle_size","title":"shingle_size","text":"<ul> <li>Type: Integer</li> <li>Default: <code>3</code></li> <li>Description: Size of text chunks (shingles) for comparison. Larger shingles are more specific but increases computation.</li> </ul>"},{"location":"data-processing/docs/pipeline-stages/deduplication/#num_perm","title":"num_perm","text":"<ul> <li>Type: Integer</li> <li>Default: <code>128</code></li> <li>Description: Number of random permutations for MinHash. Higher values increase accuracy but use more memory</li> </ul>"},{"location":"data-processing/docs/pipeline-stages/deduplication/#threshold","title":"threshold","text":"<ul> <li>Type: Float</li> <li>Default: <code>0.8</code></li> <li>Range: 0.0-1.0</li> <li>Description: Similarity threshold for duplicate detection. Higher values find closer duplicates but may miss some.</li> </ul>"},{"location":"data-processing/docs/pipeline-stages/deduplication/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about content cleaning</li> <li>Configure PII removal</li> <li>Set up metadata extraction</li> </ul>"},{"location":"data-processing/docs/pipeline-stages/export/","title":"Export Stage","text":"<p>The final stage of the pipeline where the processed documents are stored in the format required by the user. The default format is .md</p> <pre><code>- name: export\n  config:\n    destination: \"./output\"\n</code></pre>"},{"location":"data-processing/docs/pipeline-stages/export/#destination","title":"destination","text":"<ul> <li>Type: String</li> <li>Default: <code>./output</code></li> <li>Description: Directory to save the processed files</li> </ul>"},{"location":"data-processing/docs/pipeline-stages/extraction/","title":"Extraction Stage","text":"<p>The extraction stage is responsible for reading and extracting content from various document formats. It's the first stage in most pipeline configurations.</p>"},{"location":"data-processing/docs/pipeline-stages/extraction/#supported-formats","title":"Supported Formats","text":"<ul> <li>PDF: Portable Document Format files</li> <li>HTML: Hypertext Markup Language files</li> <li>XML: Extensible Markup Language files</li> <li>Markdown: Markdown text files</li> <li>JSONL: JSON Lines format (one JSON object per line)</li> </ul>"},{"location":"data-processing/docs/pipeline-stages/extraction/#configuration","title":"Configuration","text":""},{"location":"data-processing/docs/pipeline-stages/extraction/#basic-configuration","title":"Basic Configuration","text":"<pre><code>- name: extraction\n  config:\n    format: \"pdf\"  # or , \"html\", \"xml\", \"markdown\"\n</code></pre>"},{"location":"data-processing/docs/pipeline-stages/extraction/#stage-behavior","title":"Stage Behavior","text":""},{"location":"data-processing/docs/pipeline-stages/extraction/#input-processing","title":"Input Processing","text":"<p>The extraction stage processes documents from the configured input directory:</p> <pre><code>pipeline:\n  inputs:\n    path: \"input_documents\"\n</code></pre> <ul> <li>Recursively scans the input directory</li> <li>Supports nested folder structures</li> </ul>"},{"location":"data-processing/docs/pipeline-stages/extraction/#format-specific-features","title":"Format-Specific Features","text":""},{"location":"data-processing/docs/pipeline-stages/extraction/#pdf-extraction","title":"PDF Extraction","text":"<p>For PDF documents, the extractor:</p> <ul> <li>Extracts text content using Nougat OCR.</li> <li>Preserves document structure (headings, paragraphs).</li> <li>Maintains table and formulas.</li> </ul> <pre><code>- name: extraction\n  config:\n    format: \"pdf\"\n</code></pre>"},{"location":"data-processing/docs/pipeline-stages/extraction/#nougat-server","title":"Nougat Server","text":"<p>You need to setup the nougat server found under the <code>/server</code></p> <pre><code>cd server\npython3 nougat_server.py\n</code></pre>"},{"location":"data-processing/docs/pipeline-stages/extraction/#html-extraction","title":"HTML Extraction","text":"<p>For HTML documents, the extractor use Trafilatura to extract the content.</p> <pre><code>- name: extraction\n  config:\n    format: \"html\"\n</code></pre>"},{"location":"data-processing/docs/pipeline-stages/extraction/#xml-extraction","title":"XML Extraction","text":"<p>For XML documents, the extractor:</p> <ul> <li>Extracts text content from XML tags</li> <li>Preserves document structure</li> <li>Handles namespaces appropriately</li> <li>Maintains attribute information when relevant</li> </ul> <pre><code>- name: extraction\n  config:\n    format: \"xml\"\n</code></pre>"},{"location":"data-processing/docs/pipeline-stages/extraction/#jsonl-extraction","title":"JSONL Extraction","text":"<p>JSONL (JSON Lines) format allows you to input pre-structured documents with custom metadata. Each line in the file must be a valid JSON object.</p> <p>Format Requirements:</p> <p>Required Fields: - <code>content</code> (string): The document text content</p> <p>Optional Fields: - <code>metadata</code> (object): Custom metadata that will be preserved throughout the pipeline - <code>embedding</code> (array): Pre-computed embedding vector (useful when using <code>use_existing_embeddings: true</code>) - <code>pipeline_metadata</code> (object): Internal metadata from previous pipeline runs</p> <p>Example JSONL file:</p> <pre><code>{\"content\": \"This is the first document.\", \"metadata\": {\"title\": \"Document 1\", \"author\": \"John Doe\", \"year\": 2024}}\n{\"content\": \"Second document with tags.\", \"metadata\": {\"title\": \"Doc 2\", \"source\": \"paper.pdf\", \"tags\": [\"AI\", \"ML\"]}}\n{\"content\": \"Document with pre-computed embedding.\", \"metadata\": {\"title\": \"Doc 3\"}, \"embedding\": [0.123, 0.456, ...]}\n</code></pre> <p>Configuration:</p> <pre><code>pipeline:\n  inputs:\n    path: \"data/documents.jsonl\"\n  stages:\n    - name: extraction\n      config:\n        format: \"jsonl\"\n</code></pre> <p>Key Features:</p> <ol> <li>Flexible Metadata: Add any custom fields you need (title, author, tags, year, etc.)</li> <li>Metadata Preservation: All metadata fields are preserved throughout the entire pipeline</li> <li>Metadata Inheritance: When documents are chunked, each chunk inherits the original document's metadata</li> <li>Pre-computed Embeddings: Include embeddings to skip re-computation in later stages</li> <li>Pipeline Chaining: Output from one pipeline can be input to another via JSONL export</li> </ol> <p>Practical Example:</p> <pre><code>pipeline:\n  inputs:\n    path: \"research_papers.jsonl\"\n  stages:\n    - name: extraction\n      config: { format: \"jsonl\" }\n    - name: chunker\n      config: { max_chunk_size: 512 }\n    - name: qdrant_upload\n      config:\n        mode: \"qdrant\"\n        # ... other config\n</code></pre> <p>After chunking, each chunk will have metadata like:</p> <pre><code>{\n  \"title\": \"Document 1\",\n  \"author\": \"John Doe\",\n  \"year\": 2024,\n  \"headers\": [\"#Introduction\", \"##Background\"]\n}\n</code></pre> <p>This metadata is then uploaded to Qdrant, making it easy to filter and search by author, year, or other custom fields.</p>"},{"location":"data-processing/docs/pipeline-stages/extraction/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about deduplication</li> <li>Explore cleaning options</li> <li>Configure PII removal</li> </ul>"},{"location":"data-processing/docs/pipeline-stages/filters/","title":"Filters","text":"<p>Filters allow you to selectively keep or discard documents based on specific criteria. They are essential for quality control and ensuring your processed documents meet your requirements.</p>"},{"location":"data-processing/docs/pipeline-stages/filters/#overview","title":"Overview","text":"<p>Filters in the Eve pipeline evaluate documents against configurable criteria and either keep or discard them. Each filter:</p> <ul> <li>Adds metadata to documents (e.g., word count, PII percentage)</li> <li>Can be configured with <code>keep</code> or <code>discard</code> actions</li> <li>Provides detailed logging of filtering results</li> <li>Can be chained together for complex filtering logic</li> </ul>"},{"location":"data-processing/docs/pipeline-stages/filters/#available-filters","title":"Available Filters","text":""},{"location":"data-processing/docs/pipeline-stages/filters/#length-filter","title":"Length Filter","text":"<p>Filters documents based on word count, useful for removing documents that are too short or too long.</p> <p>Step name: <code>length_filter</code></p>"},{"location":"data-processing/docs/pipeline-stages/filters/#configuration-parameters","title":"Configuration Parameters","text":"Parameter Type Required Default Description <code>length</code> int Yes - Word count threshold for filtering <code>comparison</code> str No <code>\"greater\"</code> Either <code>\"less\"</code> or <code>\"greater\"</code> to compare against threshold <code>action</code> str No <code>\"keep\"</code> Either <code>\"keep\"</code> or <code>\"discard\"</code> documents matching the condition"},{"location":"data-processing/docs/pipeline-stages/filters/#examples","title":"Examples","text":"<pre><code># Keep only documents with more than 1000 words\n- name: length_filter\n  config:\n    length: 1000\n    comparison: \"greater\"\n    action: \"keep\"\n</code></pre> <pre><code># Remove short documents (less than 100 words)\n- name: length_filter\n  config:\n    length: 100\n    comparison: \"less\"\n    action: \"discard\"\n</code></pre> <pre><code># Create a range filter: keep documents between 50-1000 words\n# This requires chaining two length filters\n- name: length_filter\n  config:\n    length: 50\n    comparison: \"greater\"\n    action: \"keep\"\n\n- name: length_filter\n  config:\n    length: 1000\n    comparison: \"less\"\n    action: \"keep\"\n</code></pre>"},{"location":"data-processing/docs/pipeline-stages/filters/#metadata-added","title":"Metadata Added","text":"<ul> <li><code>word_count</code>: Number of words in the document</li> </ul>"},{"location":"data-processing/docs/pipeline-stages/filters/#pii-filter","title":"PII Filter","text":"<p>Filters documents based on the percentage of Personally Identifiable Information (PII) tokens. Useful for removing documents with excessive PII or keeping only anonymized documents.</p> <p>Step name: <code>pii_filter</code></p> <p>Note: This filter expects PII tokens to already be marked in the document (e.g., <code>[PERSON]</code>, <code>[EMAIL_ADDRESS]</code>). Use the PII removal step before this filter to anonymize PII.</p>"},{"location":"data-processing/docs/pipeline-stages/filters/#special-behavior","title":"Special Behavior","text":"<p>Documents containing \"abstract\" or \"introduction\" sections are always kept regardless of PII percentage. This is because academic papers often mention author names in these sections.</p>"},{"location":"data-processing/docs/pipeline-stages/filters/#configuration-parameters_1","title":"Configuration Parameters","text":"Parameter Type Required Default Description <code>threshold</code> float Yes - PII token percentage threshold (e.g., 0.03 for 3%) <code>action</code> str No <code>\"discard\"</code> Either <code>\"keep\"</code> or <code>\"discard\"</code> documents meeting the threshold <code>apply_filter</code> bool No <code>true</code> Whether to apply filtering or just calculate PII percentage"},{"location":"data-processing/docs/pipeline-stages/filters/#examples_1","title":"Examples","text":"<pre><code># Remove documents with 3% or more PII tokens (keep abstracts/intros)\n- name: pii_filter\n  config:\n    threshold: 0.03\n    action: \"discard\"\n</code></pre> <pre><code># Keep only documents with low PII (less than 1%)\n- name: pii_filter\n  config:\n    threshold: 0.01\n    action: \"discard\"\n</code></pre> <pre><code># Just calculate PII percentage without filtering\n- name: pii_filter\n  config:\n    threshold: 0.03\n    apply_filter: false\n</code></pre>"},{"location":"data-processing/docs/pipeline-stages/filters/#metadata-added_1","title":"Metadata Added","text":"<ul> <li><code>pii_tokens_percentage</code>: Percentage of PII tokens in the document (0.0 to 1.0)</li> </ul>"},{"location":"data-processing/docs/pipeline-stages/filters/#newline-filter","title":"Newline Filter","text":"<p>Filters documents based on the number of newline characters, useful for identifying documents with specific formatting characteristics.</p> <p>Step name: <code>newline_filter</code></p>"},{"location":"data-processing/docs/pipeline-stages/filters/#configuration-parameters_2","title":"Configuration Parameters","text":"Parameter Type Required Default Description <code>chunks</code> int Yes - Newline count threshold for filtering <code>comparison</code> str No <code>\"greater\"</code> Either <code>\"less\"</code> or <code>\"greater\"</code> to compare against threshold <code>action</code> str No <code>\"keep\"</code> Either <code>\"keep\"</code> or <code>\"discard\"</code> documents matching the condition"},{"location":"data-processing/docs/pipeline-stages/filters/#examples_2","title":"Examples","text":"<pre><code># Keep documents with more than 10 newlines (well-structured content)\n- name: newline_filter\n  config:\n    chunks: 10\n    comparison: \"greater\"\n    action: \"keep\"\n</code></pre> <pre><code># Remove documents with too many newlines (likely poorly formatted)\n- name: newline_filter\n  config:\n    chunks: 100\n    comparison: \"greater\"\n    action: \"discard\"\n</code></pre>"},{"location":"data-processing/docs/pipeline-stages/filters/#metadata-added_2","title":"Metadata Added","text":"<ul> <li><code>newline_count</code>: Number of newline characters in the document</li> </ul>"},{"location":"data-processing/docs/pipeline-stages/filters/#reference-filter","title":"Reference Filter","text":"<p>Filters documents that contain references or acknowledgements sections, useful for academic paper processing.</p> <p>Step name: <code>reference_filter</code></p> <p>The filter detects references and acknowledgements by checking:</p> <ul> <li>Document headers metadata for keywords: \"reference\", \"references\", \"acknowledgement\", \"acknowledgements\"</li> <li>Text content for markdown headers containing these keywords</li> </ul>"},{"location":"data-processing/docs/pipeline-stages/filters/#configuration-parameters_3","title":"Configuration Parameters","text":"Parameter Type Required Default Description <code>action</code> str No <code>\"discard\"</code> Either <code>\"keep\"</code> or <code>\"discard\"</code> documents with references/acknowledgements"},{"location":"data-processing/docs/pipeline-stages/filters/#examples_3","title":"Examples","text":"<pre><code># Remove documents with references or acknowledgements\n- name: reference_filter\n  config:\n    action: \"discard\"\n</code></pre> <pre><code># Keep only documents with references (academic papers)\n- name: reference_filter\n  config:\n    action: \"keep\"\n</code></pre>"},{"location":"data-processing/docs/pipeline-stages/filters/#perplexity-filter","title":"Perplexity Filter","text":"<p>Filters documents based on perplexity scores calculated by a language model. Lower perplexity indicates more natural, coherent text.</p> <p>Step name: <code>perplexity_filter</code></p> <p>Note: This filter loads a language model which requires significant memory and compute resources.</p>"},{"location":"data-processing/docs/pipeline-stages/filters/#configuration-parameters_4","title":"Configuration Parameters","text":"Parameter Type Required Default Description <code>threshold</code> float No <code>0.0</code> Perplexity threshold for filtering <code>enable_threshold</code> bool No <code>false</code> Whether to apply threshold-based filtering <code>model_name</code> str No <code>\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"</code> Hugging Face model to use for perplexity calculation <code>stride</code> int No <code>128</code> Stride for sliding window perplexity calculation <code>batch_size</code> int No <code>128</code> Batch size for model inference <code>max_length</code> int No <code>1024</code> Maximum sequence length for model"},{"location":"data-processing/docs/pipeline-stages/filters/#examples_4","title":"Examples","text":"<pre><code># Calculate perplexity for all documents without filtering\n- name: perplexity_filter\n  config:\n    model_name: \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n    enable_threshold: false\n</code></pre> <pre><code># Keep only documents with perplexity below 50\n- name: perplexity_filter\n  config:\n    threshold: 50.0\n    enable_threshold: true\n    model_name: \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n</code></pre>"},{"location":"data-processing/docs/pipeline-stages/filters/#metadata-added_3","title":"Metadata Added","text":"<ul> <li><code>perplexity</code>: Perplexity score calculated by the language model</li> </ul>"},{"location":"data-processing/docs/pipeline-stages/filters/#chaining-filters","title":"Chaining Filters","text":"<p>Filters can be chained to create complex filtering logic. Filters are applied sequentially, with each filter receiving the output of the previous filter.</p>"},{"location":"data-processing/docs/pipeline-stages/filters/#example-multi-stage-quality-filter","title":"Example: Multi-Stage Quality Filter","text":"<pre><code>pipeline:\n  inputs:\n    path: \"input_docs\"\n  stages:\n    - name: extraction\n\n    - name: cleaning\n\n    # Remove very short documents\n    - name: length_filter\n      config:\n        length: 50\n        comparison: \"less\"\n        action: \"discard\"\n\n    # Remove very long documents\n    - name: length_filter\n      config:\n        length: 10000\n        comparison: \"greater\"\n        action: \"discard\"\n\n    # Remove documents with reference sections\n    - name: reference_filter\n      config:\n        action: \"discard\"\n\n    # Remove documents with high PII\n    - name: pii_filter\n      config:\n        threshold: 0.05\n        action: \"discard\"\n\n    - name: export\n      config:\n        output_dir: \"filtered_output\"\n</code></pre>"},{"location":"data-processing/docs/pipeline-stages/filters/#best-practices","title":"Best Practices","text":"<ol> <li>Order matters: Apply computationally expensive filters (like perplexity) after cheaper filters (like length) to reduce processing time</li> <li>Test thresholds: Start with permissive thresholds and adjust based on your data</li> <li>Use metadata: Even if <code>apply_filter: false</code>, the metadata added by filters can be useful for analysis</li> <li>Monitor filtering: Check the logs to ensure you're not filtering out too many documents</li> <li>Chain wisely: Use multiple filters to create precise selection criteria</li> </ol>"},{"location":"data-processing/docs/pipeline-stages/filters/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Chunking for splitting documents</li> <li>Configure Metadata Extraction</li> <li>Set up Document Export</li> </ul>"},{"location":"data-processing/docs/pipeline-stages/filters/#code-reference","title":"Code Reference","text":"<p>Length-based document filtering step.</p> <p>PII-based document filtering step with abstract/introduction exceptions.</p> <p>Newline-based document filtering step.</p> <p>Reference and acknowledgement filtering step.</p> <p>Perplexity-based document filtering step.</p>"},{"location":"data-processing/docs/pipeline-stages/filters/#eve.steps.filters.length_filter.LengthFilterStep","title":"<code>LengthFilterStep</code>","text":"<p>               Bases: <code>PipelineStep</code></p> <p>Filter documents based on content length (word count).</p> <p>This step filters documents based on their word count with configurable thresholds and policies.</p> <p>Config parameters:</p> <pre><code>- length (int): The word count threshold for filtering\n- comparison (str): Either \"less\" or \"greater\" to compare against threshold\n- action (str): Either \"keep\" or \"discard\" - what to do with documents matching the condition\n</code></pre> <p>Examples:</p>"},{"location":"data-processing/docs/pipeline-stages/filters/#eve.steps.filters.length_filter.LengthFilterStep--keep-documents-with-more-than-1000-words","title":"Keep documents with more than 1000 words","text":"<p>config: {length: 1000, comparison: \"greater\", action: \"keep\"}</p>"},{"location":"data-processing/docs/pipeline-stages/filters/#eve.steps.filters.length_filter.LengthFilterStep--discard-documents-with-less-than-100-words","title":"Discard documents with less than 100 words","text":"<p>config: {length: 100, comparison: \"less\", action: \"discard\"}</p>"},{"location":"data-processing/docs/pipeline-stages/filters/#eve.steps.filters.length_filter.LengthFilterStep--keep-documents-with-less-than-5000-words-filter-out-long-docs","title":"Keep documents with less than 5000 words (filter out long docs)","text":"<p>config: {length: 5000, comparison: \"less\", action: \"keep\"}</p> Source code in <code>data-processing/eve/steps/filters/length_filter.py</code> <pre><code>class LengthFilterStep(PipelineStep):\n    \"\"\"Filter documents based on content length (word count).\n\n    This step filters documents based on their word count with configurable\n    thresholds and policies.\n\n    Config parameters:\n\n        - length (int): The word count threshold for filtering\n        - comparison (str): Either \"less\" or \"greater\" to compare against threshold\n        - action (str): Either \"keep\" or \"discard\" - what to do with documents matching the condition\n\n    Examples:\n        # Keep documents with more than 1000 words\n        config: {length: 1000, comparison: \"greater\", action: \"keep\"}\n\n        # Discard documents with less than 100 words\n        config: {length: 100, comparison: \"less\", action: \"discard\"}\n\n        # Keep documents with less than 5000 words (filter out long docs)\n        config: {length: 5000, comparison: \"less\", action: \"keep\"}\n    \"\"\"\n\n    def __init__(self, config: dict):\n        super().__init__(config, name=\"LengthFilter\")\n\n        # Validate required config parameters\n        if \"length\" not in config:\n            raise ValueError(\"LengthFilterStep requires 'length' parameter in config\")\n\n        self.length_threshold = config.get(\"length\")\n        self.comparison = config.get(\"comparison\", \"greater\").lower()\n        self.action = config.get(\"action\", \"keep\").lower()\n\n        # Validate comparison parameter\n        if self.comparison not in [\"less\", \"greater\"]:\n            raise ValueError(f\"Invalid comparison '{self.comparison}'. Must be 'less' or 'greater'\")\n\n        # Validate action parameter\n        if self.action not in [\"keep\", \"discard\"]:\n            raise ValueError(f\"Invalid action '{self.action}'. Must be 'keep' or 'discard'\")\n\n        self.logger.info(\n            f\"Initialized LengthFilter: {self.action} documents with \"\n            f\"{self.comparison} than {self.length_threshold} words\"\n        )\n\n    def _get_word_count(self, document: Document) -&gt; int:\n        \"\"\"Get the word count of a document.\n\n        Args:\n            document: Document to count words for\n\n        Returns:\n            Number of words in the document\n        \"\"\"\n        return len(document.content.split())\n\n    def _meets_length_condition(self, document: Document) -&gt; bool:\n        \"\"\"Check if document meets the length condition.\n\n        Args:\n            document: Document to check\n\n        Returns:\n            True if document word count meets the comparison condition\n        \"\"\"\n        word_count = self._get_word_count(document)\n\n        if self.comparison == \"greater\":\n            return word_count &gt; self.length_threshold\n        else:  # \"less\"\n            return word_count &lt; self.length_threshold\n\n    async def execute(self, documents: List[Document]) -&gt; List[Document]:\n        \"\"\"Execute length filtering on documents.\n\n        Args:\n            documents: List of documents to filter\n\n        Returns:\n            Filtered list of documents based on length criteria\n        \"\"\"\n        if not documents:\n            self.logger.warning(\"No documents to filter\")\n            return documents\n\n        original_count = len(documents)\n\n        # Add word count to pipeline metadata for all documents\n        for document in documents:\n            word_count = self._get_word_count(document)\n            document.add_pipeline_metadata(\"word_count\", word_count)\n\n        # Apply filtering based on action\n        filtered_documents = []\n\n        for document in documents:\n            meets_condition = self._meets_length_condition(document)\n\n            # Keep document if:\n            # - action is \"keep\" AND condition is met\n            # - action is \"discard\" AND condition is NOT met\n            should_keep = (self.action == \"keep\" and meets_condition) or \\\n                         (self.action == \"discard\" and not meets_condition)\n\n            if should_keep:\n                filtered_documents.append(document)\n            else:\n                word_count = self._get_word_count(document)\n                self.logger.debug(\n                    f\"Filtered out {document.filename} ({word_count} words)\"\n                )\n\n        filtered_count = len(filtered_documents)\n        removed_count = original_count - filtered_count\n\n        # Log statistics\n        if original_count &gt; 0:\n            percentage_kept = (filtered_count / original_count) * 100\n            self.logger.info(\n                f\"Length filtering complete: {filtered_count}/{original_count} documents kept \"\n                f\"({percentage_kept:.2f}%), {removed_count} documents removed\"\n            )\n        else:\n            self.logger.info(\"No documents were processed\")\n\n        return filtered_documents\n</code></pre>"},{"location":"data-processing/docs/pipeline-stages/filters/#eve.steps.filters.length_filter.LengthFilterStep.execute","title":"<code>execute(documents)</code>  <code>async</code>","text":"<p>Execute length filtering on documents.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>List[Document]</code> <p>List of documents to filter</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>Filtered list of documents based on length criteria</p> Source code in <code>data-processing/eve/steps/filters/length_filter.py</code> <pre><code>async def execute(self, documents: List[Document]) -&gt; List[Document]:\n    \"\"\"Execute length filtering on documents.\n\n    Args:\n        documents: List of documents to filter\n\n    Returns:\n        Filtered list of documents based on length criteria\n    \"\"\"\n    if not documents:\n        self.logger.warning(\"No documents to filter\")\n        return documents\n\n    original_count = len(documents)\n\n    # Add word count to pipeline metadata for all documents\n    for document in documents:\n        word_count = self._get_word_count(document)\n        document.add_pipeline_metadata(\"word_count\", word_count)\n\n    # Apply filtering based on action\n    filtered_documents = []\n\n    for document in documents:\n        meets_condition = self._meets_length_condition(document)\n\n        # Keep document if:\n        # - action is \"keep\" AND condition is met\n        # - action is \"discard\" AND condition is NOT met\n        should_keep = (self.action == \"keep\" and meets_condition) or \\\n                     (self.action == \"discard\" and not meets_condition)\n\n        if should_keep:\n            filtered_documents.append(document)\n        else:\n            word_count = self._get_word_count(document)\n            self.logger.debug(\n                f\"Filtered out {document.filename} ({word_count} words)\"\n            )\n\n    filtered_count = len(filtered_documents)\n    removed_count = original_count - filtered_count\n\n    # Log statistics\n    if original_count &gt; 0:\n        percentage_kept = (filtered_count / original_count) * 100\n        self.logger.info(\n            f\"Length filtering complete: {filtered_count}/{original_count} documents kept \"\n            f\"({percentage_kept:.2f}%), {removed_count} documents removed\"\n        )\n    else:\n        self.logger.info(\"No documents were processed\")\n\n    return filtered_documents\n</code></pre>"},{"location":"data-processing/docs/pipeline-stages/filters/#eve.steps.filters.pii_filter.PiiFilterStep","title":"<code>PiiFilterStep</code>","text":"<p>               Bases: <code>PipelineStep</code></p> <p>Filter documents based on PII (Personally Identifiable Information) token percentage.</p> <p>This step calculates the percentage of PII tokens ([PERSON], [EMAIL_ADDRESS]) in documents and filters them based on a threshold. Documents with \"abstract\" or \"introduction\" in their headers or text are kept regardless of PII percentage.</p> <p>Config parameters:</p> <pre><code>- threshold (float): PII token percentage threshold (e.g., 0.03 for 3%)\n- action (str): Either \"keep\" or \"discard\" (default: \"discard\")\n    - \"discard\": Remove documents with PII &gt;= threshold (except abstract/intro)\n    - \"keep\": Keep only documents with PII &gt;= threshold (except abstract/intro)\n- apply_filter (bool): Whether to apply filtering (default: True)\n</code></pre> <p>Examples:</p>"},{"location":"data-processing/docs/pipeline-stages/filters/#eve.steps.filters.pii_filter.PiiFilterStep--remove-documents-with-3-pii-tokens-but-keep-abstractsintros","title":"Remove documents with &gt;= 3% PII tokens (but keep abstracts/intros)","text":"<p>config: {threshold: 0.03, action: \"discard\", apply_filter: true}</p>"},{"location":"data-processing/docs/pipeline-stages/filters/#eve.steps.filters.pii_filter.PiiFilterStep--only-calculate-pii-percentage-without-filtering","title":"Only calculate PII percentage without filtering","text":"<p>config: {threshold: 0.03, apply_filter: false}</p> Source code in <code>data-processing/eve/steps/filters/pii_filter.py</code> <pre><code>class PiiFilterStep(PipelineStep):\n    \"\"\"Filter documents based on PII (Personally Identifiable Information) token percentage.\n\n    This step calculates the percentage of PII tokens ([PERSON], [EMAIL_ADDRESS]) in documents\n    and filters them based on a threshold. Documents with \"abstract\" or \"introduction\" in their\n    headers or text are kept regardless of PII percentage.\n\n    Config parameters:\n\n        - threshold (float): PII token percentage threshold (e.g., 0.03 for 3%)\n        - action (str): Either \"keep\" or \"discard\" (default: \"discard\")\n            - \"discard\": Remove documents with PII &gt;= threshold (except abstract/intro)\n            - \"keep\": Keep only documents with PII &gt;= threshold (except abstract/intro)\n        - apply_filter (bool): Whether to apply filtering (default: True)\n\n    Examples:\n        # Remove documents with &gt;= 3% PII tokens (but keep abstracts/intros)\n        config: {threshold: 0.03, action: \"discard\", apply_filter: true}\n\n        # Only calculate PII percentage without filtering\n        config: {threshold: 0.03, apply_filter: false}\n    \"\"\"\n\n    def __init__(self, config: dict):\n        super().__init__(config, name=\"PiiFilter\")\n\n        # Validate required config\n        if \"threshold\" not in config:\n            raise ValueError(\"PiiFilterStep requires 'threshold' parameter in config\")\n\n        self.threshold = config.get(\"threshold\")\n        self.special_tokens = [\"[PERSON]\", \"[EMAIL_ADDRESS]\"]\n        self.apply_filter = config.get(\"apply_filter\", True)\n        self.action = config.get(\"action\", \"discard\").lower()\n\n        # Validate action parameter\n        if self.action not in [\"keep\", \"discard\"]:\n            raise ValueError(\n                f\"Invalid action '{self.action}'. Must be 'keep' or 'discard'\"\n            )\n\n        # Regex patterns for detecting abstract and introduction\n        # Allow for leading whitespace before the header\n        self.abstract_header_regex = re.compile(\n            r\"^\\s*#{1,6}\\s*abstract\\b.*$\", re.IGNORECASE | re.MULTILINE\n        )\n        self.introduction_header_regex = re.compile(\n            r\"^\\s*#{1,6}\\s*introduction\\b.*$\", re.IGNORECASE | re.MULTILINE\n        )\n\n        self.logger.info(\n            f\"Initialized PiiFilter: {self.action} documents with PII &gt;= {self.threshold} \"\n            f\"(except abstract/introduction sections), apply_filter={self.apply_filter}\"\n        )\n\n    def _has_abstract_or_introduction(self, doc: Document) -&gt; bool:\n        \"\"\"Check if document has abstract or introduction in headers or text.\n\n        Args:\n            doc: Document to check\n\n        Returns:\n            True if document contains abstract or introduction\n        \"\"\"\n        # Check in text content\n        has_abstract_in_text = bool(self.abstract_header_regex.search(doc.content))\n        has_introduction_in_text = bool(\n            self.introduction_header_regex.search(doc.content)\n        )\n\n        # Check in headers metadata\n        has_abstract_in_headers = False\n        has_introduction_in_headers = False\n\n        if \"headers\" in doc.metadata and isinstance(doc.metadata[\"headers\"], list):\n            headers = doc.metadata[\"headers\"]\n            has_abstract_in_headers = any(\n                \"abstract\" in h.strip().lower() for h in headers\n            )\n            has_introduction_in_headers = any(\n                \"introduction\" in h.strip().lower() for h in headers\n            )\n\n        return (\n            has_abstract_in_text\n            or has_introduction_in_text\n            or has_abstract_in_headers\n            or has_introduction_in_headers\n        )\n\n    async def execute(self, documents: List[Document]) -&gt; List[Document]:\n        \"\"\"Execute PII filtering on documents.\n\n        Args:\n            documents: List of documents to process\n\n        Returns:\n            Filtered list of documents (if apply_filter is True)\n        \"\"\"\n        if not documents:\n            self.logger.warning(\"No documents to process\")\n            return documents\n\n        # Calculate PII percentage for all documents\n        for document in documents:\n            try:\n                total_words = len(document.content.split())\n                special_tokens_count = 0\n                for special_token in self.special_tokens:\n                    special_tokens_count += document.content.count(special_token)\n                # Percentage of pii_tokens overall\n                pii_percentage = special_tokens_count / total_words if total_words &gt; 0 else 0\n                document.add_pipeline_metadata(\"pii_tokens_percentage\", pii_percentage)\n            except Exception as e:\n                self.logger.error(\n                    f\"Error processing {document.filename}, exception {e}\"\n                )\n                document.add_pipeline_metadata(\"pii_tokens_percentage\", 0)\n\n        # Apply filtering if enabled\n        if self.apply_filter:\n            original_len = len(documents)\n            filtered_documents = []\n\n            for doc in documents:\n                pii_percentage = doc.get_pipeline_metadata(\"pii_tokens_percentage\", 0)\n                has_abstract_intro = self._has_abstract_or_introduction(doc)\n                meets_threshold = pii_percentage &gt;= self.threshold\n\n                # Logic:\n                # - If document has abstract/intro: ALWAYS keep (regardless of PII)\n                # - Otherwise: apply threshold-based filtering according to action\n                if has_abstract_intro:\n                    should_keep = True\n                elif self.action == \"discard\":\n                    # Discard documents with high PII (keep documents with low PII)\n                    should_keep = not meets_threshold\n                else:  # action == \"keep\"\n                    # Keep only documents with high PII\n                    should_keep = meets_threshold\n\n                if should_keep:\n                    filtered_documents.append(doc)\n                else:\n                    self.logger.debug(\n                        f\"Filtered out {doc.filename} (PII: {pii_percentage:.4f})\"\n                    )\n\n            filtered_count = len(filtered_documents)\n            removed_count = original_len - filtered_count\n            percentage_kept = (\n                (filtered_count / original_len) * 100 if original_len &gt; 0 else 0\n            )\n\n            self.logger.info(\n                f\"PII filtering complete: {filtered_count}/{original_len} documents kept \"\n                f\"({percentage_kept:.2f}%), {removed_count} documents removed\"\n            )\n\n            return filtered_documents\n\n        return documents\n</code></pre>"},{"location":"data-processing/docs/pipeline-stages/filters/#eve.steps.filters.pii_filter.PiiFilterStep.execute","title":"<code>execute(documents)</code>  <code>async</code>","text":"<p>Execute PII filtering on documents.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>List[Document]</code> <p>List of documents to process</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>Filtered list of documents (if apply_filter is True)</p> Source code in <code>data-processing/eve/steps/filters/pii_filter.py</code> <pre><code>async def execute(self, documents: List[Document]) -&gt; List[Document]:\n    \"\"\"Execute PII filtering on documents.\n\n    Args:\n        documents: List of documents to process\n\n    Returns:\n        Filtered list of documents (if apply_filter is True)\n    \"\"\"\n    if not documents:\n        self.logger.warning(\"No documents to process\")\n        return documents\n\n    # Calculate PII percentage for all documents\n    for document in documents:\n        try:\n            total_words = len(document.content.split())\n            special_tokens_count = 0\n            for special_token in self.special_tokens:\n                special_tokens_count += document.content.count(special_token)\n            # Percentage of pii_tokens overall\n            pii_percentage = special_tokens_count / total_words if total_words &gt; 0 else 0\n            document.add_pipeline_metadata(\"pii_tokens_percentage\", pii_percentage)\n        except Exception as e:\n            self.logger.error(\n                f\"Error processing {document.filename}, exception {e}\"\n            )\n            document.add_pipeline_metadata(\"pii_tokens_percentage\", 0)\n\n    # Apply filtering if enabled\n    if self.apply_filter:\n        original_len = len(documents)\n        filtered_documents = []\n\n        for doc in documents:\n            pii_percentage = doc.get_pipeline_metadata(\"pii_tokens_percentage\", 0)\n            has_abstract_intro = self._has_abstract_or_introduction(doc)\n            meets_threshold = pii_percentage &gt;= self.threshold\n\n            # Logic:\n            # - If document has abstract/intro: ALWAYS keep (regardless of PII)\n            # - Otherwise: apply threshold-based filtering according to action\n            if has_abstract_intro:\n                should_keep = True\n            elif self.action == \"discard\":\n                # Discard documents with high PII (keep documents with low PII)\n                should_keep = not meets_threshold\n            else:  # action == \"keep\"\n                # Keep only documents with high PII\n                should_keep = meets_threshold\n\n            if should_keep:\n                filtered_documents.append(doc)\n            else:\n                self.logger.debug(\n                    f\"Filtered out {doc.filename} (PII: {pii_percentage:.4f})\"\n                )\n\n        filtered_count = len(filtered_documents)\n        removed_count = original_len - filtered_count\n        percentage_kept = (\n            (filtered_count / original_len) * 100 if original_len &gt; 0 else 0\n        )\n\n        self.logger.info(\n            f\"PII filtering complete: {filtered_count}/{original_len} documents kept \"\n            f\"({percentage_kept:.2f}%), {removed_count} documents removed\"\n        )\n\n        return filtered_documents\n\n    return documents\n</code></pre>"},{"location":"data-processing/docs/pipeline-stages/filters/#eve.steps.filters.newline_filter.NewLineFilterStep","title":"<code>NewLineFilterStep</code>","text":"<p>               Bases: <code>PipelineStep</code></p> <p>Filter documents based on the number of newline chunks.</p> <p>This step filters documents based on their newline chunk count with configurable thresholds and policies. A newline chunk is defined as a sequence of text separated by newline characters.</p> <p>Config parameters:</p> <pre><code>- chunks (int): The newline chunk count threshold for filtering\n- comparison (str): Either \"less\" or \"greater\" to compare against threshold\n- action (str): Either \"keep\" or \"discard\" - what to do with documents matching the condition\n</code></pre> <p>Examples:</p>"},{"location":"data-processing/docs/pipeline-stages/filters/#eve.steps.filters.newline_filter.NewLineFilterStep--keep-documents-with-more-than-10-chunks","title":"Keep documents with more than 10 chunks","text":"<p>config: {chunks: 10, comparison: \"greater\", action: \"keep\"}</p>"},{"location":"data-processing/docs/pipeline-stages/filters/#eve.steps.filters.newline_filter.NewLineFilterStep--discard-documents-with-less-than-5-chunks","title":"Discard documents with less than 5 chunks","text":"<p>config: {chunks: 5, comparison: \"less\", action: \"discard\"}</p>"},{"location":"data-processing/docs/pipeline-stages/filters/#eve.steps.filters.newline_filter.NewLineFilterStep--keep-documents-with-less-than-100-chunks-filter-out-heavily-chunked-docs","title":"Keep documents with less than 100 chunks (filter out heavily chunked docs)","text":"<p>config: {chunks: 100, comparison: \"less\", action: \"keep\"}</p> Source code in <code>data-processing/eve/steps/filters/newline_filter.py</code> <pre><code>class NewLineFilterStep(PipelineStep):\n    \"\"\"Filter documents based on the number of newline chunks.\n\n    This step filters documents based on their newline chunk count with configurable\n    thresholds and policies. A newline chunk is defined as a sequence of text\n    separated by newline characters.\n\n    Config parameters:\n\n        - chunks (int): The newline chunk count threshold for filtering\n        - comparison (str): Either \"less\" or \"greater\" to compare against threshold\n        - action (str): Either \"keep\" or \"discard\" - what to do with documents matching the condition\n\n    Examples:\n        # Keep documents with more than 10 chunks\n        config: {chunks: 10, comparison: \"greater\", action: \"keep\"}\n\n        # Discard documents with less than 5 chunks\n        config: {chunks: 5, comparison: \"less\", action: \"discard\"}\n\n        # Keep documents with less than 100 chunks (filter out heavily chunked docs)\n        config: {chunks: 100, comparison: \"less\", action: \"keep\"}\n    \"\"\"\n\n    def __init__(self, config: dict):\n        super().__init__(config, name=\"NewLineFilter\")\n\n        # Validate required config parameters\n        if \"chunks\" not in config:\n            raise ValueError(\"NewLineFilterStep requires 'chunks' parameter in config\")\n\n        self.chunk_threshold = config.get(\"chunks\")\n        self.comparison = config.get(\"comparison\", \"greater\").lower()\n        self.action = config.get(\"action\", \"keep\").lower()\n\n        # Validate comparison parameter\n        if self.comparison not in [\"less\", \"greater\"]:\n            raise ValueError(f\"Invalid comparison '{self.comparison}'. Must be 'less' or 'greater'\")\n\n        # Validate action parameter\n        if self.action not in [\"keep\", \"discard\"]:\n            raise ValueError(f\"Invalid action '{self.action}'. Must be 'keep' or 'discard'\")\n\n        self.logger.info(\n            f\"Initialized NewLineFilter: {self.action} documents with \"\n            f\"{self.comparison} than {self.chunk_threshold} chunks\"\n        )\n\n    def _get_chunk_count(self, document: Document) -&gt; int:\n        \"\"\"Get the number of newline characters in a document.\n\n        Counts the actual newline characters (\\n) in the document content.\n\n        Args:\n            document: Document to count newlines for\n\n        Returns:\n            Number of newline characters in the document\n        \"\"\"\n        # Count newline characters\n        return document.content.count('\\n')\n\n    def _meets_chunk_condition(self, document: Document) -&gt; bool:\n        \"\"\"Check if document meets the chunk count condition.\n\n        Args:\n            document: Document to check\n\n        Returns:\n            True if document chunk count meets the comparison condition\n        \"\"\"\n        chunk_count = self._get_chunk_count(document)\n\n        if self.comparison == \"greater\":\n            return chunk_count &gt; self.chunk_threshold\n        else:  # \"less\"\n            return chunk_count &lt; self.chunk_threshold\n\n    async def execute(self, documents: List[Document]) -&gt; List[Document]:\n        \"\"\"Execute newline chunk filtering on documents.\n\n        Args:\n            documents: List of documents to filter\n\n        Returns:\n            Filtered list of documents based on chunk count criteria\n        \"\"\"\n        if not documents:\n            self.logger.warning(\"No documents to filter\")\n            return documents\n\n        original_count = len(documents)\n\n        # Add chunk count to pipeline metadata for all documents\n        for document in documents:\n            chunk_count = self._get_chunk_count(document)\n            document.add_pipeline_metadata(\"newline_count\", chunk_count)\n\n        # Apply filtering based on action\n        filtered_documents = []\n\n        for document in documents:\n            meets_condition = self._meets_chunk_condition(document)\n\n            # Keep document if:\n            # - action is \"keep\" AND condition is met\n            # - action is \"discard\" AND condition is NOT met\n            should_keep = (self.action == \"keep\" and meets_condition) or \\\n                         (self.action == \"discard\" and not meets_condition)\n\n            if should_keep:\n                filtered_documents.append(document)\n            else:\n                chunk_count = self._get_chunk_count(document)\n                self.logger.debug(\n                    f\"Filtered out {document.filename} ({chunk_count} chunks)\"\n                )\n\n        filtered_count = len(filtered_documents)\n        removed_count = original_count - filtered_count\n\n        # Log statistics\n        if original_count &gt; 0:\n            percentage_kept = (filtered_count / original_count) * 100\n            self.logger.info(\n                f\"NewLine filtering complete: {filtered_count}/{original_count} documents kept \"\n                f\"({percentage_kept:.2f}%), {removed_count} documents removed\"\n            )\n        else:\n            self.logger.info(\"No documents were processed\")\n\n        return filtered_documents\n</code></pre>"},{"location":"data-processing/docs/pipeline-stages/filters/#eve.steps.filters.newline_filter.NewLineFilterStep.execute","title":"<code>execute(documents)</code>  <code>async</code>","text":"<p>Execute newline chunk filtering on documents.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>List[Document]</code> <p>List of documents to filter</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>Filtered list of documents based on chunk count criteria</p> Source code in <code>data-processing/eve/steps/filters/newline_filter.py</code> <pre><code>async def execute(self, documents: List[Document]) -&gt; List[Document]:\n    \"\"\"Execute newline chunk filtering on documents.\n\n    Args:\n        documents: List of documents to filter\n\n    Returns:\n        Filtered list of documents based on chunk count criteria\n    \"\"\"\n    if not documents:\n        self.logger.warning(\"No documents to filter\")\n        return documents\n\n    original_count = len(documents)\n\n    # Add chunk count to pipeline metadata for all documents\n    for document in documents:\n        chunk_count = self._get_chunk_count(document)\n        document.add_pipeline_metadata(\"newline_count\", chunk_count)\n\n    # Apply filtering based on action\n    filtered_documents = []\n\n    for document in documents:\n        meets_condition = self._meets_chunk_condition(document)\n\n        # Keep document if:\n        # - action is \"keep\" AND condition is met\n        # - action is \"discard\" AND condition is NOT met\n        should_keep = (self.action == \"keep\" and meets_condition) or \\\n                     (self.action == \"discard\" and not meets_condition)\n\n        if should_keep:\n            filtered_documents.append(document)\n        else:\n            chunk_count = self._get_chunk_count(document)\n            self.logger.debug(\n                f\"Filtered out {document.filename} ({chunk_count} chunks)\"\n            )\n\n    filtered_count = len(filtered_documents)\n    removed_count = original_count - filtered_count\n\n    # Log statistics\n    if original_count &gt; 0:\n        percentage_kept = (filtered_count / original_count) * 100\n        self.logger.info(\n            f\"NewLine filtering complete: {filtered_count}/{original_count} documents kept \"\n            f\"({percentage_kept:.2f}%), {removed_count} documents removed\"\n        )\n    else:\n        self.logger.info(\"No documents were processed\")\n\n    return filtered_documents\n</code></pre>"},{"location":"data-processing/docs/pipeline-stages/filters/#eve.steps.filters.reference_filter.ReferenceFilterStep","title":"<code>ReferenceFilterStep</code>","text":"<p>               Bases: <code>PipelineStep</code></p> <p>Filter documents containing references or acknowledgements.</p> <p>This step removes documents that contain reference or acknowledgement sections, checking both in document headers and in the document text content.</p> <p>The filter checks for:</p> <ul> <li>Headers containing: \"reference\", \"references\", \"acknowledgement\", \"acknowledgements\"</li> <li>Text content containing markdown headers with these keywords</li> </ul> <p>Config parameters:</p> <pre><code>- action (str): Either \"keep\" or \"discard\" (default: \"discard\")\n    - \"discard\": Remove documents with references/acknowledgements\n    - \"keep\": Keep only documents with references/acknowledgements\n</code></pre> <p>Examples:</p>"},{"location":"data-processing/docs/pipeline-stages/filters/#eve.steps.filters.reference_filter.ReferenceFilterStep--remove-documents-with-references-or-acknowledgements-default-behavior","title":"Remove documents with references or acknowledgements (default behavior)","text":"<p>config: {action: \"discard\"}</p>"},{"location":"data-processing/docs/pipeline-stages/filters/#eve.steps.filters.reference_filter.ReferenceFilterStep--keep-only-documents-with-references-or-acknowledgements","title":"Keep only documents with references or acknowledgements","text":"<p>config: {action: \"keep\"}</p> Source code in <code>data-processing/eve/steps/filters/reference_filter.py</code> <pre><code>class ReferenceFilterStep(PipelineStep):\n    \"\"\"Filter documents containing references or acknowledgements.\n\n    This step removes documents that contain reference or acknowledgement sections,\n    checking both in document headers and in the document text content.\n\n    The filter checks for:\n\n    - Headers containing: \"reference\", \"references\", \"acknowledgement\", \"acknowledgements\"\n    - Text content containing markdown headers with these keywords\n\n    Config parameters:\n\n        - action (str): Either \"keep\" or \"discard\" (default: \"discard\")\n            - \"discard\": Remove documents with references/acknowledgements\n            - \"keep\": Keep only documents with references/acknowledgements\n\n    Examples:\n        # Remove documents with references or acknowledgements (default behavior)\n        config: {action: \"discard\"}\n\n        # Keep only documents with references or acknowledgements\n        config: {action: \"keep\"}\n    \"\"\"\n\n    def __init__(self, config: dict):\n        super().__init__(config, name=\"ReferenceFilter\")\n\n        self.action = config.get(\"action\", \"discard\").lower()\n\n        # Validate action parameter\n        if self.action not in [\"keep\", \"discard\"]:\n            raise ValueError(f\"Invalid action '{self.action}'. Must be 'keep' or 'discard'\")\n\n        # Regex patterns for detecting references and acknowledgements in text\n        # Allow for leading whitespace before the header\n        self.reference_header_regex = re.compile(\n            r\"^\\s*#{1,6}\\s*references?\\b.*$\", re.IGNORECASE | re.MULTILINE\n        )\n        self.acknowledgement_header_regex = re.compile(\n            r\"^\\s*#{1,6}\\s*acknowledgements?\\b.*$\", re.IGNORECASE | re.MULTILINE\n        )\n\n        # Keywords to check in headers\n        self.reference_keywords = [\"reference\", \"references\"]\n        self.acknowledgement_keywords = [\"acknowledgement\", \"acknowledgements\"]\n\n        self.logger.info(\n            f\"Initialized ReferenceFilter: {self.action} documents with references/acknowledgements\"\n        )\n\n    def _has_reference_in_headers(self, document: Document) -&gt; bool:\n        \"\"\"Check if document has reference in headers metadata.\n\n        Args:\n            document: Document to check\n\n        Returns:\n            True if any header contains reference keywords\n        \"\"\"\n        if \"headers\" not in document.metadata:\n            return False\n\n        headers = document.metadata[\"headers\"]\n        if not isinstance(headers, list):\n            return False\n\n        return any(\n            h.strip().lower() in self.reference_keywords\n            for h in headers\n        )\n\n    def _has_acknowledgement_in_headers(self, document: Document) -&gt; bool:\n        \"\"\"Check if document has acknowledgement in headers metadata.\n\n        Args:\n            document: Document to check\n\n        Returns:\n            True if any header contains acknowledgement keywords\n        \"\"\"\n        if \"headers\" not in document.metadata:\n            return False\n\n        headers = document.metadata[\"headers\"]\n        if not isinstance(headers, list):\n            return False\n\n        return any(\n            h.strip().lower() in self.acknowledgement_keywords\n            for h in headers\n        )\n\n    def _has_reference_in_text(self, document: Document) -&gt; bool:\n        \"\"\"Check if document has reference header in text content.\n\n        Args:\n            document: Document to check\n\n        Returns:\n            True if text contains reference markdown header\n        \"\"\"\n        return bool(self.reference_header_regex.search(document.content))\n\n    def _has_acknowledgement_in_text(self, document: Document) -&gt; bool:\n        \"\"\"Check if document has acknowledgement header in text content.\n\n        Args:\n            document: Document to check\n\n        Returns:\n            True if text contains acknowledgement markdown header\n        \"\"\"\n        return bool(self.acknowledgement_header_regex.search(document.content))\n\n    def _contains_reference_or_acknowledgement(self, document: Document) -&gt; bool:\n        \"\"\"Check if document contains references or acknowledgements.\n\n        Checks both headers metadata and text content.\n\n        Args:\n            document: Document to check\n\n        Returns:\n            True if document contains references or acknowledgements\n        \"\"\"\n        return (\n            self._has_reference_in_headers(document) or\n            self._has_acknowledgement_in_headers(document) or\n            self._has_reference_in_text(document) or\n            self._has_acknowledgement_in_text(document)\n        )\n\n    async def execute(self, documents: List[Document]) -&gt; List[Document]:\n        \"\"\"Execute reference/acknowledgement filtering on documents.\n\n        Args:\n            documents: List of documents to filter\n\n        Returns:\n            Filtered list of documents based on reference/acknowledgement criteria\n        \"\"\"\n        if not documents:\n            self.logger.warning(\"No documents to filter\")\n            return documents\n\n        original_count = len(documents)\n        filtered_documents = []\n\n        for document in documents:\n            contains_ref_ack = self._contains_reference_or_acknowledgement(document)\n\n            # Keep document if:\n            # - action is \"discard\" AND document does NOT contain ref/ack\n            # - action is \"keep\" AND document DOES contain ref/ack\n            should_keep = (self.action == \"discard\" and not contains_ref_ack) or \\\n                         (self.action == \"keep\" and contains_ref_ack)\n\n            if should_keep:\n                filtered_documents.append(document)\n            else:\n                self.logger.debug(\n                    f\"Filtered out {document.filename} (contains ref/ack: {contains_ref_ack})\"\n                )\n\n        filtered_count = len(filtered_documents)\n        removed_count = original_count - filtered_count\n\n        # Log statistics\n        if original_count &gt; 0:\n            percentage_kept = (filtered_count / original_count) * 100\n            self.logger.info(\n                f\"Reference/Acknowledgement filtering complete: {filtered_count}/{original_count} documents kept \"\n                f\"({percentage_kept:.2f}%), {removed_count} documents removed\"\n            )\n        else:\n            self.logger.info(\"No documents were processed\")\n\n        return filtered_documents\n</code></pre>"},{"location":"data-processing/docs/pipeline-stages/filters/#eve.steps.filters.reference_filter.ReferenceFilterStep.execute","title":"<code>execute(documents)</code>  <code>async</code>","text":"<p>Execute reference/acknowledgement filtering on documents.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>List[Document]</code> <p>List of documents to filter</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>Filtered list of documents based on reference/acknowledgement criteria</p> Source code in <code>data-processing/eve/steps/filters/reference_filter.py</code> <pre><code>async def execute(self, documents: List[Document]) -&gt; List[Document]:\n    \"\"\"Execute reference/acknowledgement filtering on documents.\n\n    Args:\n        documents: List of documents to filter\n\n    Returns:\n        Filtered list of documents based on reference/acknowledgement criteria\n    \"\"\"\n    if not documents:\n        self.logger.warning(\"No documents to filter\")\n        return documents\n\n    original_count = len(documents)\n    filtered_documents = []\n\n    for document in documents:\n        contains_ref_ack = self._contains_reference_or_acknowledgement(document)\n\n        # Keep document if:\n        # - action is \"discard\" AND document does NOT contain ref/ack\n        # - action is \"keep\" AND document DOES contain ref/ack\n        should_keep = (self.action == \"discard\" and not contains_ref_ack) or \\\n                     (self.action == \"keep\" and contains_ref_ack)\n\n        if should_keep:\n            filtered_documents.append(document)\n        else:\n            self.logger.debug(\n                f\"Filtered out {document.filename} (contains ref/ack: {contains_ref_ack})\"\n            )\n\n    filtered_count = len(filtered_documents)\n    removed_count = original_count - filtered_count\n\n    # Log statistics\n    if original_count &gt; 0:\n        percentage_kept = (filtered_count / original_count) * 100\n        self.logger.info(\n            f\"Reference/Acknowledgement filtering complete: {filtered_count}/{original_count} documents kept \"\n            f\"({percentage_kept:.2f}%), {removed_count} documents removed\"\n        )\n    else:\n        self.logger.info(\"No documents were processed\")\n\n    return filtered_documents\n</code></pre>"},{"location":"data-processing/docs/pipeline-stages/filters/#eve.steps.filters.perplexity.PerplexityFilterStep","title":"<code>PerplexityFilterStep</code>","text":"<p>               Bases: <code>PipelineStep</code></p> <p>Filter documents based on perplexity scores calculated by a language model.</p> <p>This step computes perplexity scores for documents using a causal language model and optionally filters them based on a threshold. Lower perplexity indicates more natural, coherent text.</p> <p>Config parameters:</p> <pre><code>- model_name (str): Hugging Face model name (default: \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\nstride (int): Stride for sliding window perplexity calculation (default: 128)\n- batch_size (int): Batch size for model inference (default: 128)\n- max_length (int): Maximum sequence length for model (default: 1024)\n- threshold (float): Perplexity threshold for filtering (default: 0.0)\n- enable_threshold (bool): Whether to apply threshold-based filtering (default: False)\n</code></pre> <p>Examples:</p>"},{"location":"data-processing/docs/pipeline-stages/filters/#eve.steps.filters.perplexity.PerplexityFilterStep--calculate-perplexity-without-filtering","title":"Calculate perplexity without filtering","text":"<p>config: {model_name: \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", enable_threshold: false}</p>"},{"location":"data-processing/docs/pipeline-stages/filters/#eve.steps.filters.perplexity.PerplexityFilterStep--keep-only-documents-with-perplexity-below-50","title":"Keep only documents with perplexity below 50","text":"<p>config: {threshold: 50.0, enable_threshold: true}</p> Source code in <code>data-processing/eve/steps/filters/perplexity.py</code> <pre><code>class PerplexityFilterStep(PipelineStep):\n    \"\"\"Filter documents based on perplexity scores calculated by a language model.\n\n    This step computes perplexity scores for documents using a causal language model\n    and optionally filters them based on a threshold. Lower perplexity indicates more\n    natural, coherent text.\n\n    Config parameters:\n\n        - model_name (str): Hugging Face model name (default: \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n        stride (int): Stride for sliding window perplexity calculation (default: 128)\n        - batch_size (int): Batch size for model inference (default: 128)\n        - max_length (int): Maximum sequence length for model (default: 1024)\n        - threshold (float): Perplexity threshold for filtering (default: 0.0)\n        - enable_threshold (bool): Whether to apply threshold-based filtering (default: False)\n\n    Examples:\n        # Calculate perplexity without filtering\n        config: {model_name: \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", enable_threshold: false}\n\n        # Keep only documents with perplexity below 50\n        config: {threshold: 50.0, enable_threshold: true}\n    \"\"\"\n\n    def __init__(self, config: dict):\n        \"\"\"Initialize the perplexity filter step.\n\n        Args:\n            config: Configuration dictionary containing model and filtering parameters\n        \"\"\"\n        super().__init__(config, name=\"PerplexityFilterStep\")\n\n        self.tokenizer = None\n        self.model = None\n        self.model_name = config.get(\"model_name\", \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n        self.stride = config.get(\"stride\", 128)\n        self.batch_size = config.get(\"batch_size\", 128)\n        self.max_length = config.get(\"max_length\", 1024)\n        self.threshold = config.get(\"threshold\", 0.0)\n\n        # Enable filtering on threshold\n        self.enable_filter = config.get(\"enable_threshold\", False)\n\n        self.init_model_tokenizer()\n\n    def init_model_tokenizer(self):\n        \"\"\"Initialize the language model and tokenizer.\n\n        Loads the model from Hugging Face and sets up the tokenizer with\n        appropriate padding configuration.\n        \"\"\"\n        self.model = AutoModelForCausalLM.from_pretrained(\n            self.model_name, device_map=\"auto\", torch_dtype=\"auto\"\n        )\n        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n\n    async def execute(self, documents: List[Document]) -&gt; List[Document]:\n        \"\"\"Execute perplexity calculation and optional filtering on documents.\n\n        Computes perplexity scores for all documents and adds them to metadata.\n        If enable_threshold is True, filters documents based on the perplexity threshold.\n\n        Args:\n            documents: List of documents to process\n\n        Returns:\n            List of documents (filtered if enable_threshold is True, otherwise all documents\n            with perplexity scores in metadata)\n        \"\"\"\n        try:\n            for doc in documents:\n                ppl = perplexity(\n                    [doc.content],\n                    self.model,\n                    self.tokenizer,\n                    self.stride,\n                    self.batch_size,\n                    self.max_length,\n                )\n                doc.metadata[\"perplexity\"] = ppl\n        except Exception as e:\n            self.logger.warning(\n                f\"Failed computing ppl for {doc.filename}, exception {e}\"\n            )\n\n        if self.enable_filter:\n            documents = [\n                doc for doc in documents if doc.metadata[\"perplexity\"] &lt;= self.threshold\n            ]\n\n        return documents\n</code></pre>"},{"location":"data-processing/docs/pipeline-stages/filters/#eve.steps.filters.perplexity.PerplexityFilterStep.__init__","title":"<code>__init__(config)</code>","text":"<p>Initialize the perplexity filter step.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Configuration dictionary containing model and filtering parameters</p> required Source code in <code>data-processing/eve/steps/filters/perplexity.py</code> <pre><code>def __init__(self, config: dict):\n    \"\"\"Initialize the perplexity filter step.\n\n    Args:\n        config: Configuration dictionary containing model and filtering parameters\n    \"\"\"\n    super().__init__(config, name=\"PerplexityFilterStep\")\n\n    self.tokenizer = None\n    self.model = None\n    self.model_name = config.get(\"model_name\", \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n    self.stride = config.get(\"stride\", 128)\n    self.batch_size = config.get(\"batch_size\", 128)\n    self.max_length = config.get(\"max_length\", 1024)\n    self.threshold = config.get(\"threshold\", 0.0)\n\n    # Enable filtering on threshold\n    self.enable_filter = config.get(\"enable_threshold\", False)\n\n    self.init_model_tokenizer()\n</code></pre>"},{"location":"data-processing/docs/pipeline-stages/filters/#eve.steps.filters.perplexity.PerplexityFilterStep.execute","title":"<code>execute(documents)</code>  <code>async</code>","text":"<p>Execute perplexity calculation and optional filtering on documents.</p> <p>Computes perplexity scores for all documents and adds them to metadata. If enable_threshold is True, filters documents based on the perplexity threshold.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>List[Document]</code> <p>List of documents to process</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>List of documents (filtered if enable_threshold is True, otherwise all documents</p> <code>List[Document]</code> <p>with perplexity scores in metadata)</p> Source code in <code>data-processing/eve/steps/filters/perplexity.py</code> <pre><code>async def execute(self, documents: List[Document]) -&gt; List[Document]:\n    \"\"\"Execute perplexity calculation and optional filtering on documents.\n\n    Computes perplexity scores for all documents and adds them to metadata.\n    If enable_threshold is True, filters documents based on the perplexity threshold.\n\n    Args:\n        documents: List of documents to process\n\n    Returns:\n        List of documents (filtered if enable_threshold is True, otherwise all documents\n        with perplexity scores in metadata)\n    \"\"\"\n    try:\n        for doc in documents:\n            ppl = perplexity(\n                [doc.content],\n                self.model,\n                self.tokenizer,\n                self.stride,\n                self.batch_size,\n                self.max_length,\n            )\n            doc.metadata[\"perplexity\"] = ppl\n    except Exception as e:\n        self.logger.warning(\n            f\"Failed computing ppl for {doc.filename}, exception {e}\"\n        )\n\n    if self.enable_filter:\n        documents = [\n            doc for doc in documents if doc.metadata[\"perplexity\"] &lt;= self.threshold\n        ]\n\n    return documents\n</code></pre>"},{"location":"data-processing/docs/pipeline-stages/filters/#eve.steps.filters.perplexity.PerplexityFilterStep.init_model_tokenizer","title":"<code>init_model_tokenizer()</code>","text":"<p>Initialize the language model and tokenizer.</p> <p>Loads the model from Hugging Face and sets up the tokenizer with appropriate padding configuration.</p> Source code in <code>data-processing/eve/steps/filters/perplexity.py</code> <pre><code>def init_model_tokenizer(self):\n    \"\"\"Initialize the language model and tokenizer.\n\n    Loads the model from Hugging Face and sets up the tokenizer with\n    appropriate padding configuration.\n    \"\"\"\n    self.model = AutoModelForCausalLM.from_pretrained(\n        self.model_name, device_map=\"auto\", torch_dtype=\"auto\"\n    )\n    self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n    self.tokenizer.pad_token = self.tokenizer.eos_token\n</code></pre>"},{"location":"data-processing/docs/pipeline-stages/metadata-extraction/","title":"Metadata Extraction Stage","text":"<p>The metadata extraction stage automatically identifies and extracts structured metadata from documents.</p>"},{"location":"data-processing/docs/pipeline-stages/metadata-extraction/#extracted-metadata-fields","title":"Extracted Metadata Fields","text":""},{"location":"data-processing/docs/pipeline-stages/metadata-extraction/#document-identification","title":"Document Identification","text":"<ul> <li>title: Document title</li> <li>authors: List of author names</li> <li>doi: Digital Object Identifier</li> <li>url: Source URL or link</li> <li>year: Publication year</li> <li>journal: Journal or publication name</li> <li>publisher: Publisher name</li> </ul>"},{"location":"data-processing/docs/pipeline-stages/metadata-extraction/#extraction-methods","title":"Extraction Methods","text":""},{"location":"data-processing/docs/pipeline-stages/metadata-extraction/#pdf-metadata-extraction","title":"PDF Metadata Extraction","text":"<p>Setup MonkeyOCR using the bash file under the <code>\\server</code> directory. Then run the extractions using this command <code>python3 parse.py &lt;dir&gt; --pred-abandon</code> You will see the predictions stored in the MonkeyOCR folder. You can then run the metadata extraction pipeline given below -</p> <pre><code>pipeline:\n  batch_size: 2\n  inputs:\n    path: \"htmls\" # path to the folder\n  stages:\n    - name: metadata\n      config:\n        enabled_formats: [\"pdf\", \"html\", \"txt\", \"md\"]\n\n    - name: export\n      config: { format: \"jsonl\", output_dir: \"output\"}\n</code></pre> <ol> <li>We first extract text from the first page of the PDF files using MonkeyOCR. The doi and the title are usually present within the first page of the document.</li> <li>We extract dois using handwritten regex patterns<ul> <li>if the file is from arXiv, we invoke the arXiv API to extract metadata.</li> <li>if the file is from other publishers, we invoke the crossref API to extract metadata.</li> </ul> </li> <li>Fallback - if doi is not present, we extract the title and then invoke the crossref API using the title to extract the metadata.</li> </ol>"},{"location":"data-processing/docs/pipeline-stages/metadata-extraction/#other-format-extraction","title":"Other format Extraction","text":"<p>For other documents like HTML, TXT, JSON, the extractor uses handwritten regex patterns to extract the document title and the URL of the page.</p>"},{"location":"data-processing/docs/pipeline-stages/metadata-extraction/#configuration-parameters","title":"Configuration Parameters","text":""},{"location":"data-processing/docs/pipeline-stages/metadata-extraction/#enabled_formats","title":"enabled_formats","text":"<ul> <li>Type: List</li> <li>Default: <code>[\"pdf\", \"html\", \"txt\", \"md\"]</code></li> <li>Description: The list of file formats to process.</li> </ul>"},{"location":"data-processing/docs/pipeline-stages/metadata-extraction/#export_metadata","title":"export_metadata","text":"<ul> <li>Type: Boolean</li> <li>Default: <code>true</code></li> <li>Description: Whether to export metadata to JSON file.</li> </ul>"},{"location":"data-processing/docs/pipeline-stages/metadata-extraction/#metadata_destination","title":"metadata_destination","text":"<ul> <li>Type: String</li> <li>Default: <code>./output</code></li> <li>Description: Directory to save metadata file</li> </ul>"},{"location":"data-processing/docs/pipeline-stages/metadata-extraction/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about document export</li> </ul>"},{"location":"data-processing/docs/pipeline-stages/pii-removal/","title":"PII Removal Stage","text":"<p>This stage detects and redacts NAMES and EMAILS to protect privacy and ensure compliance. We use the Presidio framework with <code>flair/ner-english-large</code> model to detect the entities.</p>"},{"location":"data-processing/docs/pipeline-stages/pii-removal/#pii-server","title":"PII Server","text":"<p>You need to setup the PII server found under the <code>/server</code></p> <pre><code>cd server\npython3 pii_server.py\n</code></pre>"},{"location":"data-processing/docs/pipeline-stages/pii-removal/#configuration","title":"Configuration","text":""},{"location":"data-processing/docs/pipeline-stages/pii-removal/#basic-configuration","title":"Basic Configuration","text":"<pre><code>- name: pii\n  config:\n    url: \"http://127.0.0.1:8000\"\n</code></pre>"},{"location":"data-processing/docs/pipeline-stages/pii-removal/#configuration-parameters","title":"Configuration Parameters","text":""},{"location":"data-processing/docs/pipeline-stages/pii-removal/#url","title":"url","text":"<ul> <li>Type: URL</li> <li>Default: <code>http://127.0.0.1:8000</code></li> <li>Description: The endpoint for the pii server.</li> </ul>"},{"location":"data-processing/docs/pipeline-stages/pii-removal/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about metadata extraction</li> <li>Configure document export</li> </ul>"},{"location":"data-processing/docs/pipeline-stages/qdrant/","title":"Qdrant Upload Stage","text":"<p>The Qdrant upload stage generates embeddings for documents and optionally uploads them to a Qdrant vector database for semantic search and retrieval.</p>"},{"location":"data-processing/docs/pipeline-stages/qdrant/#features","title":"Features","text":"<ul> <li>Dual Mode Operation: Upload to Qdrant database or store embeddings locally</li> <li>Flexible Embedding Sources: Generate new embeddings or use existing ones from metadata</li> <li>Automatic Deduplication: Skips documents already present in the collection</li> <li>Batch Processing: Efficient batch upload with configurable size</li> <li>Optimized Collection Setup: Automatic creation with HNSW indexing and quantization</li> <li>Metadata Indexing: Creates indexes on key fields for efficient filtering</li> <li>Retry Logic: Automatic retry on failures with configurable attempts</li> </ul>"},{"location":"data-processing/docs/pipeline-stages/qdrant/#operating-modes","title":"Operating Modes","text":""},{"location":"data-processing/docs/pipeline-stages/qdrant/#qdrant-mode-default","title":"Qdrant Mode (Default)","text":"<p>Uploads document embeddings to a Qdrant vector database:</p> <pre><code>- name: qdrant\n  config:\n    mode: \"qdrant\"\n    vector_store:\n      url: \"http://localhost:6333\"\n      api_key: \"your-api-key\"  # Optional\n      collection_name: \"my_documents\"\n      batch_size: 100\n      vector_size: 768\n    embedder:\n      url: \"http://localhost:8000\"\n      model_name: \"BAAI/bge-base-en-v1.5\"\n      timeout: 300\n      api_key: \"EMPTY\"  # Optional\n</code></pre>"},{"location":"data-processing/docs/pipeline-stages/qdrant/#local-mode","title":"Local Mode","text":"<p>Generates embeddings and stores them in document metadata without uploading:</p> <pre><code>- name: qdrant\n  config:\n    mode: \"local\"\n    batch_size: 10\n    embedder:\n      url: \"http://localhost:8000\"\n      model_name: \"BAAI/bge-base-en-v1.5\"\n      timeout: 300\n</code></pre>"},{"location":"data-processing/docs/pipeline-stages/qdrant/#configuration-parameters","title":"Configuration Parameters","text":""},{"location":"data-processing/docs/pipeline-stages/qdrant/#mode-configuration","title":"Mode Configuration","text":""},{"location":"data-processing/docs/pipeline-stages/qdrant/#mode","title":"mode","text":"<ul> <li>Type: String</li> <li>Default: <code>\"qdrant\"</code></li> <li>Options: <code>\"qdrant\"</code>, <code>\"local\"</code></li> <li>Description: Operating mode - upload to Qdrant database or store embeddings locally</li> </ul>"},{"location":"data-processing/docs/pipeline-stages/qdrant/#use_existing_embeddings","title":"use_existing_embeddings","text":"<ul> <li>Type: Boolean</li> <li>Default: <code>false</code></li> <li>Description: Use embeddings already stored in <code>document.embedding</code> field instead of generating new ones</li> </ul>"},{"location":"data-processing/docs/pipeline-stages/qdrant/#upload_pipeline_metadata","title":"upload_pipeline_metadata","text":"<ul> <li>Type: Boolean</li> <li>Default: <code>false</code></li> <li>Description: Include pipeline processing metadata in the Qdrant payload</li> </ul>"},{"location":"data-processing/docs/pipeline-stages/qdrant/#batch_size","title":"batch_size","text":"<ul> <li>Type: Integer</li> <li>Default: <code>10</code> (local mode)</li> <li>Description: Number of documents to process in each batch (for local mode only)</li> </ul>"},{"location":"data-processing/docs/pipeline-stages/qdrant/#vector-store-configuration-qdrant-mode","title":"Vector Store Configuration (Qdrant Mode)","text":""},{"location":"data-processing/docs/pipeline-stages/qdrant/#vector_storeurl","title":"vector_store.url","text":"<ul> <li>Type: String</li> <li>Default: <code>\"http://localhost:6333\"</code></li> <li>Description: URL of the Qdrant instance</li> </ul>"},{"location":"data-processing/docs/pipeline-stages/qdrant/#vector_storeapi_key","title":"vector_store.api_key","text":"<ul> <li>Type: String</li> <li>Default: None</li> <li>Description: API key for Qdrant authentication (optional for local instances)</li> </ul>"},{"location":"data-processing/docs/pipeline-stages/qdrant/#vector_storecollection_name","title":"vector_store.collection_name","text":"<ul> <li>Type: String</li> <li>Required: Yes (for Qdrant mode)</li> <li>Description: Name of the target collection in Qdrant</li> </ul>"},{"location":"data-processing/docs/pipeline-stages/qdrant/#vector_storebatch_size","title":"vector_store.batch_size","text":"<ul> <li>Type: Integer</li> <li>Required: Yes (for Qdrant mode)</li> <li>Description: Number of documents to upload per batch</li> </ul>"},{"location":"data-processing/docs/pipeline-stages/qdrant/#vector_storevector_size","title":"vector_store.vector_size","text":"<ul> <li>Type: Integer</li> <li>Required: Yes (for Qdrant mode)</li> <li>Description: Dimension of embedding vectors (must match model output)</li> </ul>"},{"location":"data-processing/docs/pipeline-stages/qdrant/#embedder-configuration","title":"Embedder Configuration","text":""},{"location":"data-processing/docs/pipeline-stages/qdrant/#embedderurl","title":"embedder.url","text":"<ul> <li>Type: String</li> <li>Required: Yes (unless <code>use_existing_embeddings: true</code>)</li> <li>Description: URL of the VLLM embedding server</li> </ul>"},{"location":"data-processing/docs/pipeline-stages/qdrant/#embeddermodel_name","title":"embedder.model_name","text":"<ul> <li>Type: String</li> <li>Required: Yes (unless <code>use_existing_embeddings: true</code>)</li> <li>Description: Name of the embedding model</li> </ul>"},{"location":"data-processing/docs/pipeline-stages/qdrant/#embeddertimeout","title":"embedder.timeout","text":"<ul> <li>Type: Integer</li> <li>Default: <code>300</code></li> <li>Description: Request timeout in seconds</li> </ul>"},{"location":"data-processing/docs/pipeline-stages/qdrant/#embedderapi_key","title":"embedder.api_key","text":"<ul> <li>Type: String</li> <li>Default: <code>\"EMPTY\"</code></li> <li>Description: API key for VLLM authentication (use \"EMPTY\" for local servers)</li> </ul>"},{"location":"data-processing/docs/pipeline-stages/qdrant/#collection-optimization","title":"Collection Optimization","text":"<p>When creating a new collection, the step automatically applies optimized settings for production use. These settings balance search quality, speed, and resource usage for large-scale document collections.</p> <p>Learn more about Qdrant Collections \u2192</p> Vector Configuration  #### Distance Metric: COSINE  Cosine similarity measures the angle between vectors, making it ideal for text embeddings where the direction matters more than magnitude. This is the standard choice for semantic search applications.  [Read about Distance Metrics \u2192](https://qdrant.tech/documentation/concepts/search/#metrics)  #### On-Disk Storage: Enabled  Stores vectors on disk rather than in RAM, significantly reducing memory requirements for large collections. While slightly slower than in-memory storage, this allows you to store millions of vectors affordably.  #### Shards: 8  Distributes data across 8 shards for parallel processing. More shards improve write throughput and allow better resource utilization, especially important for large datasets.  [Learn about Sharding \u2192](https://qdrant.tech/documentation/guides/distributed_deployment/)"},{"location":"data-processing/docs/pipeline-stages/qdrant/#hnsw-index-parameters","title":"HNSW Index Parameters","text":"<p>HNSW (Hierarchical Navigable Small World) is the indexing algorithm that enables fast approximate nearest neighbor search.</p> <p>Deep dive into HNSW \u2192</p>"},{"location":"data-processing/docs/pipeline-stages/qdrant/#m-16","title":"m: 16","text":"<p>Number of bidirectional links created for each node. Higher values improve search quality but increase memory usage and indexing time. 16 is a balanced choice for most applications.</p> <ul> <li>Lower (4-8): Less memory, faster indexing, slightly lower recall</li> <li>Higher (32-64): Better recall, more memory, slower indexing</li> </ul>"},{"location":"data-processing/docs/pipeline-stages/qdrant/#ef_construct-128","title":"ef_construct: 128","text":"<p>Size of the dynamic candidate list during index construction. Higher values produce better index quality but take longer to build. 128 provides good quality without excessive build time.</p> <ul> <li>Lower (64): Faster indexing, slightly lower search quality</li> <li>Higher (256-512): Better search quality, slower indexing</li> </ul>"},{"location":"data-processing/docs/pipeline-stages/qdrant/#full_scan_threshold-10000","title":"full_scan_threshold: 10,000","text":"<p>When collection size is below this threshold, Qdrant uses exact (brute-force) search instead of the HNSW index. Exact search is faster for small collections.</p>"},{"location":"data-processing/docs/pipeline-stages/qdrant/#max_indexing_threads-2","title":"max_indexing_threads: 2","text":"<p>Limits CPU cores used during indexing. Prevents indexing from consuming all available resources.</p>"},{"location":"data-processing/docs/pipeline-stages/qdrant/#on_disk-enabled","title":"on_disk: Enabled","text":"<p>Stores the HNSW graph on disk to reduce RAM usage. Essential for collections with millions of vectors.</p>"},{"location":"data-processing/docs/pipeline-stages/qdrant/#quantization","title":"Quantization","text":"<p>Binary quantization compresses vectors from 32-bit floats to 1-bit representations, reducing memory by ~32x with minimal quality loss. This makes it possible to store much larger collections.</p> <p>Learn about Quantization \u2192</p>"},{"location":"data-processing/docs/pipeline-stages/qdrant/#type-binary-quantization","title":"Type: Binary Quantization","text":"<p>Converts vector components to binary (0 or 1) for massive memory savings. The original vectors are still used for final re-ranking, so search quality remains high.</p> <p>Benefits:</p> <ul> <li>32x memory reduction (32-bit float \u2192 1-bit)</li> <li>Faster distance calculations</li> <li>More vectors fit in RAM for better performance</li> <li>Negligible impact on search quality (typically &lt;2% recall loss)</li> </ul>"},{"location":"data-processing/docs/pipeline-stages/qdrant/#always_ram-false","title":"always_ram: false","text":"<p>Allows quantized vectors to be stored on disk when needed, rather than always keeping them in RAM. This provides flexibility for very large collections.</p>"},{"location":"data-processing/docs/pipeline-stages/qdrant/#optimizer-settings","title":"Optimizer Settings","text":"<p>These settings control how Qdrant manages and optimizes data segments over time.</p> <p>Learn about Storage Optimization \u2192</p>"},{"location":"data-processing/docs/pipeline-stages/qdrant/#indexing_threshold-20000","title":"indexing_threshold: 20,000","text":"<p>Build HNSW index when segment reaches this size. Smaller values create indexes sooner but may cause more frequent rebuilds.</p>"},{"location":"data-processing/docs/pipeline-stages/qdrant/#memmap_threshold-5000","title":"memmap_threshold: 5,000","text":"<p>Use memory-mapped files for segments larger than this. Memory mapping allows efficient disk-based storage without loading everything into RAM.</p>"},{"location":"data-processing/docs/pipeline-stages/qdrant/#max_segment_size-5000000","title":"max_segment_size: 5,000,000","text":"<p>Maximum vectors per segment. Larger segments are more memory-efficient but may slow down some operations.</p>"},{"location":"data-processing/docs/pipeline-stages/qdrant/#max_optimization_threads-2","title":"max_optimization_threads: 2","text":"<p>CPU cores dedicated to background optimization tasks. Prevents optimization from impacting query performance.</p>"},{"location":"data-processing/docs/pipeline-stages/qdrant/#payload-indexes","title":"Payload Indexes","text":"<p>Payload indexes enable fast filtering on metadata fields, similar to database indexes. Without these, filtering requires scanning all documents.</p> <p>Learn about Payload Indexes \u2192</p> <p>The pipeline automatically creates indexes on common academic metadata fields:</p>"},{"location":"data-processing/docs/pipeline-stages/qdrant/#text-indexes-title-journal","title":"Text Indexes (title, journal)","text":"<p>Enable full-text search and filtering on text fields. The word tokenizer splits text into searchable terms.</p> <ul> <li>min_token_len: Minimum word length to index</li> <li>max_token_len: Maximum word length to index</li> <li>lowercase: Normalize to lowercase for case-insensitive search</li> </ul> <p>Example filters:</p> <ul> <li>Find papers with \"neural\" in title</li> <li>Filter by journal name</li> <li>Combine with vector search for semantic + keyword search</li> </ul>"},{"location":"data-processing/docs/pipeline-stages/qdrant/#integer-indexes-year-n_citations","title":"Integer Indexes (year, n_citations)","text":"<p>Enable efficient range queries on numeric fields.</p> <p>Example filters:</p> <ul> <li>Papers published after 2020</li> <li>Papers with &gt;100 citations</li> <li>Combine filters: papers from 2015-2023 with &gt;50 citations</li> </ul> <p>Performance Impact:</p> <ul> <li>Indexes speed up filtering by 100-1000x</li> <li>Small storage overhead (~10-20% of original data)</li> <li>Slightly slower writes (indexes must be updated)</li> </ul>"},{"location":"data-processing/docs/pipeline-stages/qdrant/#stage-behavior","title":"Stage Behavior","text":""},{"location":"data-processing/docs/pipeline-stages/qdrant/#metadata-handling","title":"Metadata Handling","text":"<p>Document metadata is prepared for storage:</p> <ul> <li>content: Document text content</li> <li>metadata: All user metadata fields (unwrapped at root level)</li> <li>pipeline_metadata: Processing metadata (if <code>upload_pipeline_metadata: true</code>)</li> </ul> <p>Type conversions:</p> <ul> <li><code>year</code> field converted to integer</li> <li><code>title</code> field converted to string</li> </ul>"},{"location":"data-processing/docs/pipeline-stages/qdrant/#error-handling","title":"Error Handling","text":"<ul> <li>Batch uploads retry up to 3 times on failure</li> <li>Failed batches are logged and skipped</li> <li>Individual embedding failures are logged without stopping the pipeline</li> <li>Scroll operations retry with exponential backoff</li> </ul>"},{"location":"data-processing/docs/pipeline-stages/qdrant/#usage-examples","title":"Usage Examples","text":""},{"location":"data-processing/docs/pipeline-stages/qdrant/#basic-upload-with-new-embeddings","title":"Basic Upload with New Embeddings","text":"<pre><code>- name: qdrant\n  config:\n    mode: \"qdrant\"\n    vector_store:\n      url: \"http://localhost:6333\"\n      collection_name: \"research_papers\"\n      batch_size: 100\n      vector_size: 768\n    embedder:\n      url: \"http://localhost:8000\"\n      model_name: \"BAAI/bge-base-en-v1.5\"\n</code></pre>"},{"location":"data-processing/docs/pipeline-stages/qdrant/#upload-with-existing-embeddings","title":"Upload with Existing Embeddings","text":"<p>If embeddings were generated in a previous step:</p> <pre><code>- name: qdrant\n  config:\n    mode: \"qdrant\"\n    use_existing_embeddings: true\n    upload_pipeline_metadata: true\n    vector_store:\n      url: \"http://localhost:6333\"\n      api_key: \"your-api-key\"\n      collection_name: \"processed_docs\"\n      batch_size: 50\n      vector_size: 1024\n</code></pre>"},{"location":"data-processing/docs/pipeline-stages/qdrant/#local-embedding-generation","title":"Local Embedding Generation","text":"<p>Store embeddings in document metadata without uploading:</p> <pre><code>- name: qdrant\n  config:\n    mode: \"local\"\n    batch_size: 20\n    embedder:\n      url: \"http://localhost:8000\"\n      model_name: \"sentence-transformers/all-MiniLM-L6-v2\"\n      timeout: 600\n</code></pre>"},{"location":"data-processing/docs/pipeline-stages/qdrant/#vllm-server-setup","title":"VLLM Server Setup","text":"<p>The Qdrant step requires a VLLM server for embedding generation. Start the server:</p> <pre><code>cd server\npython vllm.py\n</code></pre> <p>The server provides an OpenAI-compatible embeddings API at <code>/v1/embeddings</code>.</p>"},{"location":"data-processing/docs/pipeline-stages/qdrant/#complete-pipeline-example","title":"Complete Pipeline Example","text":"<pre><code>pipeline:\n  inputs:\n    path: \"processed_documents\"\n\n  stages:\n    - name: chunking\n      config:\n        max_chunk_size: 512\n        chunk_overlap: 50\n\n    - name: metadata\n      config:\n        enabled_formats: [\"pdf\", \"markdown\"]\n        enable_scholar_search: true\n\n    - name: qdrant\n      config:\n        mode: \"qdrant\"\n        upload_pipeline_metadata: true\n        vector_store:\n          url: \"http://localhost:6333\"\n          collection_name: \"academic_papers\"\n          batch_size: 100\n          vector_size: 768\n        embedder:\n          url: \"http://localhost:8000\"\n          model_name: \"BAAI/bge-base-en-v1.5\"\n          timeout: 300\n</code></pre>"},{"location":"data-processing/docs/pipeline-stages/qdrant/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about chunking strategy</li> <li>Configure metadata extraction</li> <li>Explore export options</li> </ul>"},{"location":"data-processing/docs/pipeline-stages/qdrant/#code-reference","title":"Code Reference","text":""},{"location":"data-processing/docs/pipeline-stages/qdrant/#eve.steps.qdrant.qdrant_step.QdrantUploadStep","title":"<code>QdrantUploadStep</code>","text":"<p>               Bases: <code>PipelineStep</code></p> <p>Pipeline step for uploading chunked documents to Qdrant vector database or storing embeddings locally.</p> <p>Supports two modes:</p> <ul> <li>\"qdrant\": Upload embeddings to a Qdrant vector database</li> <li>\"local\": Store embeddings in document metadata without uploading</li> </ul> Source code in <code>data-processing/eve/steps/qdrant/qdrant_step.py</code> <pre><code>class QdrantUploadStep(PipelineStep):\n    \"\"\"Pipeline step for uploading chunked documents to Qdrant vector database or storing embeddings locally.\n\n    Supports two modes:\n\n    - \"qdrant\": Upload embeddings to a Qdrant vector database\n    - \"local\": Store embeddings in document metadata without uploading\n    \"\"\"\n\n    def __init__(self, config: dict, name: str = \"QdrantUpload\"):\n        \"\"\"Initialize the Qdrant upload step.\n\n        Args:\n            config (dict): Configuration dictionary containing:\n\n                - mode (str, optional): \"qdrant\" or \"local\". Defaults to \"qdrant\".\n                - use_existing_embeddings (bool, optional): If True, use embeddings\n                    from document.embedding field. Defaults to False.\n                - upload_pipeline_metadata (bool, optional): If True, include\n                    pipeline_metadata in Qdrant payload. Defaults to False.\n                - vector_store (dict, required for \"qdrant\" mode):\n                    - url (str): Qdrant instance URL.\n                    - api_key (str, optional): API key for Qdrant authentication.\n                    - collection_name (str): Target collection name.\n                    - batch_size (int): Number of documents per batch.\n                    - vector_size (int): Dimension of embedding vectors.\n                - embedder (dict, required if use_existing_embeddings=False):\n                    - url (str): URL of VLLM embedding server.\n                    - model_name (str): Embedding model identifier.\n                    - timeout (int, optional): Request timeout in seconds. Defaults to 300.\n                    - api_key (str, optional): API key for VLLM. Defaults to \"EMPTY\".\n                - batch_size (int, optional): Batch size for local mode. Defaults to 10.\n            name (str, optional): Name for logging purposes. Defaults to \"QdrantUpload\".\n\n        Raises:\n            ValueError: If mode is not \"qdrant\" or \"local\".\n        \"\"\"\n        super().__init__(config, name)\n\n        # Determine mode: \"qdrant\" or \"local\"\n        self.mode = config.get(\"mode\", \"qdrant\").lower()\n\n        if self.mode not in [\"qdrant\", \"local\"]:\n            raise ValueError(f\"Invalid mode '{self.mode}'. Must be 'qdrant' or 'local'\")\n\n        # Check if we should use existing embeddings from document.embedding field\n        self.use_existing_embeddings = config.get(\"use_existing_embeddings\", False)\n\n        # Check if we should upload pipeline_metadata to Qdrant\n        self.upload_pipeline_metadata = config.get(\"upload_pipeline_metadata\", False)\n\n        # Initialize VLLM embedder only if not using existing embeddings\n        if not self.use_existing_embeddings:\n            embedding_cfg = config[\"embedder\"]\n            self.embedder = VLLMEmbedder(\n                url=embedding_cfg[\"url\"],\n                model_name=embedding_cfg[\"model_name\"],\n                timeout=embedding_cfg.get(\"timeout\", 300),\n                api_key=embedding_cfg.get(\"api_key\", \"EMPTY\"),\n            )\n            self.logger.info(f\"Initialized VLLM embedder at {embedding_cfg['url']}\")\n        else:\n            self.embedder = None\n            self.logger.info(\"Using existing embeddings from document metadata\")\n\n        self.logger.info(f\"Mode: {self.mode}\")\n\n        # Initialize Qdrant-specific configuration only if mode is \"qdrant\"\n        if self.mode == \"qdrant\":\n            vector_store_cfg = config.get(\"vector_store\", {})\n            self.qdrant_url = vector_store_cfg.get(\"url\", \"http://localhost:6333\")\n            self.vector_store_api_key = vector_store_cfg.get(\"api_key\")\n\n            # Get collection configuration\n            self.collection_name = vector_store_cfg[\"collection_name\"]\n            self.batch_size = vector_store_cfg[\"batch_size\"]\n            self.vector_size = vector_store_cfg[\"vector_size\"]\n\n            # Initialize Qdrant client\n            self.client = QdrantClient(\n                url=self.qdrant_url, api_key=self.vector_store_api_key\n            )\n\n            # Ensure collection exists\n            self._ensure_collection()\n            self.existing_ids = self._get_existing_ids()\n        else:\n            # Local mode: set batch size for processing\n            self.batch_size = config.get(\"batch_size\", 10)\n            self.client = None\n\n    def _ensure_collection(self) -&gt; None:\n        \"\"\"Create Qdrant collection if it doesn't exist.\n\n        Creates a new collection with optimized settings including HNSW indexing,\n        binary quantization, and on-disk storage. Also creates payload indexes\n        for efficient filtering.\n        \"\"\"\n        if self.client.collection_exists(self.collection_name):\n            self.logger.info(f\"Collection '{self.collection_name}' already exists\")\n            return\n\n        self.logger.info(f\"Creating collection '{self.collection_name}'\")\n\n        # Create collection with optimized settings\n        self.client.create_collection(\n            collection_name=self.collection_name,\n            vectors_config=models.VectorParams(\n                size=self.vector_size,\n                distance=models.Distance.COSINE,\n                on_disk=True,\n            ),\n            shard_number=8,\n            on_disk_payload=True,\n            quantization_config=models.BinaryQuantization(\n                binary=models.BinaryQuantizationConfig(always_ram=False)\n            ),\n        )\n\n        # Update HNSW configuration\n        self.client.update_collection(\n            collection_name=self.collection_name,\n            hnsw_config=models.HnswConfigDiff(\n                m=16,\n                ef_construct=128,\n                full_scan_threshold=10_000,\n                max_indexing_threads=2,\n                on_disk=True,\n            ),\n        )\n\n        # Update optimizer configuration\n        self.client.update_collection(\n            collection_name=self.collection_name,\n            optimizers_config=models.OptimizersConfigDiff(\n                indexing_threshold=20000,\n                memmap_threshold=5000,\n                deleted_threshold=0.2,\n                vacuum_min_vector_number=1000,\n                default_segment_number=2,\n                max_segment_size=5_000_000,\n                max_optimization_threads=2,\n            ),\n        )\n\n        # Create payload indexes\n        self._create_payload_indexes()\n\n        self.logger.info(\"Collection created and optimized\")\n\n    def _create_payload_indexes(self) -&gt; None:\n        \"\"\"Create indexes on payload fields for efficient filtering.\n\n        Creates text indexes for 'title' and 'journal' fields, and integer\n        indexes for 'year' and 'n_citations' fields to enable fast filtering\n        and searching on these metadata fields.\n        \"\"\"\n        # Text index for title\n        self.client.create_payload_index(\n            collection_name=self.collection_name,\n            field_name=\"title\",\n            field_schema=models.TextIndexParams(\n                type=\"text\",\n                tokenizer=models.TokenizerType.WORD,\n                min_token_len=2,\n                max_token_len=50,\n                lowercase=True,\n            ),\n        )\n\n        # Integer indexes\n        for field in [\"year\", \"n_citations\"]:\n            self.client.create_payload_index(\n                collection_name=self.collection_name,\n                field_name=field,\n                field_schema=\"integer\",\n            )\n\n        # Text index for journal\n        self.client.create_payload_index(\n            collection_name=self.collection_name,\n            field_name=\"journal\",\n            field_schema=models.TextIndexParams(\n                type=\"text\",\n                tokenizer=models.TokenizerType.WORD,\n                min_token_len=1,\n                max_token_len=50,\n                lowercase=True,\n            ),\n        )\n\n    @staticmethod\n    def _string_to_uint(s: str) -&gt; int:\n        \"\"\"Convert string to unsigned integer using SHA256 hash.\n\n        Args:\n            s (str): Input string to hash.\n\n        Returns:\n            int: Unsigned 64-bit integer derived from the hash.\n        \"\"\"\n        hash_bytes = hashlib.sha256(s.encode(\"utf-8\")).digest()\n        return int.from_bytes(hash_bytes[:8], byteorder=\"big\", signed=False)\n\n    def _get_existing_ids(self) -&gt; Set[int]:\n        \"\"\"Retrieve all existing point IDs from the collection with retry logic.\n\n        Scrolls through the entire collection to fetch all point IDs. Implements\n        retry logic with exponential backoff to handle temporary failures.\n\n        Returns:\n            Set[int]: Set of existing point IDs in the collection.\n\n        Raises:\n            Exception: If scroll request fails after max retries.\n        \"\"\"\n        existing_ids = set()\n        scroll_offset = None\n        max_retries = 3\n        retry_delay = 5\n\n        while True:\n            response = None\n            for attempt in range(max_retries):\n                try:\n                    response = self.client.scroll(\n                        collection_name=self.collection_name,\n                        offset=scroll_offset,\n                        limit=10000,\n                        with_payload=False,\n                        with_vectors=False,\n                        timeout=3000,\n                    )\n                    break  # Success, exit retry loop\n                except Exception as e:\n                    if attempt &lt; max_retries - 1:\n                        self.logger.warning(f\"Scroll request failed (attempt {attempt + 1}/{max_retries}): {e}\")\n                        self.logger.info(f\"Retrying in {retry_delay}s...\")\n                        time.sleep(retry_delay)\n                    else:\n                        self.logger.error(f\"Scroll request failed after {max_retries} attempts: {e}\")\n                        raise\n\n            for point in response[0]:\n                existing_ids.add(point.id)\n\n            if response[1] is None:\n                break\n            scroll_offset = response[1]\n\n        return existing_ids\n\n    def _upload_batch(\n        self, batch_ids: List[int], batch_chunks: List[str], batch_metadata: List[dict], batch_embeddings: List[List[float]] = None\n    ) -&gt; None:\n        \"\"\"Upload a batch of documents to Qdrant.\n\n        Generates embeddings (if not provided) and uploads points to Qdrant.\n        Implements retry logic with up to 3 attempts on failure.\n\n        Args:\n            batch_ids (List[int]): List of unique point IDs.\n            batch_chunks (List[str]): List of text chunks to embed.\n            batch_metadata (List[dict]): List of metadata dictionaries for each point.\n            batch_embeddings (List[List[float]], optional): Pre-computed embeddings\n                to use instead of generating new ones. Defaults to None.\n        \"\"\"\n        # Generate embeddings if not provided\n        if batch_embeddings is None:\n            if self.use_existing_embeddings:\n                self.logger.error(\"use_existing_embeddings=True but no embeddings provided\")\n                return\n\n            try:\n                batch_vectors = self.embedder.embed_documents(batch_chunks)\n            except Exception as e:\n                self.logger.error(f\"Embedding error: {e}\")\n                return\n        else:\n            batch_vectors = batch_embeddings\n\n        points = [\n            PointStruct(id=id_val, vector=vec, payload=meta)\n            for id_val, vec, meta in zip(batch_ids, batch_vectors, batch_metadata)\n        ]\n\n        for attempt in range(3):\n            try:\n                self.client.upload_points(\n                    collection_name=self.collection_name,\n                    points=points,\n                    parallel=10,\n                    max_retries=3,\n                )\n                return\n            except Exception as e:\n                self.logger.error(f\"Error uploading batch (attempt {attempt + 1}): {e}\")\n                time.sleep(10)\n                if attempt &lt; 2:\n                    self.logger.info(\"Retrying...\")\n                else:\n                    self.logger.warning(\"Skipping batch after 3 failed attempts\")\n\n    def _prepare_metadata(self, doc: Document) -&gt; dict:\n        \"\"\"Prepare metadata from Document object for Qdrant storage.\n\n        Extracts and cleans metadata fields, performing type conversions and\n        formatting for Qdrant compatibility. Includes document content, user\n        metadata (unwrapped), and optionally pipeline metadata (wrapped).\n\n        Args:\n            doc (Document): Document object containing content and metadata.\n\n        Returns:\n            dict: Dictionary with cleaned metadata ready for Qdrant payload.\n                Includes 'content' field, all metadata fields at root level,\n                and optionally 'pipeline_metadata' as a nested dict.\n        \"\"\"\n        payload = {}\n\n        # Add content\n        payload[\"content\"] = doc.content\n\n        # Add original metadata fields directly to root level (unwrapped)\n        if doc.metadata:\n            # Clean up metadata types\n            metadata_copy = doc.metadata.copy()\n\n            # Ensure year is an integer if present\n            if \"year\" in metadata_copy:\n                try:\n                    metadata_copy[\"year\"] = int(float(metadata_copy[\"year\"]))\n                except (ValueError, TypeError):\n                    metadata_copy[\"year\"] = None\n\n            # Ensure title is a string if present\n            if \"title\" in metadata_copy:\n                metadata_copy[\"title\"] = str(metadata_copy[\"title\"])\n\n            # Add all metadata fields directly to payload (unwrapped)\n            payload.update(metadata_copy)\n\n        # Add pipeline_metadata as wrapped dict if configured\n        if self.upload_pipeline_metadata and doc.pipeline_metadata:\n            payload[\"pipeline_metadata\"] = doc.pipeline_metadata.copy()\n\n        return payload\n\n    async def execute(self, documents: List[Document]) -&gt; List[Document]:\n        \"\"\"Execute the upload step.\n\n        Routes to appropriate execution method based on configured mode\n        (local or qdrant).\n\n        Args:\n            documents (List[Document]): List of Document objects to process.\n\n        Returns:\n            List[Document]: The same list of documents passed through for\n                pipeline chaining.\n        \"\"\"\n        if not documents:\n            self.logger.warning(\"No documents to process\")\n            return documents\n\n        self.logger.info(f\"Processing {len(documents)} documents\")\n\n        if self.mode == \"local\":\n            # Local mode: add embeddings to document metadata\n            return await self._execute_local(documents)\n        else:\n            # Qdrant mode: upload to vector database\n            return await self._execute_qdrant(documents)\n\n    async def _execute_local(self, documents: List[Document]) -&gt; List[Document]:\n        \"\"\"Execute local embedding storage.\n\n        Generates embeddings for documents and stores them in the document.embedding\n        field without uploading to Qdrant. Processes documents in configurable batches.\n\n        Args:\n            documents (List[Document]): List of Document objects to process.\n\n        Returns:\n            List[Document]: Documents with embeddings added to embedding field.\n        \"\"\"\n        self.logger.info(\"Generating embeddings in local mode\")\n\n        # Process in batches\n        for i in tqdm(\n            range(0, len(documents), self.batch_size),\n            desc=\"Generating embeddings\",\n        ):\n            batch = documents[i : i + self.batch_size]\n            batch_texts = [doc.content for doc in batch]\n\n            try:\n                # Generate embeddings\n                batch_embeddings = self.embedder.embed_documents(batch_texts)\n\n                # Add embeddings to document.embedding field\n                for doc, embedding in zip(batch, batch_embeddings):\n                    doc.embedding = embedding\n\n            except Exception as e:\n                self.logger.error(f\"Error generating embeddings for batch: {e}\")\n                # Continue with next batch\n\n        self.logger.info(f\"Successfully generated embeddings for {len(documents)} documents\")\n        return documents\n\n    async def _execute_qdrant(self, documents: List[Document]) -&gt; List[Document]:\n        \"\"\"Execute Qdrant upload mode.\n\n        Prepares document data, generates or extracts embeddings, filters out\n        existing documents, and uploads to Qdrant in batches.\n\n        Args:\n            documents (List[Document]): List of Document objects to upload.\n\n        Returns:\n            List[Document]: The same list of documents passed through for\n                pipeline chaining.\n        \"\"\"\n        # Prepare data for upload\n        ids = []\n        chunks = []\n        metadata = []\n        embeddings = [] if self.use_existing_embeddings else None\n\n        for doc in documents:\n            # Create unique ID based on file path and content hash\n            doc_id = (\n                f\"{doc.filename}_{hashlib.md5(doc.content.encode()).hexdigest()[:8]}\"\n            )\n            ids.append(doc_id)\n            chunks.append(doc.content)\n            metadata.append(self._prepare_metadata(doc))\n\n            # Extract existing embeddings if configured\n            if self.use_existing_embeddings:\n                if doc.embedding is None:\n                    self.logger.error(f\"Document {doc.filename} missing embedding\")\n                    # Skip this document\n                    ids.pop()\n                    chunks.pop()\n                    metadata.pop()\n                    continue\n                embeddings.append(doc.embedding)\n\n        # Convert to uint IDs\n        uint_ids = [self._string_to_uint(id_str) for id_str in ids]\n\n        if self.use_existing_embeddings:\n            to_process = list(zip(uint_ids, chunks, metadata, embeddings, ids))\n        else:\n            to_process = list(zip(uint_ids, chunks, metadata, ids))\n\n        # Filter out existing IDs\n        if self.use_existing_embeddings:\n            to_process = [item for item in to_process if item[0] not in self.existing_ids]\n        else:\n            to_process = [item for item in to_process if item[0] not in self.existing_ids]\n\n        skipped = len(uint_ids) - len(to_process)\n        self.logger.info(f\"Skipping {skipped} existing documents\")\n        self.logger.info(f\"Uploading {len(to_process)} new vectors\")\n\n        # Upload in batches\n        for i in tqdm(\n            range(0, len(to_process), self.batch_size),\n            desc=f\"Uploading to {self.collection_name}\",\n        ):\n            batch = to_process[i : i + self.batch_size]\n\n            if self.use_existing_embeddings:\n                batch_ids = [item[0] for item in batch]\n                batch_chunks = [item[1] for item in batch]\n                batch_metadata = [item[2] for item in batch]\n                batch_embeddings = [item[3] for item in batch]\n                self._upload_batch(batch_ids, batch_chunks, batch_metadata, batch_embeddings)\n            else:\n                batch_ids = [item[0] for item in batch]\n                batch_chunks = [item[1] for item in batch]\n                batch_metadata = [item[2] for item in batch]\n                self._upload_batch(batch_ids, batch_chunks, batch_metadata)\n\n        self.logger.info(f\"Successfully uploaded {len(to_process)} documents\")\n\n        # Return documents for potential further processing\n        return documents\n</code></pre>"},{"location":"data-processing/docs/pipeline-stages/qdrant/#eve.steps.qdrant.qdrant_step.QdrantUploadStep.__init__","title":"<code>__init__(config, name='QdrantUpload')</code>","text":"<p>Initialize the Qdrant upload step.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Configuration dictionary containing:</p> <ul> <li>mode (str, optional): \"qdrant\" or \"local\". Defaults to \"qdrant\".</li> <li>use_existing_embeddings (bool, optional): If True, use embeddings     from document.embedding field. Defaults to False.</li> <li>upload_pipeline_metadata (bool, optional): If True, include     pipeline_metadata in Qdrant payload. Defaults to False.</li> <li>vector_store (dict, required for \"qdrant\" mode):<ul> <li>url (str): Qdrant instance URL.</li> <li>api_key (str, optional): API key for Qdrant authentication.</li> <li>collection_name (str): Target collection name.</li> <li>batch_size (int): Number of documents per batch.</li> <li>vector_size (int): Dimension of embedding vectors.</li> </ul> </li> <li>embedder (dict, required if use_existing_embeddings=False):<ul> <li>url (str): URL of VLLM embedding server.</li> <li>model_name (str): Embedding model identifier.</li> <li>timeout (int, optional): Request timeout in seconds. Defaults to 300.</li> <li>api_key (str, optional): API key for VLLM. Defaults to \"EMPTY\".</li> </ul> </li> <li>batch_size (int, optional): Batch size for local mode. Defaults to 10.</li> </ul> required <code>name</code> <code>str</code> <p>Name for logging purposes. Defaults to \"QdrantUpload\".</p> <code>'QdrantUpload'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If mode is not \"qdrant\" or \"local\".</p> Source code in <code>data-processing/eve/steps/qdrant/qdrant_step.py</code> <pre><code>def __init__(self, config: dict, name: str = \"QdrantUpload\"):\n    \"\"\"Initialize the Qdrant upload step.\n\n    Args:\n        config (dict): Configuration dictionary containing:\n\n            - mode (str, optional): \"qdrant\" or \"local\". Defaults to \"qdrant\".\n            - use_existing_embeddings (bool, optional): If True, use embeddings\n                from document.embedding field. Defaults to False.\n            - upload_pipeline_metadata (bool, optional): If True, include\n                pipeline_metadata in Qdrant payload. Defaults to False.\n            - vector_store (dict, required for \"qdrant\" mode):\n                - url (str): Qdrant instance URL.\n                - api_key (str, optional): API key for Qdrant authentication.\n                - collection_name (str): Target collection name.\n                - batch_size (int): Number of documents per batch.\n                - vector_size (int): Dimension of embedding vectors.\n            - embedder (dict, required if use_existing_embeddings=False):\n                - url (str): URL of VLLM embedding server.\n                - model_name (str): Embedding model identifier.\n                - timeout (int, optional): Request timeout in seconds. Defaults to 300.\n                - api_key (str, optional): API key for VLLM. Defaults to \"EMPTY\".\n            - batch_size (int, optional): Batch size for local mode. Defaults to 10.\n        name (str, optional): Name for logging purposes. Defaults to \"QdrantUpload\".\n\n    Raises:\n        ValueError: If mode is not \"qdrant\" or \"local\".\n    \"\"\"\n    super().__init__(config, name)\n\n    # Determine mode: \"qdrant\" or \"local\"\n    self.mode = config.get(\"mode\", \"qdrant\").lower()\n\n    if self.mode not in [\"qdrant\", \"local\"]:\n        raise ValueError(f\"Invalid mode '{self.mode}'. Must be 'qdrant' or 'local'\")\n\n    # Check if we should use existing embeddings from document.embedding field\n    self.use_existing_embeddings = config.get(\"use_existing_embeddings\", False)\n\n    # Check if we should upload pipeline_metadata to Qdrant\n    self.upload_pipeline_metadata = config.get(\"upload_pipeline_metadata\", False)\n\n    # Initialize VLLM embedder only if not using existing embeddings\n    if not self.use_existing_embeddings:\n        embedding_cfg = config[\"embedder\"]\n        self.embedder = VLLMEmbedder(\n            url=embedding_cfg[\"url\"],\n            model_name=embedding_cfg[\"model_name\"],\n            timeout=embedding_cfg.get(\"timeout\", 300),\n            api_key=embedding_cfg.get(\"api_key\", \"EMPTY\"),\n        )\n        self.logger.info(f\"Initialized VLLM embedder at {embedding_cfg['url']}\")\n    else:\n        self.embedder = None\n        self.logger.info(\"Using existing embeddings from document metadata\")\n\n    self.logger.info(f\"Mode: {self.mode}\")\n\n    # Initialize Qdrant-specific configuration only if mode is \"qdrant\"\n    if self.mode == \"qdrant\":\n        vector_store_cfg = config.get(\"vector_store\", {})\n        self.qdrant_url = vector_store_cfg.get(\"url\", \"http://localhost:6333\")\n        self.vector_store_api_key = vector_store_cfg.get(\"api_key\")\n\n        # Get collection configuration\n        self.collection_name = vector_store_cfg[\"collection_name\"]\n        self.batch_size = vector_store_cfg[\"batch_size\"]\n        self.vector_size = vector_store_cfg[\"vector_size\"]\n\n        # Initialize Qdrant client\n        self.client = QdrantClient(\n            url=self.qdrant_url, api_key=self.vector_store_api_key\n        )\n\n        # Ensure collection exists\n        self._ensure_collection()\n        self.existing_ids = self._get_existing_ids()\n    else:\n        # Local mode: set batch size for processing\n        self.batch_size = config.get(\"batch_size\", 10)\n        self.client = None\n</code></pre>"},{"location":"data-processing/docs/pipeline-stages/qdrant/#eve.steps.qdrant.qdrant_step.QdrantUploadStep.execute","title":"<code>execute(documents)</code>  <code>async</code>","text":"<p>Execute the upload step.</p> <p>Routes to appropriate execution method based on configured mode (local or qdrant).</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>List[Document]</code> <p>List of Document objects to process.</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>List[Document]: The same list of documents passed through for pipeline chaining.</p> Source code in <code>data-processing/eve/steps/qdrant/qdrant_step.py</code> <pre><code>async def execute(self, documents: List[Document]) -&gt; List[Document]:\n    \"\"\"Execute the upload step.\n\n    Routes to appropriate execution method based on configured mode\n    (local or qdrant).\n\n    Args:\n        documents (List[Document]): List of Document objects to process.\n\n    Returns:\n        List[Document]: The same list of documents passed through for\n            pipeline chaining.\n    \"\"\"\n    if not documents:\n        self.logger.warning(\"No documents to process\")\n        return documents\n\n    self.logger.info(f\"Processing {len(documents)} documents\")\n\n    if self.mode == \"local\":\n        # Local mode: add embeddings to document metadata\n        return await self._execute_local(documents)\n    else:\n        # Qdrant mode: upload to vector database\n        return await self._execute_qdrant(documents)\n</code></pre>"},{"location":"data-processing/docs/pipeline-stages/qdrant/#eve.steps.qdrant.qdrant_step.VLLMEmbedder","title":"<code>VLLMEmbedder</code>","text":"<p>Client for VLLM embedding server.</p> <p>This class provides a simple interface to interact with a VLLM server that exposes an OpenAI-compatible embeddings API endpoint.</p> Source code in <code>data-processing/eve/steps/qdrant/qdrant_step.py</code> <pre><code>class VLLMEmbedder:\n    \"\"\"Client for VLLM embedding server.\n\n    This class provides a simple interface to interact with a VLLM server\n    that exposes an OpenAI-compatible embeddings API endpoint.\n    \"\"\"\n\n    def __init__(\n        self, url: str, model_name: str, timeout: int = 300, api_key: str = \"EMPTY\"\n    ):\n        \"\"\"Initialize VLLM embedder client.\n\n        Args:\n            url (str): Base URL of the VLLM server.\n            model_name (str): Name of the embedding model to use.\n            timeout (int, optional): Request timeout in seconds. Defaults to 300.\n            api_key (str, optional): API key for authentication. Use \"EMPTY\" for\n                local servers. Defaults to \"EMPTY\".\n        \"\"\"\n        self.url = url.rstrip(\"/\")\n        self.model_name = model_name\n        self.timeout = timeout\n        self.api_key = api_key\n\n        # Set up headers with API key\n        headers = (\n            {\"Authorization\": f\"Bearer {api_key}\"}\n            if api_key and api_key != \"EMPTY\"\n            else {}\n        )\n        self.client = httpx.Client(timeout=timeout, headers=headers)\n\n    def embed_documents(self, texts: List[str]) -&gt; List[List[float] | None]:\n        \"\"\"Generate embeddings for a list of texts.\n\n        Sends requests to the VLLM server's /v1/embeddings endpoint to generate\n        embeddings for each text. Failed requests return None for that text.\n\n        Args:\n            texts (List[str]): List of text strings to embed.\n\n        Returns:\n            List[List[float] | None]: List of embedding vectors. Each element is\n                either a list of floats (the embedding) or None if embedding failed.\n        \"\"\"\n        embeddings = []\n        for text in texts:\n            endpoint = f\"{self.url}/v1/embeddings\"\n\n            payload = {\"input\": [text], \"model\": self.model_name, \"encoding_format\": \"float\"}\n\n            try:\n                response = self.client.post(endpoint, json=payload)\n                response.raise_for_status()\n\n                result = response.json()\n\n                # Extract embedding (single document)\n                embedding = result[\"data\"][0][\"embedding\"]\n                embeddings.append(embedding)\n\n            except httpx.HTTPError as e:\n                print(f\"VLLM embedding request failed: {e}\")\n                embeddings.append(None)\n            except KeyError as e:\n                embeddings.append(None)\n                print(f\"Unexpected response format from VLLM server: {e}\")\n        return embeddings\n\n    def __del__(self):\n        \"\"\"Clean up HTTP client.\"\"\"\n        if hasattr(self, \"client\"):\n            self.client.close()\n</code></pre>"},{"location":"data-processing/docs/pipeline-stages/qdrant/#eve.steps.qdrant.qdrant_step.VLLMEmbedder.__del__","title":"<code>__del__()</code>","text":"<p>Clean up HTTP client.</p> Source code in <code>data-processing/eve/steps/qdrant/qdrant_step.py</code> <pre><code>def __del__(self):\n    \"\"\"Clean up HTTP client.\"\"\"\n    if hasattr(self, \"client\"):\n        self.client.close()\n</code></pre>"},{"location":"data-processing/docs/pipeline-stages/qdrant/#eve.steps.qdrant.qdrant_step.VLLMEmbedder.__init__","title":"<code>__init__(url, model_name, timeout=300, api_key='EMPTY')</code>","text":"<p>Initialize VLLM embedder client.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>Base URL of the VLLM server.</p> required <code>model_name</code> <code>str</code> <p>Name of the embedding model to use.</p> required <code>timeout</code> <code>int</code> <p>Request timeout in seconds. Defaults to 300.</p> <code>300</code> <code>api_key</code> <code>str</code> <p>API key for authentication. Use \"EMPTY\" for local servers. Defaults to \"EMPTY\".</p> <code>'EMPTY'</code> Source code in <code>data-processing/eve/steps/qdrant/qdrant_step.py</code> <pre><code>def __init__(\n    self, url: str, model_name: str, timeout: int = 300, api_key: str = \"EMPTY\"\n):\n    \"\"\"Initialize VLLM embedder client.\n\n    Args:\n        url (str): Base URL of the VLLM server.\n        model_name (str): Name of the embedding model to use.\n        timeout (int, optional): Request timeout in seconds. Defaults to 300.\n        api_key (str, optional): API key for authentication. Use \"EMPTY\" for\n            local servers. Defaults to \"EMPTY\".\n    \"\"\"\n    self.url = url.rstrip(\"/\")\n    self.model_name = model_name\n    self.timeout = timeout\n    self.api_key = api_key\n\n    # Set up headers with API key\n    headers = (\n        {\"Authorization\": f\"Bearer {api_key}\"}\n        if api_key and api_key != \"EMPTY\"\n        else {}\n    )\n    self.client = httpx.Client(timeout=timeout, headers=headers)\n</code></pre>"},{"location":"data-processing/docs/pipeline-stages/qdrant/#eve.steps.qdrant.qdrant_step.VLLMEmbedder.embed_documents","title":"<code>embed_documents(texts)</code>","text":"<p>Generate embeddings for a list of texts.</p> <p>Sends requests to the VLLM server's /v1/embeddings endpoint to generate embeddings for each text. Failed requests return None for that text.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>List[str]</code> <p>List of text strings to embed.</p> required <p>Returns:</p> Type Description <code>List[List[float] | None]</code> <p>List[List[float] | None]: List of embedding vectors. Each element is either a list of floats (the embedding) or None if embedding failed.</p> Source code in <code>data-processing/eve/steps/qdrant/qdrant_step.py</code> <pre><code>def embed_documents(self, texts: List[str]) -&gt; List[List[float] | None]:\n    \"\"\"Generate embeddings for a list of texts.\n\n    Sends requests to the VLLM server's /v1/embeddings endpoint to generate\n    embeddings for each text. Failed requests return None for that text.\n\n    Args:\n        texts (List[str]): List of text strings to embed.\n\n    Returns:\n        List[List[float] | None]: List of embedding vectors. Each element is\n            either a list of floats (the embedding) or None if embedding failed.\n    \"\"\"\n    embeddings = []\n    for text in texts:\n        endpoint = f\"{self.url}/v1/embeddings\"\n\n        payload = {\"input\": [text], \"model\": self.model_name, \"encoding_format\": \"float\"}\n\n        try:\n            response = self.client.post(endpoint, json=payload)\n            response.raise_for_status()\n\n            result = response.json()\n\n            # Extract embedding (single document)\n            embedding = result[\"data\"][0][\"embedding\"]\n            embeddings.append(embedding)\n\n        except httpx.HTTPError as e:\n            print(f\"VLLM embedding request failed: {e}\")\n            embeddings.append(None)\n        except KeyError as e:\n            embeddings.append(None)\n            print(f\"Unexpected response format from VLLM server: {e}\")\n    return embeddings\n</code></pre>"}]}